{"docstore/metadata": {"42dfc06b-bed4-4399-a4d5-585e7c3d8a9d": {"doc_hash": "2fc6cb4cdfbea08a445f07c5af165d9904201cd44f352662fecab97f3a1fd29d"}, "2c13a6b1-322e-413f-886d-15fb0dad4c30": {"doc_hash": "701e79f2c16b7cab2cc72e570cc873a5a6376262e5672850f8e62d4a1619e9fd"}, "c2c679f6-aa44-4219-b7b2-02b80385c7d4": {"doc_hash": "9b74892be46082ea53f623ce0c905bb16e7d80b26aab78f07d054f38e5855a01"}, "ff5a837d-9dac-4240-a556-379ad7c0fb63": {"doc_hash": "b2fd28c68a7dda5807a6e3a8d71602a5c920971d70dcec03bc4bc4c3552c1be6"}, "bcbe17cb-d5a2-44cd-b718-cd59130b531e": {"doc_hash": "cf07f413336a3cf9eba1ad2ca4688b7569caf3712203a6649edcaceaa0e167ef"}, "a3356b4b-8302-4edb-8271-ab839ddf2b5a": {"doc_hash": "ff079a9dbfe02ac2c4aaab6c80516f55331bded84ee32448f15c65c95e1ba2ab"}, "b5f15307-d81e-4764-874a-4061b39198f6": {"doc_hash": "01f22f9288c5b4a93f8ae7611a253f2a2a288758226514d94103ed58aa375ac2"}, "9a727675-6b4c-4065-bf0a-2c31152f7cb2": {"doc_hash": "8937b37272d256ae2e2a5fa8e4480a441c543147b6548212bd79e0f8b868f728"}, "ec1486e7-a8af-46f9-b662-61e579b2c00b": {"doc_hash": "0ed50a026ee830b703db712b55013a0a4a1c6ee73a359c925a3740036d675c79"}, "236cfce7-63f0-422a-9231-4f6cd1e49bfa": {"doc_hash": "d6c8e49e7698b3cb4c368f70b456339dd31be0635f6a37d5d8276c86c55b0d43"}, "40155453-fe17-4363-a9e6-cf3e7b94541b": {"doc_hash": "af3b609f35cde44233658f603f0b014cad4d1b5bee9561b35fb49e73c5ed2217"}, "463eb972-64fb-47d6-a299-e117425adc0d": {"doc_hash": "d1057033f9bbbea2ccc2b4222097612a3ba43b3e914926174db59cc51352c7f0"}, "b14cf05b-cb8f-4706-98c7-5aa7838e6b00": {"doc_hash": "5843657329162d91e0e7615d19716081744a31621e4deb9a939993dd16a4f4e8"}, "f043dc5b-32a0-44b0-8003-76ef4c10d44e": {"doc_hash": "49e03381ffc1a4c385b132b413a91e3857e807331bb20a208fc51792489dc6e2"}, "3b495b58-2261-4c79-9ef7-56ef823b29ae": {"doc_hash": "b05917359018bab6a3aebc3482a927e384d823ae7b0910624fe8df32890ee601"}, "300fa914-1a49-4e28-be76-ffa821a9c22c": {"doc_hash": "2ce8b42f925654085e4a8fc17c73aed396d95246b790618539bcb08c49847dd8"}, "9e9ee98d-f957-414c-a7be-b407a3f0eff8": {"doc_hash": "64caabec7e125bc1e68f7269762863b8ea66a2c3b294bb47efec438ac073d915"}, "145d45b6-8460-4056-911a-382d6e4d322b": {"doc_hash": "b807f19673773b7868802dc0896b7439400c8187400b810b556a2bafd2ae922a"}, "9b562cbf-8e90-4b91-96ba-3094fe3fa559": {"doc_hash": "14184690a28a43818a27800fd36b5a4f5b0f7cbdcd9338f9781a65b03e4cf01d"}, "dc5316fe-3305-4445-9d15-30dd1f46117d": {"doc_hash": "684ed14b30b0fa9b27b5d4bd6ca3e85d25d7585d6a9ad53239f850caa56b930a"}, "e8d1afae-2b38-41a4-a784-141fb81c6fd4": {"doc_hash": "e777cf54c24633ebc23c55c08a12d905716cba5e61d58f18989ce92bc60d2920"}, "91528d1a-8dbc-41b0-922f-dfe21a9cca72": {"doc_hash": "1d62f78ae8ba9795dd83c4cbe3a7c112ddb5c21c028a161f4b07607053b566f8"}, "6391795a-8c64-4f4c-a62e-922cfd00a928": {"doc_hash": "a86812a40d79aa0af45d7477316b44a70728d50d24e4e0f0529c09278e8c594e"}, "92edad98-3281-46f2-968f-5a6a78cde3e2": {"doc_hash": "c1bcf00b4fded462af7490fc98798018f56a88990b353a7a9f25128f96667edb"}, "529005d4-ff38-410a-8c57-399b16b82484": {"doc_hash": "9c3c48b13aa81b1cfd403852424b32acb3342b7b3844e208ce844d23cc0c8e44"}, "3ae2fc96-95f8-4af2-b2d5-1f343bbd6cd8": {"doc_hash": "a846215a5505f84579c1a4a390914d48356cda71d79b93fc22fab9c8ce772874"}, "05c91038-c4fd-45dc-bb30-fe11f6af60d8": {"doc_hash": "fe2da8d451bff20ab434296b05c95712cb086718d7dc8e186edffaca63205c3b"}, "f62527f3-faf3-4cb1-a502-bcb3672b72b2": {"doc_hash": "37e44519da2fd3e08b46f72721e8df1dceb7b55f5bec1f0282d233a22a690352"}, "896b708d-b98a-4393-bde9-b572afdd793f": {"doc_hash": "04e60b2a8b30a329a9d23933c8a2d753fb980a1a734ba4f0b4834eb88de2d961"}, "2d714854-2f05-4db6-a0ae-010d0c7207dd": {"doc_hash": "29593a5c02372efb8e575002be886e8f580cb429ed243511abfbe0fe9ed3c33d"}, "cac1dd84-9e09-4d57-8b7e-9f26057bd77e": {"doc_hash": "896a22abdba986104f6c47ded1b656d552f9b6ea1c01a02bd952542fd352eecf"}, "e255d32d-2304-43e8-a73a-84dc3a995e17": {"doc_hash": "6324d6aa39ec2d8510d529c46b83f84da0a426b2bb1ded8f729bbeadea8d31c4"}, "3a49a07b-e5f0-4d89-9143-35441564041b": {"doc_hash": "0e6819eaa1b49283e83a6c683dd3a60a62c9ea96a1b379b9cf2f6b96c36c305b"}, "e6252d24-3ec6-433c-bf6c-71387eb1ef4c": {"doc_hash": "fb8fa0d788bd76854a4c934f4c98950d36bfcd971f625948dbf7cc7727b0eedc"}, "231d88f9-4a99-41a2-9b70-2686e067d0d4": {"doc_hash": "da89afaf1ab0362abcafcb75f9ee66fcfccbbe59701d4281a9ecdada4e134e42"}, "77bca458-a819-4b0e-ac9b-740545893ab4": {"doc_hash": "35c02fff9a4b0184ea2e7b67b8630a13da95a68cd244e92cec1cebe01c5c23b4"}, "1aa2b63d-c630-48f0-9759-a1a20df01941": {"doc_hash": "5004cc66518cc7e20942a0f9b6eade6cf799acc02797fdb624f9f5c03a5cad36"}, "57646547-3bed-458b-9a97-9d67154c7286": {"doc_hash": "67a8263d278e18cd7e06ad932dcc37df49818c231791b9fbd5d064aad77be7f7"}, "22ee1e88-5647-465a-b62c-61af6cdad8b4": {"doc_hash": "eac08a297169a896aceeddf052d46bf59225887b0428ce6b4a86b884755c8c69"}, "8b6851e6-3986-426b-a835-95a001537067": {"doc_hash": "29dfecc692e7cd59cc783d5c0b495d78faf32403f0158b3e41f9fd5150dde1ac"}, "eca461b2-31dd-49f4-8e89-684463bb7d30": {"doc_hash": "a17e651f185b04e2bf219687706c92f96a09a13006b56cef2e9b0149677c3f54"}, "ff04144a-ff21-400c-a57a-dc4a8e83c8dc": {"doc_hash": "936ce594b501a79b54d8f0eb3c83758a7b42f67688f4cd0859b3f8aa6470ee2f"}, "bee74d58-9e81-4711-ade1-92cc2922b28d": {"doc_hash": "dfd0a9495d1a67767988e83d0a540d53610b03f2afeb1b17bc21b263a8e3d0d1"}, "166efef4-6439-467e-bbbf-bc4c9959597d": {"doc_hash": "a6725ca2d2f9a903e99e8b202f79e6f80157c379f1361427dda0d246d853e580"}, "16f5ca3f-72d2-42e7-a58b-5efa196c2798": {"doc_hash": "8a696dab9e378edbabfba0c17a42de5641f668fd98f6225af1ddfe922738b6c0"}, "952adb7b-a96b-47ae-838a-13381806395a": {"doc_hash": "032e560a50ca1f7d58ca378f282f03dd1f0b99bf9c08ee2484fb8913c062f752"}, "72f70266-cc7c-4783-bd8f-54e3da726b64": {"doc_hash": "142b91c0f7e47ec90be5c07052f9825cb00b34bafb4bef59607f83089ffb7697"}, "dad7c135-9a03-4c63-aaa8-e6c056775732": {"doc_hash": "c4a92abf50b94fc8b1c7a9565e0c665456875a30d287209c3af84d604581a881"}, "a05dd0f5-f3e3-4614-b721-c22c184e4171": {"doc_hash": "4929b881d8e67c6ac7f596dac15dc02df1f8dac8f89cb007646350dd92dd8a0d"}, "3b66cd3f-b0a9-46b2-9169-efd9f5ab88b3": {"doc_hash": "d9809f47829aedac6e8676ec3c18cd98674f1a356ce79eaff7aff9ccbc39263f"}, "8337b4ab-37fd-4c5c-94dd-e73ae77bf82b": {"doc_hash": "6c61d331178525a4ab2295933e1866db97aaa6642f492aab1100b7b11e91c07e"}, "5b7cb74b-91ca-4cba-9b0a-4ea45ec07018": {"doc_hash": "61b771387aa7f5734ccee679ee82883002e6e0e838499402ddf7355e7a5bae89"}, "17700155-9b95-4044-8bdd-56f6a25b41c5": {"doc_hash": "aa34b8a7ee63e6995b025cf39a340a1634f76c8f8d4f749c80960bca59bb4b7a"}, "04f346ca-c21f-4cbc-b462-70b60b75c872": {"doc_hash": "79179e9cf6b937607f938098b7591e48b608b51f236704a9417ad762724286e2"}, "f8564556-755a-4f62-a64a-b923a7b64262": {"doc_hash": "7d40f3716e08f2a0218068058c0bb827d02b0994361b3cc5485cd547361e1e49"}, "62936da7-cbd5-44bd-8952-1899d9bd1736": {"doc_hash": "8c642f970560ca8bf3df45b9c6cab6b98ef1709aebc6b583eb15595a4f968ed6"}, "eac05390-70d0-49cc-be8e-3c0c617a38ca": {"doc_hash": "acb4e35ec8dc73988ba79d5d3eaac6e1c7cc8ac3e8bd3da705e851f5751ccbe9"}, "11c5b1e0-d02a-48eb-ab94-04a17e350133": {"doc_hash": "ebac0a5cd8519a5fb4aa931b342af4d94e67d07efd459647c6f960007a5b2292"}, "82b97858-dab1-4527-bc5f-56061962431e": {"doc_hash": "4cac14dd19bd3a0186cdef32defb608b437feede10eeef557abbef401590219d"}, "beebe7be-037d-4c1f-abfa-6a5b6f3cf974": {"doc_hash": "2644f5d679643dd76ca9ec45129cd54e846c20ba418060b0c17ae42873dd8251"}, "f685bc46-fec8-417a-b9f1-5363da5466f9": {"doc_hash": "1851fd09b66277ab9e32b9c571841832f9be5a434e85adab63592b8c139ab168"}, "ab2e30c6-ceb5-48e0-bb32-0bb147deec11": {"doc_hash": "6bcf8522af70572ea41cc421923d27be415b6a5ecbac832dc76dd79997d369be"}, "93c3cf00-5dab-4807-b566-2173f8e32b19": {"doc_hash": "c8ad9e3354e5a3484e7fd2736ed8cad0e0b4abf7742ec7d975a6e4ec605c26d1"}, "c5948655-dac7-4e52-91a7-9184c2c27339": {"doc_hash": "dba8f6fe650735ae23f52945a029c204d1583c9d94a7163cdcb6e6504067c6cf"}, "dbf10e25-8ce0-4cb4-bdd8-bea644d79cf6": {"doc_hash": "6864eede04f945939581da24e43a82d4ed9f92dbd7ed00afcdb9931915a6de35"}, "1a618400-0ee9-45f9-a8ae-5d06524711ac": {"doc_hash": "0d8fd36eed20f500528b0492b662bca7058a12c5512535470b719a1786d95374"}, "4aba9541-cccd-49e5-b2ac-e250522d9618": {"doc_hash": "46a4e43192d992ed9ea4a65d3117ec2bc026d6cb2097d0676b8894f46ce1eb45"}, "fa1c767d-4e6b-4a38-b356-672d353b0e17": {"doc_hash": "61cc6a944159aba960d04fde6c3b744f8ffccc3e1ebae31722e8dda5000938bb"}, "4ef05452-1812-44b9-90ae-7fea2cdd255a": {"doc_hash": "4369ae43db7023576b8aca0a1e7c66495bef76ce1f77c6e6d22d03fa9110bd32"}, "3e69a925-8276-4828-8be8-47e81923fca0": {"doc_hash": "bca2e4fb5bba54805042737423b27fadb593e042f0352c5fecca4b292f493140"}, "73a89634-2cb5-4c16-a73b-89ad22fcb4ed": {"doc_hash": "5165bf34dafe522623231db25e73222292d33aa16dc5c6bbc876801d0b709764"}, "4b16ebc7-bc2e-47e4-b6cb-043478a5b937": {"doc_hash": "b4331f9982a6b87c83a7ae4f8f9f97708b5da081651c323d1187694912b10dfd"}, "f83e2fd0-70ac-4676-8137-970b4fa47ea0": {"doc_hash": "03ca0f637cf47c1527561a89cf0f10a50349825a8601d82f45ee4aa32ff91584"}, "8b62eb6c-9ca7-4889-b8e5-96ece790887d": {"doc_hash": "1edebc027e57dc71821fe2aff665b83c4f36d82d68c4606d5b1136cae1292b02"}, "f2de2223-c69c-450a-a273-63b345aa8043": {"doc_hash": "6a0d962fd2628fa03a7bb4b76a43f2794bd7247ab3678431b84215e3c5637e64"}, "02c012ff-ec10-4a3d-8f41-f60e812ac611": {"doc_hash": "64801ef9ca5cc8264daff304ecd0c1d79ed827e3a71208f519e04bfce945d90c"}, "671a4e26-0453-4a71-9a17-08cd9faca12c": {"doc_hash": "03de14ff41951cd3ad503d5f9a733ecd82780c2f467530d6a638bd4610edc0df"}, "84dc5868-0544-492d-9926-8ebcce33a25d": {"doc_hash": "8fca2c42b48bb27d64162cf4fa5f5985d549bfce3a0ff14fb0dba6b867d512da"}, "a2b046e6-16b2-4143-ac1a-e10772a9b55a": {"doc_hash": "a864b07f328e4a96e8756da7d93273d44922d1c25c66f6af1d0e5827202166e2"}, "cb66a87e-c56c-40a2-b8a1-4a75ea693a26": {"doc_hash": "ef4951e8ce567f6afc1c55d83f72090c394ab4d47d11161277806ca02f71bd10"}, "83f9605f-2565-4e30-8d4e-14d2d3b22650": {"doc_hash": "f1169922e06d835c6ea22cc123d34906a286c709527a14361e8ecb4c436daa06"}, "500061dc-92f7-4365-8406-99234abea413": {"doc_hash": "f61bdce7a6efdd6beac82353a28f3540b38db2c7bf2b2519846ed887d61fe46f"}, "63e3323b-29c5-437a-a896-71540232be79": {"doc_hash": "66d9ec1ce9dfbe74652a4fe3d4d3b88948a73017a16e4940d46973e940232e69"}, "921bca7a-668b-409a-ad25-e3f04b3cc4ec": {"doc_hash": "21b83ee0834eb47bf551e064f509919f1423caeccad0225db57f28f23492fcd7"}, "013a546c-781c-4734-b316-8f1b3271edb8": {"doc_hash": "5904c3c49a3e66844720f2fabd8a77ffce56eafa7e85c6ffc903b5634c9b304f"}, "57b8399e-2bed-45ed-bc3e-57bb94ea2fc8": {"doc_hash": "875dc91f516a9fecad1e994ac3d5a5f1335193777092bc2f0aacb5c54728679d"}, "97d240b7-4ed6-4279-98e0-2cc3afa86524": {"doc_hash": "0240aa5765808cfdbae5a4e8e90cdf446a4a1ce8a4e652bb55c0f218a751a7b4"}, "8d812b50-3a71-491e-b361-a30b1036a4a9": {"doc_hash": "86636f19086c0d7fcbc20938b2e47233bbac3d534b0267040daaa44801e3754d"}, "e2a67bef-a949-46fc-ac65-7bdcc16566d3": {"doc_hash": "87c6c0608d96ca99545e6e9b7efb45e559972eaa0182ec884c2e5b9ac1033cc8"}, "02083bcc-7bb1-43f8-bfb1-f61efafa959a": {"doc_hash": "c8edd7d4f481bbd48470a0126b552d8496e662c4aee0ac86739a94cb97e7a606"}, "ccea3dca-d0a4-4c92-bef0-34cd17edca09": {"doc_hash": "9190a2705677aceedb9758b34b76e50c792a7a5e53550dba752e1e347f0c8416"}, "43ba82ed-d9c5-48e5-a435-11251cd31cf0": {"doc_hash": "9824d7ddf0b4b4343d287be422fc00dcf54aea9e91263e81738ef27b570e55a9"}, "dc51e147-8f17-430e-add5-69dcb9ecc197": {"doc_hash": "0904a81c388236c50ab29eca2f768ed9d44a6e28d9752a6c9fce20cc422421ce"}, "bffdf3e8-ab45-427b-8751-0001c80ad499": {"doc_hash": "86959f83ea85639966c000d9355c48fbcf1b6c3b251ca5fe774fffd173925272"}, "836466aa-7804-4b6c-8c8f-9ace9f971975": {"doc_hash": "7620a8f10c163250834ff0efa66702eb45020bd3f6fe5e3fa9e7348eec20759e"}, "704c6663-eadf-44ce-a249-f8a9748bf8c7": {"doc_hash": "b4b192ddae6d04c75027281c56c92afac53f07dddc8ba3f07af600b0711e2e56"}, "66e6121f-6154-4cab-ad5f-179959b99859": {"doc_hash": "edcf253bcbc93df57e8add2e73081473bd460139bed565c613bd31180f9400e6"}, "158b4fd4-84fa-46cb-ba88-cc6ce9af1377": {"doc_hash": "cb2af18599ebcdf8217ac2769bfffa6573f674cbea24972bfb904371bd3bb2fe"}, "1529b363-1ad3-4341-b83e-e21ba1ab453b": {"doc_hash": "53d13099f32cb8e2379345148cc8b8805f7a4d02c5710452841cfa68dd5e01bb"}, "b3340588-3f20-4dcb-80de-7ef76144940d": {"doc_hash": "c63bc6312b87209af0df7d26e42306f3d6bb71f9cec34ab971483bc99e25610b"}, "8ee95c9a-9392-4039-b857-011841488536": {"doc_hash": "ff16160cbb7ce1832b28a34ebfca112bf77288f0585c5a6779d4bc4daa3996f9"}, "f4ee1c8a-3e53-44a5-af15-43c0e0c155df": {"doc_hash": "acbe0348226df096e006ce7450f1ea3338bb34aa2ad7e16e86d0c554dff582ca"}, "5d3e9c11-2d62-4873-be37-dd8c745e2e4e": {"doc_hash": "eb4d73d4bcfb863b3a0ce0f801e6702fb276fe295a3645c4d909019631aded16"}, "1b14d0e2-118b-4c3c-86dc-d2bc8b126a26": {"doc_hash": "fac5b23775dcc8d5ef33a10fe540803774563a1338600592b85d58d9c1c417e8"}, "88cb42ce-2a10-45a2-a6db-d00847e1251c": {"doc_hash": "0a660b8ffaf98e51315c39fd528791a0d457c5235746c7ed75960a0c13da3cc5"}, "1163e26b-b26e-417b-a405-bffa47ae1dfa": {"doc_hash": "afbe23810d8f03e48eaaba0be680ca0a1354313d64d12846d247ff625b2c6d4b"}, "be0ae290-c25e-4b00-a405-6401e9e485e9": {"doc_hash": "8147d55bae8d2f9eec1847481b5ba96494ba7d33ad589de6e568b7122bb77181"}, "fb00f818-6704-4b93-8229-5a69a872387f": {"doc_hash": "3ca6753be2de72cc12cef105acedfb4f0c6fb3567d03e36d4b1525cf6ad174bd"}, "acb9bc96-3194-42ec-94d7-3216d5d4c7bd": {"doc_hash": "93423b2729a04cad7057503acdef08866ef826a992d42b1b5dccfae8d4b956a6"}, "1207bb23-3cc4-4ccf-8acd-e7fbf0dbde9c": {"doc_hash": "79544cd3a1c0ad64d3429b5ecf96a91c5ba05226d5fe2b115b48d89652ea020a"}, "5c26d7fb-a644-4547-bd3a-86589b716f75": {"doc_hash": "9bdd5f6ee694bd51c49be62df83ceecfcb9da62960a23bbe4a9354368e38023f"}, "35e98686-cc2b-4fc4-9aa9-5e0d2ce033e3": {"doc_hash": "aa15c482268f4ad3a84322effab379056cf7cb3cfc0bec128ce2657e4b292cdb"}, "66ec9c54-e70f-4f8f-9774-e291067e35cc": {"doc_hash": "7a984990f8498f83b18aa2799a9e5fc5750b58e2bf320066694dc0756f78ead6"}, "1f3abe10-a5a1-459e-915f-ccf799871a58": {"doc_hash": "7673708a4c1e996bb4372a3a5aa695f86b4524550b57ae49cbea0851040521e7"}, "e6d3dcf4-4ba5-4eef-8cf1-7dee1d6b622a": {"doc_hash": "c14d1835f0f8a53efc72da28a8549747fb5aa9c1672ed14ee1b47ac22e1ac756"}, "b939cf73-df5f-4b35-86a6-8bee5adb074b": {"doc_hash": "f3c0cfcec24a458414fdaea6eb81d5d17900e56289afc9013ab74321e4e96c6a"}, "6df73be9-77f3-4296-b9df-b20a1d1e3b6e": {"doc_hash": "02d4e2613c8f76d69a2db6cdc89b129b9c934d518db4e1bd62122e7daaaa2b2b"}, "36acd708-e0f9-49e6-a6fb-74218b35fa92": {"doc_hash": "b263c189fb9804199b68cb0313dca6930ab383f0cb878a28d97f23703c1180c5"}, "e71a8006-d0b0-4e85-b8b4-a1b1209a86f1": {"doc_hash": "37a150baa618cbf55f869a801f8d65c2aebb7663734aef3d63c970f9f89eff2e"}, "3a140472-3f96-4831-a7ee-0a8d9ea53dd6": {"doc_hash": "9dbbfa4bf5ef1c9bc822812ae704c6c8a380c027ec7a485841e7b17579c6b2cb"}, "e1d3c0c2-8fa4-435a-ab65-3e9cbbef7c2c": {"doc_hash": "7383ef572cf836d6a16d9285ac9f88068a32ae9585e7a194aa3cbc43d70e9f83"}, "1aacf9a1-a264-4d8c-ad62-4ed02ae7df37": {"doc_hash": "6443a2b7dc3068b9ead38d7ce9b65175a9633922ef8af0346718d7bed9ad455d"}, "cbead1e3-8b11-46c1-8d1c-49a9f8a3349c": {"doc_hash": "3b8f533656a0fc28c72df9415c48957d0902551835dc495d47abb6252ad3cdde"}, "700d1c7e-836f-4f29-bf39-1e714843a563": {"doc_hash": "0b0871080c73d42fa20fdb5eed0d299addb97ed49b0fb9361f8555eb7ce10190"}, "d7786585-6867-429c-9ddc-2acd93d53254": {"doc_hash": "54915eba257f7c45c45184902d95d8ccbfbb7125da4b3e49d75177f29769509f"}, "ced98744-aa5b-45bc-8fba-8adaf28df761": {"doc_hash": "1a066dc1e1821d7bd10837014cb93ffa147731a7a071fa360be0ef71d9eadb95"}, "6d528b76-e117-40de-ac36-9e604926f4c5": {"doc_hash": "7e407013d97cdcc61af4a7ae37d8e03d9b1984fd84eac4aa2eef603d28105338"}, "b185674b-5baf-49fb-88d6-4411302ae73d": {"doc_hash": "9151c565d1321750b80aee4158f3ed09ecd685f6d5c9be0731b033f1c3df5d44"}, "d73481fc-c3c2-4e7e-b776-32a0958e0574": {"doc_hash": "a559018c1423d9d711e91726df54eaed275732e9ab529dcea7d6b65b6582d034"}, "87873179-575f-41fd-9513-6bfdd647d1fb": {"doc_hash": "d689aeacc6414a86f056878467de96bc35ca6f3e4ecfc0fc6ed72b078dd84c48"}, "e23c8d66-ff75-4c09-aae8-c634e024b8f7": {"doc_hash": "7b16291df60cd9684173df2fda545673c20165db5ebfa1e9912463542fb6ffdb"}, "d35b2684-ad48-4be0-943c-09979736fad0": {"doc_hash": "2f462e978f1ed8ab1d8732cafb9d903eaec79279754ad43fae24ad7b764320a8"}, "62755857-a7ba-4aab-bfa3-b8336c238a7e": {"doc_hash": "c75e9336de404856ad8b0b70f40088e414604eb4be38276c3a4778ee66256a91"}, "db91506a-d180-47dc-9c99-1e35423a8894": {"doc_hash": "4f2dffcc24a957cc70ad7125cf163b2aafdea38d883def8ee6dd4d8a7efb660e"}, "e10369e5-eccd-4cca-a8e4-1e4e829f6e10": {"doc_hash": "ba1cd82a42519b2eb943796327e000ee3b53d99e182a7b7307e812b6b9483008"}, "13c717ad-ba15-463a-9bf6-25f490b025a3": {"doc_hash": "2fc6cb4cdfbea08a445f07c5af165d9904201cd44f352662fecab97f3a1fd29d", "ref_doc_id": "42dfc06b-bed4-4399-a4d5-585e7c3d8a9d"}, "09257282-d717-4e7f-b8ba-6054f3eb6e6f": {"doc_hash": "701e79f2c16b7cab2cc72e570cc873a5a6376262e5672850f8e62d4a1619e9fd", "ref_doc_id": "2c13a6b1-322e-413f-886d-15fb0dad4c30"}, "339c6e91-87aa-4a42-a4e4-14104d548941": {"doc_hash": "9b74892be46082ea53f623ce0c905bb16e7d80b26aab78f07d054f38e5855a01", "ref_doc_id": "c2c679f6-aa44-4219-b7b2-02b80385c7d4"}, "61e7e04e-0b15-40fe-9d07-806ed0a30f74": {"doc_hash": "b2fd28c68a7dda5807a6e3a8d71602a5c920971d70dcec03bc4bc4c3552c1be6", "ref_doc_id": "ff5a837d-9dac-4240-a556-379ad7c0fb63"}, "0e5eacef-d23e-4e25-ac93-322e17e40379": {"doc_hash": "cf07f413336a3cf9eba1ad2ca4688b7569caf3712203a6649edcaceaa0e167ef", "ref_doc_id": "bcbe17cb-d5a2-44cd-b718-cd59130b531e"}, "78fc2af1-a8b9-47ad-a011-f0d3b12ffb12": {"doc_hash": "ff079a9dbfe02ac2c4aaab6c80516f55331bded84ee32448f15c65c95e1ba2ab", "ref_doc_id": "a3356b4b-8302-4edb-8271-ab839ddf2b5a"}, "8f63876c-4705-40c2-a6ec-20e85593ac7a": {"doc_hash": "01f22f9288c5b4a93f8ae7611a253f2a2a288758226514d94103ed58aa375ac2", "ref_doc_id": "b5f15307-d81e-4764-874a-4061b39198f6"}, "c3ef254a-3da0-4f1c-9f18-5bdf1ecf2ca6": {"doc_hash": "8937b37272d256ae2e2a5fa8e4480a441c543147b6548212bd79e0f8b868f728", "ref_doc_id": "9a727675-6b4c-4065-bf0a-2c31152f7cb2"}, "87ed0bd6-70ac-470f-bfb5-f0b1620ee280": {"doc_hash": "0ed50a026ee830b703db712b55013a0a4a1c6ee73a359c925a3740036d675c79", "ref_doc_id": "ec1486e7-a8af-46f9-b662-61e579b2c00b"}, "6f34b1dd-be63-4574-8d45-27e98d5ec2e7": {"doc_hash": "d6c8e49e7698b3cb4c368f70b456339dd31be0635f6a37d5d8276c86c55b0d43", "ref_doc_id": "236cfce7-63f0-422a-9231-4f6cd1e49bfa"}, "f080200b-667b-474f-bb2d-4b322d5838d7": {"doc_hash": "af3b609f35cde44233658f603f0b014cad4d1b5bee9561b35fb49e73c5ed2217", "ref_doc_id": "40155453-fe17-4363-a9e6-cf3e7b94541b"}, "e080ee58-03f1-44bf-9487-7f672ae6880a": {"doc_hash": "d1057033f9bbbea2ccc2b4222097612a3ba43b3e914926174db59cc51352c7f0", "ref_doc_id": "463eb972-64fb-47d6-a299-e117425adc0d"}, "cf6fda56-25ab-4f3c-9a72-05c6d440bf16": {"doc_hash": "5843657329162d91e0e7615d19716081744a31621e4deb9a939993dd16a4f4e8", "ref_doc_id": "b14cf05b-cb8f-4706-98c7-5aa7838e6b00"}, "eb88094e-6ff4-4470-8002-7a1adce41a06": {"doc_hash": "49e03381ffc1a4c385b132b413a91e3857e807331bb20a208fc51792489dc6e2", "ref_doc_id": "f043dc5b-32a0-44b0-8003-76ef4c10d44e"}, "61a87f38-60a2-4e69-b999-e73b2e335de2": {"doc_hash": "b05917359018bab6a3aebc3482a927e384d823ae7b0910624fe8df32890ee601", "ref_doc_id": "3b495b58-2261-4c79-9ef7-56ef823b29ae"}, "c85028cd-14f7-4ea8-8e7a-a12da317ac27": {"doc_hash": "2ce8b42f925654085e4a8fc17c73aed396d95246b790618539bcb08c49847dd8", "ref_doc_id": "300fa914-1a49-4e28-be76-ffa821a9c22c"}, "e7ca454d-295b-46a2-9fbb-90681869df2a": {"doc_hash": "64caabec7e125bc1e68f7269762863b8ea66a2c3b294bb47efec438ac073d915", "ref_doc_id": "9e9ee98d-f957-414c-a7be-b407a3f0eff8"}, "8a57dad2-394d-4dad-ba35-de02065cc213": {"doc_hash": "b807f19673773b7868802dc0896b7439400c8187400b810b556a2bafd2ae922a", "ref_doc_id": "145d45b6-8460-4056-911a-382d6e4d322b"}, "e6ebfc45-f578-460e-8d9e-21c4042ce86d": {"doc_hash": "14184690a28a43818a27800fd36b5a4f5b0f7cbdcd9338f9781a65b03e4cf01d", "ref_doc_id": "9b562cbf-8e90-4b91-96ba-3094fe3fa559"}, "957fa270-d43a-482e-87de-8492a4674010": {"doc_hash": "684ed14b30b0fa9b27b5d4bd6ca3e85d25d7585d6a9ad53239f850caa56b930a", "ref_doc_id": "dc5316fe-3305-4445-9d15-30dd1f46117d"}, "afac65e7-07d0-4fb7-9722-af0d81399971": {"doc_hash": "e777cf54c24633ebc23c55c08a12d905716cba5e61d58f18989ce92bc60d2920", "ref_doc_id": "e8d1afae-2b38-41a4-a784-141fb81c6fd4"}, "09c3568d-e935-4c2e-baea-4a463ec3c6e9": {"doc_hash": "1d62f78ae8ba9795dd83c4cbe3a7c112ddb5c21c028a161f4b07607053b566f8", "ref_doc_id": "91528d1a-8dbc-41b0-922f-dfe21a9cca72"}, "8390815e-ba1b-49cf-a931-f9d8039fd2f2": {"doc_hash": "a86812a40d79aa0af45d7477316b44a70728d50d24e4e0f0529c09278e8c594e", "ref_doc_id": "6391795a-8c64-4f4c-a62e-922cfd00a928"}, "378763e0-901e-4403-b815-db458afaf31e": {"doc_hash": "c1bcf00b4fded462af7490fc98798018f56a88990b353a7a9f25128f96667edb", "ref_doc_id": "92edad98-3281-46f2-968f-5a6a78cde3e2"}, "171aaa29-6868-4a13-9a86-eecd0dac2811": {"doc_hash": "9c3c48b13aa81b1cfd403852424b32acb3342b7b3844e208ce844d23cc0c8e44", "ref_doc_id": "529005d4-ff38-410a-8c57-399b16b82484"}, "b721aae6-72f1-4b58-bc87-b6109094c0e1": {"doc_hash": "a846215a5505f84579c1a4a390914d48356cda71d79b93fc22fab9c8ce772874", "ref_doc_id": "3ae2fc96-95f8-4af2-b2d5-1f343bbd6cd8"}, "5043bf18-90e8-4d1c-bf6a-6915f07b3627": {"doc_hash": "fe2da8d451bff20ab434296b05c95712cb086718d7dc8e186edffaca63205c3b", "ref_doc_id": "05c91038-c4fd-45dc-bb30-fe11f6af60d8"}, "9f62ce4f-1e0f-4597-81df-aa14959f78f8": {"doc_hash": "37e44519da2fd3e08b46f72721e8df1dceb7b55f5bec1f0282d233a22a690352", "ref_doc_id": "f62527f3-faf3-4cb1-a502-bcb3672b72b2"}, "b96b7a7b-a003-4363-8382-49651fd7e713": {"doc_hash": "04e60b2a8b30a329a9d23933c8a2d753fb980a1a734ba4f0b4834eb88de2d961", "ref_doc_id": "896b708d-b98a-4393-bde9-b572afdd793f"}, "160e5079-55d8-4dd2-8bf5-c61f2331bcf3": {"doc_hash": "29593a5c02372efb8e575002be886e8f580cb429ed243511abfbe0fe9ed3c33d", "ref_doc_id": "2d714854-2f05-4db6-a0ae-010d0c7207dd"}, "b5b4c5d0-d013-4f2c-971a-dbc62905ea04": {"doc_hash": "896a22abdba986104f6c47ded1b656d552f9b6ea1c01a02bd952542fd352eecf", "ref_doc_id": "cac1dd84-9e09-4d57-8b7e-9f26057bd77e"}, "5b7ea7af-c383-4743-815e-c56b6fab8543": {"doc_hash": "6324d6aa39ec2d8510d529c46b83f84da0a426b2bb1ded8f729bbeadea8d31c4", "ref_doc_id": "e255d32d-2304-43e8-a73a-84dc3a995e17"}, "d28791c8-40ec-42d5-b583-c80855cf254e": {"doc_hash": "0e6819eaa1b49283e83a6c683dd3a60a62c9ea96a1b379b9cf2f6b96c36c305b", "ref_doc_id": "3a49a07b-e5f0-4d89-9143-35441564041b"}, "be60ad8c-2f1f-442d-9516-2264a1301be3": {"doc_hash": "fb8fa0d788bd76854a4c934f4c98950d36bfcd971f625948dbf7cc7727b0eedc", "ref_doc_id": "e6252d24-3ec6-433c-bf6c-71387eb1ef4c"}, "b80e2dda-07a0-445c-a92c-c45d51495ad5": {"doc_hash": "da89afaf1ab0362abcafcb75f9ee66fcfccbbe59701d4281a9ecdada4e134e42", "ref_doc_id": "231d88f9-4a99-41a2-9b70-2686e067d0d4"}, "c902e9a2-14e3-4f69-85c5-01da062b492d": {"doc_hash": "35c02fff9a4b0184ea2e7b67b8630a13da95a68cd244e92cec1cebe01c5c23b4", "ref_doc_id": "77bca458-a819-4b0e-ac9b-740545893ab4"}, "4802546c-8ee0-4757-abe6-33c7b68425af": {"doc_hash": "5004cc66518cc7e20942a0f9b6eade6cf799acc02797fdb624f9f5c03a5cad36", "ref_doc_id": "1aa2b63d-c630-48f0-9759-a1a20df01941"}, "c5f6b640-7eb9-407c-9380-262be5fcc394": {"doc_hash": "67a8263d278e18cd7e06ad932dcc37df49818c231791b9fbd5d064aad77be7f7", "ref_doc_id": "57646547-3bed-458b-9a97-9d67154c7286"}, "552b268a-2e01-49f8-90c3-ce7149701707": {"doc_hash": "eac08a297169a896aceeddf052d46bf59225887b0428ce6b4a86b884755c8c69", "ref_doc_id": "22ee1e88-5647-465a-b62c-61af6cdad8b4"}, "fb43f2b9-c458-4c8a-9d71-e10e350b13c0": {"doc_hash": "29dfecc692e7cd59cc783d5c0b495d78faf32403f0158b3e41f9fd5150dde1ac", "ref_doc_id": "8b6851e6-3986-426b-a835-95a001537067"}, "bd3adb36-d586-4f85-a2a0-6863cf86b86f": {"doc_hash": "a17e651f185b04e2bf219687706c92f96a09a13006b56cef2e9b0149677c3f54", "ref_doc_id": "eca461b2-31dd-49f4-8e89-684463bb7d30"}, "5c2684d3-8221-4508-8569-270867c838be": {"doc_hash": "936ce594b501a79b54d8f0eb3c83758a7b42f67688f4cd0859b3f8aa6470ee2f", "ref_doc_id": "ff04144a-ff21-400c-a57a-dc4a8e83c8dc"}, "70bde962-238f-4dc4-8114-9cc0201d4853": {"doc_hash": "dfd0a9495d1a67767988e83d0a540d53610b03f2afeb1b17bc21b263a8e3d0d1", "ref_doc_id": "bee74d58-9e81-4711-ade1-92cc2922b28d"}, "ec72f00c-b906-488c-ab23-0f243b45ea9d": {"doc_hash": "a6725ca2d2f9a903e99e8b202f79e6f80157c379f1361427dda0d246d853e580", "ref_doc_id": "166efef4-6439-467e-bbbf-bc4c9959597d"}, "ff02cec2-25b4-4f21-b701-dae6fb6d791c": {"doc_hash": "8a696dab9e378edbabfba0c17a42de5641f668fd98f6225af1ddfe922738b6c0", "ref_doc_id": "16f5ca3f-72d2-42e7-a58b-5efa196c2798"}, "84547772-9245-4382-8c65-836bd343c163": {"doc_hash": "032e560a50ca1f7d58ca378f282f03dd1f0b99bf9c08ee2484fb8913c062f752", "ref_doc_id": "952adb7b-a96b-47ae-838a-13381806395a"}, "12ba2ef1-b1c8-4169-ac3d-0f26ea3b916b": {"doc_hash": "142b91c0f7e47ec90be5c07052f9825cb00b34bafb4bef59607f83089ffb7697", "ref_doc_id": "72f70266-cc7c-4783-bd8f-54e3da726b64"}, "daea85b6-9b80-452a-b059-9882c8c05669": {"doc_hash": "c4a92abf50b94fc8b1c7a9565e0c665456875a30d287209c3af84d604581a881", "ref_doc_id": "dad7c135-9a03-4c63-aaa8-e6c056775732"}, "77ebf0ba-23af-4770-a416-870596cbcb94": {"doc_hash": "4929b881d8e67c6ac7f596dac15dc02df1f8dac8f89cb007646350dd92dd8a0d", "ref_doc_id": "a05dd0f5-f3e3-4614-b721-c22c184e4171"}, "dc09360e-c541-48ca-b29d-e975d0220eed": {"doc_hash": "d9809f47829aedac6e8676ec3c18cd98674f1a356ce79eaff7aff9ccbc39263f", "ref_doc_id": "3b66cd3f-b0a9-46b2-9169-efd9f5ab88b3"}, "6230913a-b347-4e8a-99d3-14b30e9476f0": {"doc_hash": "6c61d331178525a4ab2295933e1866db97aaa6642f492aab1100b7b11e91c07e", "ref_doc_id": "8337b4ab-37fd-4c5c-94dd-e73ae77bf82b"}, "d92335ae-48ba-4e2e-999e-f956a6c15a95": {"doc_hash": "61b771387aa7f5734ccee679ee82883002e6e0e838499402ddf7355e7a5bae89", "ref_doc_id": "5b7cb74b-91ca-4cba-9b0a-4ea45ec07018"}, "8819ea56-9a5e-4ae5-a4dd-d5a772861dd4": {"doc_hash": "aa34b8a7ee63e6995b025cf39a340a1634f76c8f8d4f749c80960bca59bb4b7a", "ref_doc_id": "17700155-9b95-4044-8bdd-56f6a25b41c5"}, "3a3d8a86-f212-4b9b-afa1-4eae5e2f8459": {"doc_hash": "79179e9cf6b937607f938098b7591e48b608b51f236704a9417ad762724286e2", "ref_doc_id": "04f346ca-c21f-4cbc-b462-70b60b75c872"}, "c100e789-01e9-4636-8efd-ad7064a5b64c": {"doc_hash": "7d40f3716e08f2a0218068058c0bb827d02b0994361b3cc5485cd547361e1e49", "ref_doc_id": "f8564556-755a-4f62-a64a-b923a7b64262"}, "6504eceb-ebe5-4edd-98f4-5cd88b2e0691": {"doc_hash": "8c642f970560ca8bf3df45b9c6cab6b98ef1709aebc6b583eb15595a4f968ed6", "ref_doc_id": "62936da7-cbd5-44bd-8952-1899d9bd1736"}, "0d3061b7-a8a4-4983-a95a-13ed81e98d18": {"doc_hash": "acb4e35ec8dc73988ba79d5d3eaac6e1c7cc8ac3e8bd3da705e851f5751ccbe9", "ref_doc_id": "eac05390-70d0-49cc-be8e-3c0c617a38ca"}, "54371945-0542-4347-9775-9123bf3fa463": {"doc_hash": "ebac0a5cd8519a5fb4aa931b342af4d94e67d07efd459647c6f960007a5b2292", "ref_doc_id": "11c5b1e0-d02a-48eb-ab94-04a17e350133"}, "b9dd4991-8ee5-4f26-b5c0-6585063741fb": {"doc_hash": "4cac14dd19bd3a0186cdef32defb608b437feede10eeef557abbef401590219d", "ref_doc_id": "82b97858-dab1-4527-bc5f-56061962431e"}, "94dc798e-abf9-4c56-ac23-42d5448aa4e2": {"doc_hash": "2644f5d679643dd76ca9ec45129cd54e846c20ba418060b0c17ae42873dd8251", "ref_doc_id": "beebe7be-037d-4c1f-abfa-6a5b6f3cf974"}, "51dd81db-2ce1-4c68-93fc-df03f52c2d15": {"doc_hash": "1851fd09b66277ab9e32b9c571841832f9be5a434e85adab63592b8c139ab168", "ref_doc_id": "f685bc46-fec8-417a-b9f1-5363da5466f9"}, "f8e27359-9c32-4dea-88fc-ba574ed60b37": {"doc_hash": "6bcf8522af70572ea41cc421923d27be415b6a5ecbac832dc76dd79997d369be", "ref_doc_id": "ab2e30c6-ceb5-48e0-bb32-0bb147deec11"}, "5a7023ac-ef10-4246-9abf-ae044c30701c": {"doc_hash": "c8ad9e3354e5a3484e7fd2736ed8cad0e0b4abf7742ec7d975a6e4ec605c26d1", "ref_doc_id": "93c3cf00-5dab-4807-b566-2173f8e32b19"}, "4344d8c1-7033-4e68-bfc9-11ebc7912263": {"doc_hash": "dba8f6fe650735ae23f52945a029c204d1583c9d94a7163cdcb6e6504067c6cf", "ref_doc_id": "c5948655-dac7-4e52-91a7-9184c2c27339"}, "8788a6ea-b5c8-4b29-8336-415514881aff": {"doc_hash": "6864eede04f945939581da24e43a82d4ed9f92dbd7ed00afcdb9931915a6de35", "ref_doc_id": "dbf10e25-8ce0-4cb4-bdd8-bea644d79cf6"}, "0e466327-b3fb-4d2f-b7ef-2caea8fb064a": {"doc_hash": "0d8fd36eed20f500528b0492b662bca7058a12c5512535470b719a1786d95374", "ref_doc_id": "1a618400-0ee9-45f9-a8ae-5d06524711ac"}, "c2bd1df4-508e-4184-857b-2fbdaa7f8812": {"doc_hash": "46a4e43192d992ed9ea4a65d3117ec2bc026d6cb2097d0676b8894f46ce1eb45", "ref_doc_id": "4aba9541-cccd-49e5-b2ac-e250522d9618"}, "ff0a2bfd-77d4-4ed0-88e6-ec1699e4375e": {"doc_hash": "61cc6a944159aba960d04fde6c3b744f8ffccc3e1ebae31722e8dda5000938bb", "ref_doc_id": "fa1c767d-4e6b-4a38-b356-672d353b0e17"}, "0af700ff-94de-4a09-acce-ff9436f1e0f1": {"doc_hash": "4369ae43db7023576b8aca0a1e7c66495bef76ce1f77c6e6d22d03fa9110bd32", "ref_doc_id": "4ef05452-1812-44b9-90ae-7fea2cdd255a"}, "84f69743-60b0-4e4e-8812-87c86274e7e5": {"doc_hash": "bca2e4fb5bba54805042737423b27fadb593e042f0352c5fecca4b292f493140", "ref_doc_id": "3e69a925-8276-4828-8be8-47e81923fca0"}, "7c5d768e-c814-4553-9e41-702fc4addd71": {"doc_hash": "5165bf34dafe522623231db25e73222292d33aa16dc5c6bbc876801d0b709764", "ref_doc_id": "73a89634-2cb5-4c16-a73b-89ad22fcb4ed"}, "d446ef3c-9ec8-4f4d-979e-6d99bac96e05": {"doc_hash": "b4331f9982a6b87c83a7ae4f8f9f97708b5da081651c323d1187694912b10dfd", "ref_doc_id": "4b16ebc7-bc2e-47e4-b6cb-043478a5b937"}, "b6519a99-6eb7-439f-8d2b-f0ae36f643f1": {"doc_hash": "03ca0f637cf47c1527561a89cf0f10a50349825a8601d82f45ee4aa32ff91584", "ref_doc_id": "f83e2fd0-70ac-4676-8137-970b4fa47ea0"}, "158146fd-2925-4430-9c4d-a7d15b06900a": {"doc_hash": "1edebc027e57dc71821fe2aff665b83c4f36d82d68c4606d5b1136cae1292b02", "ref_doc_id": "8b62eb6c-9ca7-4889-b8e5-96ece790887d"}, "b08db4ef-854f-412b-9eea-6de6b74341f2": {"doc_hash": "6a0d962fd2628fa03a7bb4b76a43f2794bd7247ab3678431b84215e3c5637e64", "ref_doc_id": "f2de2223-c69c-450a-a273-63b345aa8043"}, "7d8f1905-1e73-402c-a430-0a773fe11a12": {"doc_hash": "64801ef9ca5cc8264daff304ecd0c1d79ed827e3a71208f519e04bfce945d90c", "ref_doc_id": "02c012ff-ec10-4a3d-8f41-f60e812ac611"}, "4e5b1d44-088b-400e-b864-b86d851a2e0e": {"doc_hash": "03de14ff41951cd3ad503d5f9a733ecd82780c2f467530d6a638bd4610edc0df", "ref_doc_id": "671a4e26-0453-4a71-9a17-08cd9faca12c"}, "82d812b8-e543-4d5d-94c5-6891b9c3794c": {"doc_hash": "8fca2c42b48bb27d64162cf4fa5f5985d549bfce3a0ff14fb0dba6b867d512da", "ref_doc_id": "84dc5868-0544-492d-9926-8ebcce33a25d"}, "4a8af527-96b6-4b97-a8c9-00dcaee75ec0": {"doc_hash": "a864b07f328e4a96e8756da7d93273d44922d1c25c66f6af1d0e5827202166e2", "ref_doc_id": "a2b046e6-16b2-4143-ac1a-e10772a9b55a"}, "280f327d-d51f-443c-abd9-7c6dd0a88929": {"doc_hash": "ef4951e8ce567f6afc1c55d83f72090c394ab4d47d11161277806ca02f71bd10", "ref_doc_id": "cb66a87e-c56c-40a2-b8a1-4a75ea693a26"}, "29586fdf-d098-40a1-9119-5ba5880435d1": {"doc_hash": "f1169922e06d835c6ea22cc123d34906a286c709527a14361e8ecb4c436daa06", "ref_doc_id": "83f9605f-2565-4e30-8d4e-14d2d3b22650"}, "8b6d9912-c11b-4511-a5b2-69e2d609a437": {"doc_hash": "f61bdce7a6efdd6beac82353a28f3540b38db2c7bf2b2519846ed887d61fe46f", "ref_doc_id": "500061dc-92f7-4365-8406-99234abea413"}, "664ad79e-23ce-4054-9fb4-3a96e2927e86": {"doc_hash": "66d9ec1ce9dfbe74652a4fe3d4d3b88948a73017a16e4940d46973e940232e69", "ref_doc_id": "63e3323b-29c5-437a-a896-71540232be79"}, "db0c9cee-c83e-4a0e-9e1d-cce392154337": {"doc_hash": "21b83ee0834eb47bf551e064f509919f1423caeccad0225db57f28f23492fcd7", "ref_doc_id": "921bca7a-668b-409a-ad25-e3f04b3cc4ec"}, "17ab93d6-753c-401e-82fe-1e291fc890f9": {"doc_hash": "5904c3c49a3e66844720f2fabd8a77ffce56eafa7e85c6ffc903b5634c9b304f", "ref_doc_id": "013a546c-781c-4734-b316-8f1b3271edb8"}, "4c8c2f8a-d6a9-46f1-8755-67c16356aa44": {"doc_hash": "875dc91f516a9fecad1e994ac3d5a5f1335193777092bc2f0aacb5c54728679d", "ref_doc_id": "57b8399e-2bed-45ed-bc3e-57bb94ea2fc8"}, "6e67c100-6106-4f4b-ae96-7c43c6a418f3": {"doc_hash": "0240aa5765808cfdbae5a4e8e90cdf446a4a1ce8a4e652bb55c0f218a751a7b4", "ref_doc_id": "97d240b7-4ed6-4279-98e0-2cc3afa86524"}, "614cff0d-f071-40c4-9704-2cfbbbce903f": {"doc_hash": "86636f19086c0d7fcbc20938b2e47233bbac3d534b0267040daaa44801e3754d", "ref_doc_id": "8d812b50-3a71-491e-b361-a30b1036a4a9"}, "3cab4973-9e01-40af-ba9e-882bdd6ed504": {"doc_hash": "87c6c0608d96ca99545e6e9b7efb45e559972eaa0182ec884c2e5b9ac1033cc8", "ref_doc_id": "e2a67bef-a949-46fc-ac65-7bdcc16566d3"}, "903d10a6-d5dd-408e-9833-8f04cb28c663": {"doc_hash": "c8edd7d4f481bbd48470a0126b552d8496e662c4aee0ac86739a94cb97e7a606", "ref_doc_id": "02083bcc-7bb1-43f8-bfb1-f61efafa959a"}, "f1e70401-da95-483f-98d7-c56e4cd89f67": {"doc_hash": "9190a2705677aceedb9758b34b76e50c792a7a5e53550dba752e1e347f0c8416", "ref_doc_id": "ccea3dca-d0a4-4c92-bef0-34cd17edca09"}, "98749495-f5fc-465f-a4e8-b2d791d58b3a": {"doc_hash": "9824d7ddf0b4b4343d287be422fc00dcf54aea9e91263e81738ef27b570e55a9", "ref_doc_id": "43ba82ed-d9c5-48e5-a435-11251cd31cf0"}, "8722d910-24e8-454b-b1f5-29edf3e09175": {"doc_hash": "0904a81c388236c50ab29eca2f768ed9d44a6e28d9752a6c9fce20cc422421ce", "ref_doc_id": "dc51e147-8f17-430e-add5-69dcb9ecc197"}, "c2f8ef3a-4caa-4109-9229-a1274e0e8f6a": {"doc_hash": "86959f83ea85639966c000d9355c48fbcf1b6c3b251ca5fe774fffd173925272", "ref_doc_id": "bffdf3e8-ab45-427b-8751-0001c80ad499"}, "920d3e7f-6893-4d67-b475-292e6cd2aa89": {"doc_hash": "7620a8f10c163250834ff0efa66702eb45020bd3f6fe5e3fa9e7348eec20759e", "ref_doc_id": "836466aa-7804-4b6c-8c8f-9ace9f971975"}, "882b111b-2fb7-4160-9ace-1add16437a1d": {"doc_hash": "b4b192ddae6d04c75027281c56c92afac53f07dddc8ba3f07af600b0711e2e56", "ref_doc_id": "704c6663-eadf-44ce-a249-f8a9748bf8c7"}, "3708e3bb-83f0-486c-be7f-ede014ac9f87": {"doc_hash": "edcf253bcbc93df57e8add2e73081473bd460139bed565c613bd31180f9400e6", "ref_doc_id": "66e6121f-6154-4cab-ad5f-179959b99859"}, "ce9d2fef-bddc-456d-9877-79429b6e21c8": {"doc_hash": "cb2af18599ebcdf8217ac2769bfffa6573f674cbea24972bfb904371bd3bb2fe", "ref_doc_id": "158b4fd4-84fa-46cb-ba88-cc6ce9af1377"}, "f8a06d2d-0eaa-4bd5-a0e6-9bdcfd3adc4d": {"doc_hash": "53d13099f32cb8e2379345148cc8b8805f7a4d02c5710452841cfa68dd5e01bb", "ref_doc_id": "1529b363-1ad3-4341-b83e-e21ba1ab453b"}, "c95d4283-c882-4d33-9969-0d1954aa35fe": {"doc_hash": "c63bc6312b87209af0df7d26e42306f3d6bb71f9cec34ab971483bc99e25610b", "ref_doc_id": "b3340588-3f20-4dcb-80de-7ef76144940d"}, "cdba7ae4-5656-42f8-b574-c0645f15f604": {"doc_hash": "ff16160cbb7ce1832b28a34ebfca112bf77288f0585c5a6779d4bc4daa3996f9", "ref_doc_id": "8ee95c9a-9392-4039-b857-011841488536"}, "5638b917-863e-4c6f-83e6-c7a962868021": {"doc_hash": "acbe0348226df096e006ce7450f1ea3338bb34aa2ad7e16e86d0c554dff582ca", "ref_doc_id": "f4ee1c8a-3e53-44a5-af15-43c0e0c155df"}, "b52fe620-5fcb-417a-8c21-22d6042a0e65": {"doc_hash": "eb4d73d4bcfb863b3a0ce0f801e6702fb276fe295a3645c4d909019631aded16", "ref_doc_id": "5d3e9c11-2d62-4873-be37-dd8c745e2e4e"}, "cde656c5-9dba-41e3-a2d6-66db017044ad": {"doc_hash": "fac5b23775dcc8d5ef33a10fe540803774563a1338600592b85d58d9c1c417e8", "ref_doc_id": "1b14d0e2-118b-4c3c-86dc-d2bc8b126a26"}, "8796bac7-75aa-408b-9b9a-40e055321fec": {"doc_hash": "0a660b8ffaf98e51315c39fd528791a0d457c5235746c7ed75960a0c13da3cc5", "ref_doc_id": "88cb42ce-2a10-45a2-a6db-d00847e1251c"}, "76fe4eb1-3815-42bc-be6c-3cc1198294ad": {"doc_hash": "afbe23810d8f03e48eaaba0be680ca0a1354313d64d12846d247ff625b2c6d4b", "ref_doc_id": "1163e26b-b26e-417b-a405-bffa47ae1dfa"}, "75e19981-a089-4f42-ad13-5f3aa9e8635e": {"doc_hash": "8147d55bae8d2f9eec1847481b5ba96494ba7d33ad589de6e568b7122bb77181", "ref_doc_id": "be0ae290-c25e-4b00-a405-6401e9e485e9"}, "d52acce8-77d5-454d-ad59-17ba89909c4f": {"doc_hash": "3ca6753be2de72cc12cef105acedfb4f0c6fb3567d03e36d4b1525cf6ad174bd", "ref_doc_id": "fb00f818-6704-4b93-8229-5a69a872387f"}, "ed2815a6-0544-4727-ba7e-2e4b44d83b0d": {"doc_hash": "93423b2729a04cad7057503acdef08866ef826a992d42b1b5dccfae8d4b956a6", "ref_doc_id": "acb9bc96-3194-42ec-94d7-3216d5d4c7bd"}, "3297d0df-bdac-4205-8d02-636e82965685": {"doc_hash": "79544cd3a1c0ad64d3429b5ecf96a91c5ba05226d5fe2b115b48d89652ea020a", "ref_doc_id": "1207bb23-3cc4-4ccf-8acd-e7fbf0dbde9c"}, "0cf6e7f5-5659-4230-9c66-0714b781e6e2": {"doc_hash": "9bdd5f6ee694bd51c49be62df83ceecfcb9da62960a23bbe4a9354368e38023f", "ref_doc_id": "5c26d7fb-a644-4547-bd3a-86589b716f75"}, "cc840564-39ae-4273-b4b2-3cc78a887177": {"doc_hash": "aa15c482268f4ad3a84322effab379056cf7cb3cfc0bec128ce2657e4b292cdb", "ref_doc_id": "35e98686-cc2b-4fc4-9aa9-5e0d2ce033e3"}, "9949ef84-c95b-4021-a186-09be6e6ffd3c": {"doc_hash": "7a984990f8498f83b18aa2799a9e5fc5750b58e2bf320066694dc0756f78ead6", "ref_doc_id": "66ec9c54-e70f-4f8f-9774-e291067e35cc"}, "7983bfc7-6ae6-4fbd-8762-d1b320e07bd6": {"doc_hash": "7673708a4c1e996bb4372a3a5aa695f86b4524550b57ae49cbea0851040521e7", "ref_doc_id": "1f3abe10-a5a1-459e-915f-ccf799871a58"}, "d529e27b-abd6-4808-82da-ec8daeebd7e5": {"doc_hash": "c14d1835f0f8a53efc72da28a8549747fb5aa9c1672ed14ee1b47ac22e1ac756", "ref_doc_id": "e6d3dcf4-4ba5-4eef-8cf1-7dee1d6b622a"}, "9036412e-c813-4d74-ad6f-d408795e9e22": {"doc_hash": "f3c0cfcec24a458414fdaea6eb81d5d17900e56289afc9013ab74321e4e96c6a", "ref_doc_id": "b939cf73-df5f-4b35-86a6-8bee5adb074b"}, "a4335725-15bb-47b3-8afc-4a2d3b31c7bd": {"doc_hash": "02d4e2613c8f76d69a2db6cdc89b129b9c934d518db4e1bd62122e7daaaa2b2b", "ref_doc_id": "6df73be9-77f3-4296-b9df-b20a1d1e3b6e"}, "e5552675-92cd-408f-bf76-7b5d271b2374": {"doc_hash": "b263c189fb9804199b68cb0313dca6930ab383f0cb878a28d97f23703c1180c5", "ref_doc_id": "36acd708-e0f9-49e6-a6fb-74218b35fa92"}, "d20653fb-2fed-43da-9e36-8b6b6ad2a265": {"doc_hash": "37a150baa618cbf55f869a801f8d65c2aebb7663734aef3d63c970f9f89eff2e", "ref_doc_id": "e71a8006-d0b0-4e85-b8b4-a1b1209a86f1"}, "c88faa36-2a24-4b24-8a0c-3ddc63d49193": {"doc_hash": "9dbbfa4bf5ef1c9bc822812ae704c6c8a380c027ec7a485841e7b17579c6b2cb", "ref_doc_id": "3a140472-3f96-4831-a7ee-0a8d9ea53dd6"}, "c6e54bac-f0bb-47f2-b803-0c23b71cf13e": {"doc_hash": "7383ef572cf836d6a16d9285ac9f88068a32ae9585e7a194aa3cbc43d70e9f83", "ref_doc_id": "e1d3c0c2-8fa4-435a-ab65-3e9cbbef7c2c"}, "634f5e64-ea86-4a55-a803-ed6c47e7f121": {"doc_hash": "6443a2b7dc3068b9ead38d7ce9b65175a9633922ef8af0346718d7bed9ad455d", "ref_doc_id": "1aacf9a1-a264-4d8c-ad62-4ed02ae7df37"}, "93d52f67-c9a2-400f-b662-9e73f7ebb849": {"doc_hash": "3b8f533656a0fc28c72df9415c48957d0902551835dc495d47abb6252ad3cdde", "ref_doc_id": "cbead1e3-8b11-46c1-8d1c-49a9f8a3349c"}, "3a37ddfb-990f-4fac-8584-a910dd65b34f": {"doc_hash": "0b0871080c73d42fa20fdb5eed0d299addb97ed49b0fb9361f8555eb7ce10190", "ref_doc_id": "700d1c7e-836f-4f29-bf39-1e714843a563"}, "40dd251d-e054-4b84-9c97-4d371ee085ec": {"doc_hash": "54915eba257f7c45c45184902d95d8ccbfbb7125da4b3e49d75177f29769509f", "ref_doc_id": "d7786585-6867-429c-9ddc-2acd93d53254"}, "31ff7da7-0539-48d4-9fd7-501cd97e96bb": {"doc_hash": "1a066dc1e1821d7bd10837014cb93ffa147731a7a071fa360be0ef71d9eadb95", "ref_doc_id": "ced98744-aa5b-45bc-8fba-8adaf28df761"}, "e6146f99-597a-49ab-9af9-89f7622b9f7a": {"doc_hash": "7e407013d97cdcc61af4a7ae37d8e03d9b1984fd84eac4aa2eef603d28105338", "ref_doc_id": "6d528b76-e117-40de-ac36-9e604926f4c5"}, "44248835-4d68-4f3e-b70e-9a87b856747c": {"doc_hash": "9151c565d1321750b80aee4158f3ed09ecd685f6d5c9be0731b033f1c3df5d44", "ref_doc_id": "b185674b-5baf-49fb-88d6-4411302ae73d"}, "2d304acc-89e9-4414-a422-1409fec55507": {"doc_hash": "a559018c1423d9d711e91726df54eaed275732e9ab529dcea7d6b65b6582d034", "ref_doc_id": "d73481fc-c3c2-4e7e-b776-32a0958e0574"}, "533ea260-62dc-4fcd-a029-63d208dea861": {"doc_hash": "d689aeacc6414a86f056878467de96bc35ca6f3e4ecfc0fc6ed72b078dd84c48", "ref_doc_id": "87873179-575f-41fd-9513-6bfdd647d1fb"}, "ea51a011-aa38-47f7-a153-44ae35d82a79": {"doc_hash": "7b16291df60cd9684173df2fda545673c20165db5ebfa1e9912463542fb6ffdb", "ref_doc_id": "e23c8d66-ff75-4c09-aae8-c634e024b8f7"}, "c0a111d9-262c-4d36-83e7-2e92be7a017e": {"doc_hash": "2f462e978f1ed8ab1d8732cafb9d903eaec79279754ad43fae24ad7b764320a8", "ref_doc_id": "d35b2684-ad48-4be0-943c-09979736fad0"}, "efa2f2e1-3999-4cab-96e6-1e9d478f11db": {"doc_hash": "c75e9336de404856ad8b0b70f40088e414604eb4be38276c3a4778ee66256a91", "ref_doc_id": "62755857-a7ba-4aab-bfa3-b8336c238a7e"}, "636a79f7-22eb-488c-ba38-9aa470f16466": {"doc_hash": "4f2dffcc24a957cc70ad7125cf163b2aafdea38d883def8ee6dd4d8a7efb660e", "ref_doc_id": "db91506a-d180-47dc-9c99-1e35423a8894"}, "a37df407-add8-460e-8ca1-9c4a620269bb": {"doc_hash": "ba1cd82a42519b2eb943796327e000ee3b53d99e182a7b7307e812b6b9483008", "ref_doc_id": "e10369e5-eccd-4cca-a8e4-1e4e829f6e10"}}, "docstore/data": {"13c717ad-ba15-463a-9bf6-25f490b025a3": {"__data__": {"id_": "13c717ad-ba15-463a-9bf6-25f490b025a3", "embedding": null, "metadata": {"page_label": "1", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "42dfc06b-bed4-4399-a4d5-585e7c3d8a9d", "node_type": "4", "metadata": {"page_label": "1", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "2fc6cb4cdfbea08a445f07c5af165d9904201cd44f352662fecab97f3a1fd29d", "class_name": "RelatedNodeInfo"}}, "text": "THE HUNDRED-PAGE \nMACHINE LEARNING   \nBOOKAndriy Burkov's", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 57, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "09257282-d717-4e7f-b8ba-6054f3eb6e6f": {"__data__": {"id_": "09257282-d717-4e7f-b8ba-6054f3eb6e6f", "embedding": null, "metadata": {"page_label": "2", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c13a6b1-322e-413f-886d-15fb0dad4c30", "node_type": "4", "metadata": {"page_label": "2", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "701e79f2c16b7cab2cc72e570cc873a5a6376262e5672850f8e62d4a1619e9fd", "class_name": "RelatedNodeInfo"}}, "text": "Preface\nLet\u2019s start by telling the truth: machines don\u2019t learn. What a typical \u201clearning machine\u201d\ndoes, is \ufb01nding a mathematical formula, which, when applied to a collection of inputs (called\n\u201ctraining data\u201d), produces the desired outputs. This mathematical formula also generates the\ncorrect outputs for most other inputs (distinct from the training data) on the condition that\nthose inputs come from the same or a similar statistical distribution as the one the training\ndata was drawn from.\nWhy isn\u2019t that learning? Because if you slightly distort the inputs, the output is very likely\nto become completely wrong. It\u2019s not how learning in animals works. If you learned to play\na video game by looking straight at the screen, you would still be a good player if someone\nrotates the screen slightly. A machine learning algorithm, if it was trained by \u201clooking\u201d\nstraight at the screen, unless it was also trained to recognize rotation, will fail to play the\ngame on a rotated screen.\nSo why the name \u201cmachine learning\u201d then? The reason, as is often the case, is marketing:\nArthur Samuel, an American pioneer in the \ufb01eld of computer gaming and arti\ufb01cial intelligence,\ncoined the term in 1959 while at IBM. Similarly to how in the 2010s IBM tried to market\nthe term \u201ccognitive computing\u201d to stand out from competition, in the 1960s, IBM used the\nnew cool term \u201cmachine learning\u201d to attract both clients and talented employees.\nAs you can see, just like arti\ufb01cial intelligence is not intelligence, machine learning is not\nlearning. However, machine learning is a universally recognized term that usually refers\nto the science and engineering of building machines capable of doing various useful things\nwithout being explicitly programmed to do so. So, the word \u201clearning\u201d in the term is used\nby analogy with the learning in animals rather than literally.\nWho This Book is For\nThis book contains only those parts of the vast body of material on machine learning developed\nsince the 1960s that have proven to have a signi\ufb01cant practical value. A beginner in machine\nlearning will \ufb01nd in this book just enough details to get a comfortable level of understanding\nof the \ufb01eld and start asking the right questions.\nPractitioners with experience can use this book as a collection of directions for further\nself-improvement. The book also comes in handy when brainstorming at the beginning of a\nproject, when you try to answer the question whether a given technical or business problem\nis \u201cmachine-learnable\u201d and, if yes, which techniques you should try to solve it.\nHow to Use This Book\nIf you are about to start learning machine learning, you should read this book from the\nbeginning to the end. (It\u2019s just a hundred pages, not a big deal.) If you are interested in a\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 3", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2821, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "339c6e91-87aa-4a42-a4e4-14104d548941": {"__data__": {"id_": "339c6e91-87aa-4a42-a4e4-14104d548941", "embedding": null, "metadata": {"page_label": "3", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c2c679f6-aa44-4219-b7b2-02b80385c7d4", "node_type": "4", "metadata": {"page_label": "3", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "9b74892be46082ea53f623ce0c905bb16e7d80b26aab78f07d054f38e5855a01", "class_name": "RelatedNodeInfo"}}, "text": "1 Introduction\n1.1 What is Machine Learning\nMachine learning is a sub\ufb01eld of computer science that is concerned with building algorithms\nwhich, to be useful, rely on a collection of examples of some phenomenon. These examples\ncan come from nature, be handcrafted by humans or generated by another algorithm.\nMachine learning can also be de\ufb01ned as the process of solving a practical problem by 1)\ngathering a dataset, and 2) algorithmically building a statistical model based on that dataset.\nThat statistical model is assumed to be used somehow to solve the practical problem.\nTo save keystrokes, I use the terms \u201clearning\u201d and \u201cmachine learning\u201d interchangeably.\n1.2 Types of Learning\nLearning can be supervised, semi-supervised, unsupervised and reinforcement.\n1.2.1 Supervised Learning\nInsupervised learning1, thedataset is the collection of labeled examples {(xi, yi)}N\ni=1.\nEach element xiamong Nis called a feature vector . A feature vector is a vector in which\neach dimension j= 1, . . . , Dcontains a value that describes the example somehow. That\nvalue is called a feature and is denoted as x(j). For instance, if each example xin our\ncollection represents a person, then the \ufb01rst feature, x(1), could contain height in cm, the\nsecond feature, x(2), could contain weight in kg, x(3)could contain gender, and so on. For all\nexamples in the dataset, the feature at position jin the feature vector always contains the\nsame kind of information. It means that if x(2)\nicontains weight in kg in some example xi,\nthenx(2)\nkwill also contain weight in kg in every example xk,k= 1, . . . , N. The label yican\nbe either an element belonging to a \ufb01nite set of classes{1,2, . . . , C}, or a real number, or a\nmore complex structure, like a vector, a matrix, a tree, or a graph. Unless otherwise stated,\nin this book yiis either one of a \ufb01nite set of classes or a real number2. You can see a class as\na category to which an example belongs. For instance, if your examples are email messages\nand your problem is spam detection, then you have two classes {spam, not _spam}.\nThe goal of a supervised learning algorithm is to use the dataset to produce a model\nthat takes a feature vector xas input and outputs information that allows deducing the label\nfor this feature vector. For instance, the model created using the dataset of people could\ntake as input a feature vector describing a person and output a probability that the person\nhas cancer.\n1If a term is in bold, that means that the term can be found in the index at the end of the book.\n2A real number is a quantity that can represent a distance along a line. Examples: 0,\u2212256 .34,1000,\n1000 .2.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 3", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2710, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "61e7e04e-0b15-40fe-9d07-806ed0a30f74": {"__data__": {"id_": "61e7e04e-0b15-40fe-9d07-806ed0a30f74", "embedding": null, "metadata": {"page_label": "4", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ff5a837d-9dac-4240-a556-379ad7c0fb63", "node_type": "4", "metadata": {"page_label": "4", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "b2fd28c68a7dda5807a6e3a8d71602a5c920971d70dcec03bc4bc4c3552c1be6", "class_name": "RelatedNodeInfo"}}, "text": "1.2.2 Unsupervised Learning\nInunsupervised learning , the dataset is a collection of unlabeled examples {xi}N\ni=1.\nAgain, xis a feature vector, and the goal of an unsupervised learning algorithm is\nto create a modelthat takes a feature vector xas input and either transforms it into\nanother vector or into a value that can be used to solve a practical problem. For example,\ninclustering , the model returns the id of the cluster for each feature vector in the dataset.\nIndimensionality reduction , the output of the model is a feature vector that has fewer\nfeatures than the input x; inoutlier detection , the output is a real number that indicates\nhowxis di\ufb00erent from a \u201ctypical\u201d example in the dataset.\n1.2.3 Semi-Supervised Learning\nInsemi-supervised learning , the dataset contains both labeled and unlabeled examples.\nUsually, the quantity of unlabeled examples is much higher than the number of labeled\nexamples. The goal of a semi-supervised learning algorithm is the same as the goal of\nthe supervised learning algorithm. The hope here is that using many unlabeled examples can\nhelp the learning algorithm to \ufb01nd (we might say \u201cproduce\u201d or \u201ccompute\u201d) a better model.\nIt could look counter-intuitive that learning could bene\ufb01t from adding more unlabeled\nexamples. It seems like we add more uncertainty to the problem. However, when you add\nunlabeled examples, you add more information about your problem: a larger sample re\ufb02ects\nbetter the probability distribution the data we labeled came from. Theoretically, a learning\nalgorithm should be able to leverage this additional information.\n1.2.4 Reinforcement Learning\nReinforcement learning is a sub\ufb01eld of machine learning where the machine \u201clives\u201d in\nan environment and is capable of perceiving the stateof that environment as a vector of\nfeatures. The machine can execute actions in every state. Di\ufb00erent actions bring di\ufb00erent\nrewards and could also move the machine to another state of the environment. The goal of\na reinforcement learning algorithm is to learn a policy.\nA policy is a function (similar to the model in supervised learning)\nthat takes the feature vector of a state as input and outputs an\noptimal action to execute in that state. The action is optimal if\nit maximizes the expected average reward .\nReinforcement learning solves a particular kind of problem where\ndecision making is sequential, and the goal is long-term, such as\ngame playing, robotics, resource management, or logistics. In this\nbook, I put emphasis on one-shot decision making where input\nexamples are independent of one another and the predictions made\nin the past. I leave reinforcement learning out of the scope of this book.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 4", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2738, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0e5eacef-d23e-4e25-ac93-322e17e40379": {"__data__": {"id_": "0e5eacef-d23e-4e25-ac93-322e17e40379", "embedding": null, "metadata": {"page_label": "5", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bcbe17cb-d5a2-44cd-b718-cd59130b531e", "node_type": "4", "metadata": {"page_label": "5", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "cf07f413336a3cf9eba1ad2ca4688b7569caf3712203a6649edcaceaa0e167ef", "class_name": "RelatedNodeInfo"}}, "text": "1.3 How Supervised Learning Works\nIn this section, I brie\ufb02y explain how supervised learning works so that you have the picture\nof the whole process before we go into detail. I decided to use supervised learning as an\nexample because it\u2019s the type of machine learning most frequently used in practice.\nThe supervised learning process starts with gathering the data. The data for supervised\nlearning is a collection of pairs (input, output). Input could be anything, for example, email\nmessages, pictures, or sensor measurements. Outputs are usually real numbers, or labels (e.g.\n\u201cspam\u201d, \u201cnot_spam\u201d, \u201ccat\u201d, \u201cdog\u201d, \u201cmouse\u201d, etc). In some cases, outputs are vectors (e.g.,\nfour coordinates of the rectangle around a person on the picture), sequences (e.g. [\u201cadjective\u201d,\n\u201cadjective\u201d, \u201cnoun\u201d] for the input \u201cbig beautiful car\u201d), or have some other structure.\nLet\u2019s say the problem that you want to solve using supervised learning is spam detection.\nYou gather the data, for example, 10,000 email messages, each with a label either \u201cspam\u201d or\n\u201cnot_spam\u201d (you could add those labels manually or pay someone to do that for us). Now,\nyou have to convert each email message into a feature vector.\nThe data analyst decides, based on their experience, how to convert a real-world entity, such\nas an email message, into a feature vector. One common way to convert a text into a feature\nvector, called bag of words , is to take a dictionary of English words (let\u2019s say it contains\n20,000 alphabetically sorted words) and stipulate that in our feature vector:\n\u2022the \ufb01rst feature is equal to 1if the email message contains the word \u201ca\u201d; otherwise,\nthis feature is 0;\n\u2022thesecondfeatureisequalto 1iftheemailmessagecontainstheword\u201caaron\u201d; otherwise,\nthis feature equals 0;\n\u2022...\n\u2022the feature at position 20,000 is equal to 1if the email message contains the word\n\u201czulu\u201d; otherwise, this feature is equal to 0.\nYou repeat the above procedure for every email message in our collection, which gives\nus 10,000 feature vectors (each vector having the dimensionality of 20,000) and a label\n(\u201cspam\u201d/\u201cnot_spam\u201d).\nNow you have a machine-readable input data, but the output labels are still in the form of\nhuman-readable text. Some learning algorithms require transforming labels into numbers.\nFor example, some algorithms require numbers like 0(to represent the label \u201cnot_spam\u201d)\nand1(to represent the label \u201cspam\u201d). The algorithm I use to illustrate supervised learning is\ncalled Support Vector Machine (SVM). This algorithm requires that the positive label (in\nour case it\u2019s \u201cspam\u201d) has the numeric value of +1(one), and the negative label (\u201cnot_spam\u201d)\nhas the value of\u22121(minus one).\nAt this point, you have a dataset and a learning algorithm , so you are ready to apply\nthe learning algorithm to the dataset to get the model.\nSVM sees every feature vector as a point in a high-dimensional space (in our case, space\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 5", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2948, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "78fc2af1-a8b9-47ad-a011-f0d3b12ffb12": {"__data__": {"id_": "78fc2af1-a8b9-47ad-a011-f0d3b12ffb12", "embedding": null, "metadata": {"page_label": "6", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a3356b4b-8302-4edb-8271-ab839ddf2b5a", "node_type": "4", "metadata": {"page_label": "6", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "ff079a9dbfe02ac2c4aaab6c80516f55331bded84ee32448f15c65c95e1ba2ab", "class_name": "RelatedNodeInfo"}}, "text": "is 20,000-dimensional). The algorithm puts all feature vectors on an imaginary 20,000-\ndimensionalplotanddrawsanimaginary19,999-dimensionalline(a hyperplane )thatseparates\nexamples with positive labels from examples with negative labels. In machine learning, the\nboundary separating the examples of di\ufb00erent classes is called the decision boundary .\nThe equation of the hyperplane is given by two parameters , a real-valued vector wof the\nsame dimensionality as our input feature vector x, and a real number blike this:\nwx\u2212b= 0,\nwhere the expression wxmeans w(1)x(1)+w(2)x(2)+. . .+w(D)x(D), and Dis the number\nof dimensions of the feature vector x.\n(If some equations aren\u2019t clear to you right now, in Chapter 2 we revisit the math and\nstatistical concepts necessary to understand them. For the moment, try to get an intuition of\nwhat\u2019s happening here. It all becomes more clear after you read the next chapter.)\nNow, the predicted label for some input feature vector xis given like this:\ny= sign( wx\u2212b),\nwhere signis a mathematical operator that takes any value as input and returns +1if the\ninput is a positive number or \u22121if the input is a negative number.\nThe goal of the learning algorithm \u2014 SVM in this case \u2014 is to leverage the dataset and \ufb01nd\nthe optimal values w\u2217andb\u2217for parameters wandb. Once the learning algorithm identi\ufb01es\nthese optimal values, the model f(x)is then de\ufb01ned as:\nf(x) = sign( w\u2217x\u2212b\u2217)\nTherefore, to predict whether an email message is spam or not spam using an SVM model,\nyou have to take a text of the message, convert it into a feature vector, then multiply this\nvector by w\u2217, subtract b\u2217and take the sign of the result. This will give us the prediction ( +1\nmeans \u201cspam\u201d,\u22121means \u201cnot_spam\u201d).\nNow, how does the machine \ufb01nd w\u2217andb\u2217? It solves an optimization problem. Machines\nare good at optimizing functions under constraints.\nSo what are the constraints we want to satisfy here? First of all, we want the model to predict\nthe labels of our 10,000 examples correctly. Remember that each example i= 1, . . . , 10000is\ngiven by a pair (xi, yi), where xiis the feature vector of example iandyiis its label that\ntakes values either \u22121or+1. So the constraints are naturally:\nwxi\u2212b\u2265+1ifyi= +1 ,\nwxi\u2212b\u2264\u22121ifyi=\u22121.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 6", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2300, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f63876c-4705-40c2-a6ec-20e85593ac7a": {"__data__": {"id_": "8f63876c-4705-40c2-a6ec-20e85593ac7a", "embedding": null, "metadata": {"page_label": "7", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b5f15307-d81e-4764-874a-4061b39198f6", "node_type": "4", "metadata": {"page_label": "7", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "01f22f9288c5b4a93f8ae7611a253f2a2a288758226514d94103ed58aa375ac2", "class_name": "RelatedNodeInfo"}}, "text": "x( 2)\nx(1)wx\u00a0\u2014\u00a0b\u00a0=\u00a00wx\u00a0\u2014\u00a0b\u00a0=\u00a01\nwx\u00a0\u2014\u00a0b\u00a0=\u00a0\u20141\nb\u00a0|| w||\u00a02\u00a0|| w||\u00a0Figure 1: An example of an SVM model for two-dimensional feature vectors.\nWe would also prefer that the hyperplane separates positive examples from negative ones with\nthe largest margin. The margin is the distance between the closest examples of two classes,\nas de\ufb01ned by the decision boundary. A large margin contributes to a better generalization ,\nthat is how well the model will classify new examples in the future. To achieve that, we need\nto minimize the Euclidean norm of wdenoted by\u2225w\u2225and given by\u2211\u2211D\nj=1(w(j))2.\nSo, the optimization problem that we want the machine to solve looks like this:\nMinimize\u2225w\u2225subject to yi(wxi\u2212b)\u22651fori= 1, . . . , N. The expression yi(wxi\u2212b)\u22651\nis just a compact way to write the above two constraints.\nThe solution of this optimization problem, given by w\u2217andb\u2217, is called the statistical\nmodel, or, simply, the model. The process of building the model is called training .\nFor two-dimensional feature vectors, the problem and the solution can be visualized as\nshown in Figure 1. The blue and orange circles represent, respectively, positive and negative\nexamples, and the line given by wx\u2212b= 0is the decision boundary.\nWhy, by minimizing the norm of w, do we \ufb01nd the highest margin between the two classes?\nGeometrically, the equations wx\u2212b= 1andwx\u2212b=\u22121de\ufb01ne two parallel hyperplanes, as\nyou see in Figure 1. The distance between these hyperplanes is given by2\n\u2225w\u2225, so the smaller\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1542, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c3ef254a-3da0-4f1c-9f18-5bdf1ecf2ca6": {"__data__": {"id_": "c3ef254a-3da0-4f1c-9f18-5bdf1ecf2ca6", "embedding": null, "metadata": {"page_label": "8", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9a727675-6b4c-4065-bf0a-2c31152f7cb2", "node_type": "4", "metadata": {"page_label": "8", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "8937b37272d256ae2e2a5fa8e4480a441c543147b6548212bd79e0f8b868f728", "class_name": "RelatedNodeInfo"}}, "text": "the norm\u2225w\u2225, the larger the distance between these two hyperplanes.\nThat\u2019s how Support Vector Machines work. This particular version of the algorithm builds\nthe so-called linear model . It\u2019s called linear because the decision boundary is a straight line\n(or a plane, or a hyperplane). SVM can also incorporate kernels that can make the decision\nboundary arbitrarily non-linear. In some cases, it could be impossible to perfectly separate\nthe two groups of points because of noise in the data, errors of labeling, or outliers (examples\nvery di\ufb00erent from a \u201ctypical\u201d example in the dataset). Another version of SVM can also\nincorporate a penalty hyperparameter3for misclassi\ufb01cation of training examples of speci\ufb01c\nclasses. We study the SVM algorithm in more detail in Chapter 3.\nAt this point, you should retain the following: any classi\ufb01cation learning algorithm that\nbuilds a model implicitly or explicitly creates a decision boundary. The decision boundary\ncan be straight, or curved, or it can have a complex form, or it can be a superposition of\nsome geometrical \ufb01gures. The form of the decision boundary determines the accuracy of\nthe model (that is the ratio of examples whose labels are predicted correctly). The form of\nthe decision boundary, the way it is algorithmically or mathematically computed based on\nthe training data, di\ufb00erentiates one learning algorithm from another.\nIn practice, there are two other essential di\ufb00erentiators of learning algorithms to consider:\nspeed of model building and prediction processing time. In many practical cases, you would\nprefer a learning algorithm that builds a less accurate model fast. Additionally, you might\nprefer a less accurate model that is much quicker at making predictions.\n1.4 Why the Model Works on New Data\nWhy is a machine-learned model capable of predicting correctly the labels of new, previously\nunseen examples? To understand that, look at the plot in Figure 1. If two classes are\nseparable from one another by a decision boundary, then, obviously, examples that belong to\neach class are located in two di\ufb00erent subspaces which the decision boundary creates.\nIf the examples used for training were selected randomly, independently of one another, and\nfollowing the same procedure, then, statistically, it is more likely that the new negative\nexample will be located on the plot somewhere not too far from other negative examples.\nThe same concerns the new positive example: it will likelycome from the surroundings of\nother positive examples. In such a case, our decision boundary will still, with high probability ,\nseparate well new positive and negative examples from one another. For other, less likely\nsituations , our model will make errors, but because such situations are less likely, the number\nof errors will likely be smaller than the number of correct predictions.\nIntuitively, the larger is the set of training examples, the more unlikely that the new examples\nwill be dissimilar to (and lie on the plot far from) the examples used for training.\n3A hyperparameter is a property of a learning algorithm, usually (but not always) having a numerical\nvalue. That value in\ufb02uences the way the algorithm works. Those values aren\u2019t learned by the algorithm itself\nfrom data. They have to be set by the data analyst before running the algorithm.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 8", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3383, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "87ed0bd6-70ac-470f-bfb5-f0b1620ee280": {"__data__": {"id_": "87ed0bd6-70ac-470f-bfb5-f0b1620ee280", "embedding": null, "metadata": {"page_label": "9", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ec1486e7-a8af-46f9-b662-61e579b2c00b", "node_type": "4", "metadata": {"page_label": "9", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "0ed50a026ee830b703db712b55013a0a4a1c6ee73a359c925a3740036d675c79", "class_name": "RelatedNodeInfo"}}, "text": "To minimize the probability of making errors on new examples,\nthe SVM algorithm, by looking for the largest margin, explicitly\ntries to draw the decision boundary in such a way that it lies as\nfar as possible from examples of both classes.\nThe reader interested in knowing more about the learnability and\nunderstanding the close relationship between the model error, the\nsize of the training set, the form of the mathematical equation\nthat de\ufb01nes the model, and the time it takes to build the model\nis encouraged to read about the PAC learning . The PAC (for\n\u201cprobably approximately correct\u201d) learning theory helps to analyze whether and under what\nconditions a learning algorithm will probably output an approximately correct classi\ufb01er.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 9", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 800, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6f34b1dd-be63-4574-8d45-27e98d5ec2e7": {"__data__": {"id_": "6f34b1dd-be63-4574-8d45-27e98d5ec2e7", "embedding": null, "metadata": {"page_label": "10", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "236cfce7-63f0-422a-9231-4f6cd1e49bfa", "node_type": "4", "metadata": {"page_label": "10", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "d6c8e49e7698b3cb4c368f70b456339dd31be0635f6a37d5d8276c86c55b0d43", "class_name": "RelatedNodeInfo"}}, "text": "2 Notation and De\ufb01nitions\n2.1 Notation\nLet\u2019s start by revisiting the mathematical notation we all learned at school, but some likely\nforgot right after the prom.\n2.1.1 Data Structures\nAscalaris a simple numerical value, like 15or\u22123.25. Variables or constants that take scalar\nvalues are denoted by an italic letter, like xora.\nAvectoris an ordered list of scalar values, called attributes. We denote a vector as a bold\ncharacter, for example, xorw. Vectors can be visualized as arrows that point to some\ndirections as well as points in a multi-dimensional space. Illustrations of three two-dimensional\nvectors, a= [2,3],b= [\u22122,5], and c= [1,0]are given in Figure 1. We denote an attribute\nof a vector as an italic value with an index, like this: w(j)orx(j). The index jdenotes a\nspeci\ufb01cdimension of the vector, the position of an attribute in the list. For instance, in the\nvector ashown in red in Figure 1, a(1)= 2anda(2)= 3.\nThe notation x(j)should not be confused with the power operator, like this x2(squared) or\nx3(cubed). If we want to apply a power operator, say square, to an indexed attribute of a\nvector, we write like this: (x(j))2.\nA variable can have two or more indices, like this: x(j)\nior like this x(k)\ni,j. For example, in\nneural networks, we denote as x(j)\nl,uthe input feature jof unituin layerl.\nAmatrixis a rectangular array of numbers arranged in rows and columns. Below is an\nexample of a matrix with two rows and three columns,\n[2 4\u22123\n21\u22126\u22121]\n.\nMatrices are denoted with bold capital letters, such as AorW.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 3", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1594, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f080200b-667b-474f-bb2d-4b322d5838d7": {"__data__": {"id_": "f080200b-667b-474f-bb2d-4b322d5838d7", "embedding": null, "metadata": {"page_label": "11", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "40155453-fe17-4363-a9e6-cf3e7b94541b", "node_type": "4", "metadata": {"page_label": "11", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "af3b609f35cde44233658f603f0b014cad4d1b5bee9561b35fb49e73c5ed2217", "class_name": "RelatedNodeInfo"}}, "text": "Figure 1: Three vectors visualized as directions and as points.\nAsetis an unordered collection of unique elements. We denote a set as a calligraphic\ncapital character, for example, S. A set of numbers can be \ufb01nite (include a \ufb01xed amount\nof values). In this case, it is denoted using accolades, for example, {1,3,18,23,235}or\n{x1,x2,x3,x4,...,xn}. A set can be in\ufb01nite and include all values in some interval. If a set\nincludes all values between aandb, including aandb, it is denoted using brackets as [a,b].\nIf the set doesn\u2019t include the values aandb, such a set is denoted using parentheses like this:\n(a,b). For example, the set [0,1]includes such values as 0,0.0001,0.25,0.784,0.9995, and\n1.0. A special set denoted Rincludes all numbers from minus in\ufb01nity to plus in\ufb01nity.\nWhen an element xbelongs to a set S, we write x\u2208S. We can obtain a new set S3as\nanintersection of two setsS1andS2. In this case, we write S3\u2190S 1\u2229S2. For example\n{1,3,5,8}\u2229{ 1,8,4}gives the new set {1,8}.\nWe can obtain a new set S3as aunionof two setsS1andS2. In this case, we write\nS3\u2190S 1\u222aS2. For example{1,3,5,8}\u222a{ 1,8,4}gives the new set {1,3,4,5,8}.\n2.1.2 Capital Sigma Notation\nThe summation over a collection X={x1,x2,...,xn\u22121,xn}or over the attributes of a vector\nx= [x(1),x(2),...,x(m\u22121),x(m)]is denoted like this:\nn\u2211\ni=1xidef=x1+x2+...+xn\u22121+xn,or else:m\u2211\nj=1x(j)def=x(1)+x(2)+...+x(m\u22121)+x(m).\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 4", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1442, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e080ee58-03f1-44bf-9487-7f672ae6880a": {"__data__": {"id_": "e080ee58-03f1-44bf-9487-7f672ae6880a", "embedding": null, "metadata": {"page_label": "12", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "463eb972-64fb-47d6-a299-e117425adc0d", "node_type": "4", "metadata": {"page_label": "12", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "d1057033f9bbbea2ccc2b4222097612a3ba43b3e914926174db59cc51352c7f0", "class_name": "RelatedNodeInfo"}}, "text": "The notationdef=means \u201cis de\ufb01ned as\u201d.\n2.1.3 Capital Pi Notation\nA notation analogous to capital sigma is the capital pi notation . It denotes a product of\nelements in a collection or attributes of a vector:\nn\u220f\ni=1xidef=x1\u00b7x2\u00b7...\u00b7xn\u22121\u00b7xn,\nwherea\u00b7bmeansamultiplied by b. Where possible, we omit \u00b7to simplify the notation, so ab\nalso means amultiplied by b.\n2.1.4 Operations on Sets\nA derived set creation operator looks like this: S\u2032\u2190{x2|x\u2208S,x> 3}. This notation means\nthat we create a new set S\u2032by putting into it xsquared such that xis inS, andxis greater\nthan 3.\nThe cardinality operator |S|returns the number of elements in set S.\n2.1.5 Operations on Vectors\nThe sum of two vectors x+zis de\ufb01ned as the vector [x(1)+z(1),x(2)+z(2),...,x(m)+z(m)].\nThe di\ufb00erence of two vectors x\u2212zis de\ufb01ned as [x(1)\u2212z(1),x(2)\u2212z(2),...,x(m)\u2212z(m)].\nA vector multiplied by a scalar is a vector. For example xcdef= [cx(1),cx(2),...,cx(m)].\nAdot-product of two vectors is a scalar. For example, wxdef=\u2211m\ni=1w(i)x(i). In some books,\nthe dot-product is denoted as w\u00b7x. The two vectors must be of the same dimensionality.\nOtherwise, the dot-product is unde\ufb01ned.\nThe multiplication of a matrix Wby a vector xresults in another vector. Let our matrix be,\nW=[w(1,1)w(1,2)w(1,3)\nw(2,1)w(2,2)w(2,3)]\n.\nWhen vectors participate in operations on matrices, a vector is by default represented as a\nmatrix with one column. When the vector is on the right of the matrix, it remains a column\nvector. We can only multiply a matrix by vector if the vector has the same number of rows\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 5", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1607, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf6fda56-25ab-4f3c-9a72-05c6d440bf16": {"__data__": {"id_": "cf6fda56-25ab-4f3c-9a72-05c6d440bf16", "embedding": null, "metadata": {"page_label": "13", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b14cf05b-cb8f-4706-98c7-5aa7838e6b00", "node_type": "4", "metadata": {"page_label": "13", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "5843657329162d91e0e7615d19716081744a31621e4deb9a939993dd16a4f4e8", "class_name": "RelatedNodeInfo"}}, "text": "as the number of columns in the matrix. Let our vector be xdef=[x(1),x(2),x(3)]. Then Wx\nis a two-dimensional vector de\ufb01ned as,\nWx =[\nw(1,1)w(1,2)w(1,3)\nw(2,1)w(2,2)w(2,3)]\uf8ee\n\uf8f0x(1)\nx(2)\nx(3)\uf8f9\n[\ndef=[w(1,1)x(1)+w(1,2)x(2)+w(1,3)x(3)\nw(2,1)x(1)+w(2,2)x(2)+w(2,3)x(3)]\n=[w(1)x\nw(2)x]\nIf our matrix had, say, \ufb01ve rows, the result of the product would be a \ufb01ve-dimensional vector.\nWhen the vector is on the left side of the matrix in the multiplication, then it has to be\ntransposed before we multiply it by the matrix. The transpose of the vector xdenoted as x\u22a4\nmakes a row vector out of a column vector. Let\u2019s say,\nx=[x(1)\nx(2)]\n,thenx\u22a4def=[\nx(1)x(2)]\n.\nThe multiplication of the vector xby the matrix Wis given by x\u22a4W,\nx\u22a4W=[\nx(1)x(2)][\nw(1,1)w(1,2)w(1,3)\nw(2,1)w(2,2)w(2,3)]\ndef=[\nw(1,1)x(1)+w(2,1)x(2),w(1,2)x(1)+w(2,2)x(2),w(1,3)x(1)+w(2,3)x(2)]\nAs you can see, we can only multiply a vector by a matrix if the vector has the same number\nof dimensions as the number of rows in the matrix.\n2.1.6 Functions\nA function is a relation that associates each element xof a setX, thedomainof the function,\nto a single element yof another setY, thecodomain of the function. A function usually has a\nname. If the function is called f, this relation is denoted y=f(x)(readfofx), the element\nxis the argument or input of the function, and yis the value of the function or the output.\nThe symbol that is used for representing the input is the variable of the function (we often\nsay thatfis a function of the variable x).\nWe say that f(x)has alocal minimum atx=ciff(x)\u2265f(c)for everyxin some open\ninterval around x=c. Anintervalis a set of real numbers with the property that any number\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 6", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1732, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eb88094e-6ff4-4470-8002-7a1adce41a06": {"__data__": {"id_": "eb88094e-6ff4-4470-8002-7a1adce41a06", "embedding": null, "metadata": {"page_label": "14", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f043dc5b-32a0-44b0-8003-76ef4c10d44e", "node_type": "4", "metadata": {"page_label": "14", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "49e03381ffc1a4c385b132b413a91e3857e807331bb20a208fc51792489dc6e2", "class_name": "RelatedNodeInfo"}}, "text": "Figure 2: A local and a global minima of a function.\nthat lies between two numbers in the set is also included in the set. An open interval does\nnot include its endpoints and is denoted using parentheses. For example, (0,1)means \u201call\nnumbers greater than 0and less than 1\u201d. The minimal value among all the local minima is\ncalled the global minimum . See illustration in Figure 2.\nA vector function, denoted as y=f(x)is a function that returns a vector y. It can have a\nvector or a scalar argument.\n2.1.7 Max and Arg Max\nGiven a set of values A={a1,a2,...,an}, the operator maxa\u2208Af(a)returns the highest\nvaluef(a)for all elements in the set A. On the other hand, the operator arg maxa\u2208Af(a)\nreturns the element of the set Athat maximizes f(a).\nSometimes, when the set is implicit or in\ufb01nite, we can write maxaf(a)orarg maxaf(a).\nOperators minandarg minoperate in a similar manner.\n2.1.8 Assignment Operator\nThe expression a\u2190f(x)means that the variable agets the new value: the result of f(x).\nWe say that the variable agets assigned a new value. Similarly, a\u2190[a1,a2]means that the\nvector variable agets the two-dimensional vector value [a1,a2].\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1206, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "61a87f38-60a2-4e69-b999-e73b2e335de2": {"__data__": {"id_": "61a87f38-60a2-4e69-b999-e73b2e335de2", "embedding": null, "metadata": {"page_label": "15", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3b495b58-2261-4c79-9ef7-56ef823b29ae", "node_type": "4", "metadata": {"page_label": "15", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "b05917359018bab6a3aebc3482a927e384d823ae7b0910624fe8df32890ee601", "class_name": "RelatedNodeInfo"}}, "text": "2.1.9 Derivative and Gradient\nAderivativef\u2032of a function fis a function or a value that describes how fast fgrows (or\ndecreases). If the derivative is a constant value, like 5or\u22123, then the function grows (or\ndecreases) constantly at any point xof its domain. If the derivative f\u2032is a function, then the\nfunctionfcan grow at a di\ufb00erent pace in di\ufb00erent regions of its domain. If the derivative f\u2032\nis positive at some point x, then the function fgrows at this point. If the derivative of fis\nnegative at some x, then the function decreases at this point. The derivative of zero at x\nmeans that the function\u2019s slope at xis horizontal.\nThe process of \ufb01nding a derivative is called di\ufb00erentiation .\nDerivatives for basic functions are known. For example if f(x) =x2, thenf\u2032(x) = 2x; if\nf(x) = 2xthenf\u2032(x) = 2; iff(x) = 2thenf\u2032(x) = 0(the derivative of any function f(x) =c,\nwherecis a constant value, is zero).\nIf the function we want to di\ufb00erentiate is not basic, we can \ufb01nd its derivative using the\nchain rule . For instance if F(x) =f(g(x)), wherefandgare some functions, then F\u2032(x) =\nf\u2032(g(x))g\u2032(x). For example if F(x) = (5x+ 1)2theng(x) = 5x+ 1andf(g(x)) = (g(x))2.\nBy applying the chain rule, we \ufb01nd F\u2032(x) = 2(5x+ 1)g\u2032(x) = 2(5x+ 1)5 = 50x+ 10.\nGradient is the generalization of derivative for functions that take several inputs (or one\ninput in the form of a vector or some other complex structure). A gradient of a function\nis a vector of partial derivatives . You can look at \ufb01nding a partial derivative of a function\nas the process of \ufb01nding the derivative by focusing on one of the function\u2019s inputs and by\nconsidering all other inputs as constant values.\nFor example, if our function is de\ufb01ned as f([x(1),x(2)]) =ax(1)+bx(2)+c, then the partial\nderivative of function fwith respect to x(1), denoted as\u2202f\n\u2202x(1), is given by,\n\u2202f\n\u2202x(1)=a+ 0 + 0 =a,\nwhereais the derivative of the function ax(1); the two zeroes are respectively derivatives of\nbx(2)andc, becausex(2)is considered constant when we compute the derivative with respect\ntox(1), and the derivative of any constant is zero.\nSimilarly, the partial derivative of function fwith respect to x(2),\u2202f\n\u2202x(2), is given by,\n\u2202f\n\u2202x(2)= 0 +b+ 0 =b.\nThe gradient of function f, denoted as\u2207fis given by the vector [\u2202f\n\u2202x(1),\u2202f\n\u2202x(2)].\nThe chain rule works with partial derivatives too, as I illustrate in Chapter 4.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 8", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2429, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c85028cd-14f7-4ea8-8e7a-a12da317ac27": {"__data__": {"id_": "c85028cd-14f7-4ea8-8e7a-a12da317ac27", "embedding": null, "metadata": {"page_label": "16", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "300fa914-1a49-4e28-be76-ffa821a9c22c", "node_type": "4", "metadata": {"page_label": "16", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "2ce8b42f925654085e4a8fc17c73aed396d95246b790618539bcb08c49847dd8", "class_name": "RelatedNodeInfo"}}, "text": "1 2 3 4\nx0.00.10.20.30.40.50.6pmf(a)\n0 2 4 6 8\nx0.00.10.20.30.40.50.6pdf\nArea = 1.0 (b)\nFigure 3: A probability mass function and a probability density function.\n2.2 Random Variable\nArandom variable , usually written as an italic capital letter, like X, is a variable whose\npossible values are numerical outcomes of a random phenomenon. Examples of random\nphenomena with a numerical outcome include a toss of a coin ( 0for heads and 1for tails), a\nroll of a dice, or the height of the \ufb01rst stranger you meet outside. There are two types of\nrandom variables: discreteandcontinuous .\nAdiscrete random variable takes on only a countable number of distinct values such as red,\nyellow,blueor1,2,3,....\nTheprobability distribution of a discrete random variable is described by a list of probabilities\nassociated with each of its possible values. This list of probabilities is called a probability\nmass function (pmf). For example: Pr(X=red) = 0.3,Pr(X=yellow ) = 0.45,Pr(X=\nblue) = 0.25. Each probability in a probability mass function is a value greater than or equal\nto0. The sum of probabilities equals 1(Figure 3a).\nAcontinuous random variable (CRV) takes an in\ufb01nite number of possible values in some\ninterval. Examples include height, weight, and time. Because the number of values of a\ncontinuous random variable Xis in\ufb01nite, the probability Pr(X=c)for anycis0. Therefore,\ninstead of the list of probabilities, the probability distribution of a CRV (a continuous\nprobability distribution) is described by a probability density function (pdf). The pdf is a\nfunction whose codomain is nonnegative and the area under the curve is equal to 1(Figure\n3b).\nLet a discrete random variable Xhavekpossible values{xi}k\ni=1. Theexpectation ofX\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 9", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1794, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e7ca454d-295b-46a2-9fbb-90681869df2a": {"__data__": {"id_": "e7ca454d-295b-46a2-9fbb-90681869df2a", "embedding": null, "metadata": {"page_label": "17", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9e9ee98d-f957-414c-a7be-b407a3f0eff8", "node_type": "4", "metadata": {"page_label": "17", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "64caabec7e125bc1e68f7269762863b8ea66a2c3b294bb47efec438ac073d915", "class_name": "RelatedNodeInfo"}}, "text": "denoted as E[X]is given by,\nE[X]def=k\u2211\ni=1[xi\u00b7Pr(X=xi)]\n=x1\u00b7Pr(X=x1) +x2\u00b7Pr(X=x2) +\u00b7\u00b7\u00b7+xk\u00b7Pr(X=xk),(1)\nwhere Pr(X=xi)is the probability that Xhas the value xiaccording to the pmf. The\nexpectation of a random variable is also called the mean,averageorexpected value and is\nfrequently denoted with the letter \u00b5. The expectation is one of the most important statistics\nof a random variable.\nAnother important statistic is the standard deviation , de\ufb01ned as,\n\u03c3def=\u221a\nE[(X\u2212\u00b5)2].\nVariance , denoted as \u03c32orvar(X), is de\ufb01ned as,\n\u03c32=E[(X\u2212\u00b5)2].\nFor a discrete random variable, the standard deviation is given by:\n\u03c3=\u221a\nPr(X=x1)(x1\u2212\u00b5)2+ Pr(X=x2)(x2\u2212\u00b5)2+\u00b7\u00b7\u00b7+ Pr(X=xk)(xk\u2212\u00b5)2,\nwhere\u00b5=E[X].\nThe expectation of a continuous random variable Xis given by,\nE[X]def=\u222b\nRxfX(x)dx, (2)\nwherefXis the pdf of the variable Xand\u222b\nRis theintegralof function xfX.\nIntegral is an equivalent of the summation over all values of the function when the function\nhas a continuous domain. It equals the area under the curve of the function. The property of\nthe pdf that the area under its curve is 1mathematically means that\u222b\nRfX(x)dx= 1.\nMost of the time we don\u2019t know fX, but we can observe some values of X. In machine\nlearning, we call these values examples , and the collection of these examples is called a\nsample or adataset.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 10", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1359, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8a57dad2-394d-4dad-ba35-de02065cc213": {"__data__": {"id_": "8a57dad2-394d-4dad-ba35-de02065cc213", "embedding": null, "metadata": {"page_label": "18", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "145d45b6-8460-4056-911a-382d6e4d322b", "node_type": "4", "metadata": {"page_label": "18", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "b807f19673773b7868802dc0896b7439400c8187400b810b556a2bafd2ae922a", "class_name": "RelatedNodeInfo"}}, "text": "2.3 Unbiased Estimators\nBecausefXis usually unknown, but we have a sample SX={xi}N\ni=1, we often content\nourselves not with the true values of statistics of the probability distribution, such as\nexpectation, but with their unbiased estimators .\nWe say that \u02c6\u03b8(SX)is an unbiased estimator of some statistic \u03b8calculated using a sample SX\ndrawn from an unknown probability distribution if \u02c6\u03b8(SX)has the following property:\nE[\n\u02c6\u03b8(SX)]\n=\u03b8,\nwhere \u02c6\u03b8is asample statistic , obtained using a sample SXand not the real statistic \u03b8that\ncan be obtained only knowing X; the expectation is taken over all possible samples drawn\nfromX. Intuitively, this means that if you can have an unlimited number of such samples\nasSX, and you compute some unbiased estimator, such as \u02c6\u00b5, using each sample, then the\naverage of all these \u02c6\u00b5equals the real statistic \u00b5that you would get computed on X.\nIt can be shown that an unbiased estimator of an unknown E[X](given by either eq. 1 or\neq. 2) is given by1\nN\u2211N\ni=1xi(called in statistics the sample mean ).\n2.4 Bayes\u2019 Rule\nThe conditional probability Pr(X=x|Y=y)is the probability of the random variable Xto\nhave a speci\ufb01c value xgiven that another random variable Yhas a speci\ufb01c value of y. The\nBayes\u2019 Rule (also known as the Bayes\u2019 Theorem ) stipulates that:\nPr(X=x|Y=y) =Pr(Y=y|X=x) Pr(X=x)\nPr(Y=y).\n2.5 Parameter Estimation\nBayes\u2019 Rule comes in handy when we have a model of X\u2019s distribution, and this model f\u03b8is a\nfunction that has some parameters in the form of a vector \u03b8. An example of such a function\ncould be the Gaussian function that has two parameters, \u00b5and\u03c3, and is de\ufb01ned as:\nf\u03b8(x) =1\u221a\n2\u03c0\u03c32e\u2212(x\u2212\u00b5)2\n2\u03c32, (3)\nwhere\u03b8def= [\u00b5,\u03c3].\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 11", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1727, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6ebfc45-f578-460e-8d9e-21c4042ce86d": {"__data__": {"id_": "e6ebfc45-f578-460e-8d9e-21c4042ce86d", "embedding": null, "metadata": {"page_label": "19", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9b562cbf-8e90-4b91-96ba-3094fe3fa559", "node_type": "4", "metadata": {"page_label": "19", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "14184690a28a43818a27800fd36b5a4f5b0f7cbdcd9338f9781a65b03e4cf01d", "class_name": "RelatedNodeInfo"}}, "text": "This function has all the properties of a pdf1. Therefore, we can use it as a model of an\nunknown distribution of X. We can update the values of parameters in the vector \u03b8from the\ndata using the Bayes\u2019 Rule:\nPr(\u03b8=\u02c6\u03b8|X=x)\u2190Pr(X=x|\u03b8=\u02c6\u03b8) Pr(\u03b8=\u02c6\u03b8)\nPr(X=x)=Pr(X=x|\u03b8=\u02c6\u03b8) Pr(\u03b8=\u02c6\u03b8)\u2211\n\u02dc\u03b8Pr(X=x|\u03b8=\u02dc\u03b8).(4)\nwhere Pr(X=x|\u03b8=\u02c6\u03b8)def=f\u02c6\u03b8.\nIf we have a sample SofXand the set of possible values for \u03b8is \ufb01nite, we can easily estimate\nPr(\u03b8=\u02c6\u03b8)by applying Bayes\u2019 Rule iteratively, one example x\u2208Sat a time. The initial value\nPr(\u03b8=\u02c6\u03b8)can be guessed such that\u2211\n\u02c6\u03b8Pr(\u03b8=\u02c6\u03b8) = 1. This guess of the probabilities for\ndi\ufb00erent \u02c6\u03b8is called the prior.\nFirst, we compute Pr(\u03b8=\u02c6\u03b8|X=x1)for all possible values \u02c6\u03b8. Then, before updating\nPr(\u03b8=\u02c6\u03b8|X=x)once again, this time for x=x2\u2208Susing eq. 4, we replace the prior\nPr(\u03b8=\u02c6\u03b8)in eq. 4 by the new estimate Pr(\u03b8=\u02c6\u03b8)\u21901\nN\u2211\nx\u2208SPr(\u03b8=\u02c6\u03b8|X=x).\nThe best value of the parameters \u03b8\u2217given one example is obtained using the principle of\nmaximum a posteriori (or MAP):\n\u03b8\u2217= arg max\n\u03b8N\u220f\ni=1Pr(\u03b8=\u02c6\u03b8|X=xi). (5)\nIf the set of possible values for \u03b8isn\u2019t \ufb01nite, then we need to optimize eq. 5 directly using a\nnumerical optimization routine, such as gradient descent, which we consider in Chapter 4.\nUsually, we optimize the natural logarithm of the right-hand side expression in eq. 5 because\nthe logarithm of a product becomes the sum of logarithms and it\u2019s easier for the machine to\nwork with a sum than with a product2.\n2.6 Parameters vs. Hyperparameters\nA hyperparameter is a property of a learning algorithm, usually (but not always) having a\nnumerical value. That value in\ufb02uences the way the algorithm works. Hyperparameters aren\u2019t\nlearned by the algorithm itself from data. They have to be set by the data analyst before\nrunning the algorithm. I show how to do that in Chapter 5.\n1In fact, eq. 3 de\ufb01nes the pdf of one of the most frequently used in practice probability distributions called\nGaussian distribution ornormal distribution and denoted as N(\u00b5,\u03c32).\n2Multiplication of many numbers can give either a very small result or a very large one. It often results in\nthe problem of numerical over\ufb02ow when the machine cannot store such extreme numbers in memory.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 12", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2224, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "957fa270-d43a-482e-87de-8492a4674010": {"__data__": {"id_": "957fa270-d43a-482e-87de-8492a4674010", "embedding": null, "metadata": {"page_label": "20", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dc5316fe-3305-4445-9d15-30dd1f46117d", "node_type": "4", "metadata": {"page_label": "20", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "684ed14b30b0fa9b27b5d4bd6ca3e85d25d7585d6a9ad53239f850caa56b930a", "class_name": "RelatedNodeInfo"}}, "text": "Parameters are variables that de\ufb01ne the model learned by the learning algorithm. Parameters\nare directly modi\ufb01ed by the learning algorithm based on the training data. The goal of\nlearning is to \ufb01nd such values of parameters that make the model optimal in a certain sense.\n2.7 Classi\ufb01cation vs. Regression\nClassi\ufb01cation is a problem of automatically assigning a labelto anunlabeled example .\nSpam detection is a famous example of classi\ufb01cation.\nIn machine learning, the classi\ufb01cation problem is solved by a classi\ufb01cation learning\nalgorithm that takes a collection of labeled examples as inputs and produces a model\nthat can take an unlabeled example as input and either directly output a label or output a\nnumber that can be used by the analyst to deduce the label. An example of such a number\nis a probability.\nIn a classi\ufb01cation problem, a label is a member of a \ufb01nite set of classes. If the size of\nthe set of classes is two (\u201csick\u201d/\u201chealthy\u201d, \u201cspam\u201d/\u201cnot_spam\u201d), we talk about binary\nclassi\ufb01cation (also called binomial in some sources). Multiclass classi\ufb01cation (also\ncalledmultinomial ) is a classi\ufb01cation problem with three or more classes3.\nWhile some learning algorithms naturally allow for more than two classes, others are by nature\nbinary classi\ufb01cation algorithms. There are strategies allowing to turn a binary classi\ufb01cation\nlearning algorithm into a multiclass one. I talk about one of them in Chapter 7.\nRegression is a problem of predicting a real-valued label (often called a target) given an\nunlabeled example. Estimating house price valuation based on house features, such as area,\nthe number of bedrooms, location and so on is a famous example of regression.\nThe regression problem is solved by a regression learning algorithm that takes a collection\nof labeled examples as inputs and produces a model that can take an unlabeled example as\ninput and output a target.\n2.8 Model-Based vs. Instance-Based Learning\nMost supervised learning algorithms are model-based. We have already seen one such\nalgorithm: SVM. Model-based learning algorithms use the training data to create a model\nthat hasparameters learned from the training data. In SVM, the two parameters we saw\nwerew\u2217andb\u2217. After the model was built, the training data can be discarded.\nInstance-based learning algorithms use the whole dataset as the model. One instance-based\nalgorithm frequently used in practice is k-Nearest Neighbors (kNN). In classi\ufb01cation, to\npredict a label for an input example the kNN algorithm looks at the close neighborhood of\nthe input example in the space of feature vectors and outputs the label that it saw the most\noften in this close neighborhood.\n3There\u2019s still one label per example though.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 13", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2766, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "afac65e7-07d0-4fb7-9722-af0d81399971": {"__data__": {"id_": "afac65e7-07d0-4fb7-9722-af0d81399971", "embedding": null, "metadata": {"page_label": "21", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e8d1afae-2b38-41a4-a784-141fb81c6fd4", "node_type": "4", "metadata": {"page_label": "21", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "e777cf54c24633ebc23c55c08a12d905716cba5e61d58f18989ce92bc60d2920", "class_name": "RelatedNodeInfo"}}, "text": "2.9 Shallow vs. Deep Learning\nA shallow learning algorithm learns the parameters of the model directly from the features\nof the training examples. Most supervised learning algorithms are shallow. The notorious\nexceptions are neural network learning algorithms, speci\ufb01cally those that build neural\nnetworks with more than one layerbetween input and output. Such neural networks are\ncalleddeep neural networks . In deep neural network learning (or, simply, deep learning),\ncontrary to shallow learning, most model parameters are learned not directly from the features\nof the training examples, but from the outputs of the preceding layers.\nDon\u2019t worry if you don\u2019t understand what that means right now. We look at neural networks\nmore closely in Chapter 6.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 14", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 818, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "09c3568d-e935-4c2e-baea-4a463ec3c6e9": {"__data__": {"id_": "09c3568d-e935-4c2e-baea-4a463ec3c6e9", "embedding": null, "metadata": {"page_label": "22", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "91528d1a-8dbc-41b0-922f-dfe21a9cca72", "node_type": "4", "metadata": {"page_label": "22", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "1d62f78ae8ba9795dd83c4cbe3a7c112ddb5c21c028a161f4b07607053b566f8", "class_name": "RelatedNodeInfo"}}, "text": "3 Fundamental Algorithms\nIn this chapter, I describe \ufb01ve algorithms which are not just the most known but also either\nvery e\ufb00ective on their own or are used as building blocks for the most e\ufb00ective learning\nalgorithms out there.\n3.1 Linear Regression\nLinear regression is a popular regression learning algorithm that learns a model which is a\nlinear combination of features of the input example.\n3.1.1 Problem Statement\nWe have a collection of labeled examples {(xi,yi)}N\ni=1, whereNis the size of the collection,\nxiis theD-dimensional feature vector of example i= 1,...,N,yiis a real-valued1target\nand every feature x(j)\ni,j= 1,...,D, is also a real number.\nWe want to build a model fw,b(x)as a linear combination of features of example x:\nfw,b(x) =wx+b, (1)\nwhere wis aD-dimensional vector of parameters and bis a real number. The notation fw,b\nmeans that the model fis parametrized by two values: wandb.\nWe will use the model to predict the unknown yfor a given xlike this:y\u2190fw,b(x). Two\nmodels parametrized by two di\ufb00erent pairs (w,b)will likely produce two di\ufb00erent predictions\nwhen applied to the same example. We want to \ufb01nd the optimal values (w\u2217,b\u2217). Obviously,\nthe optimal values of parameters de\ufb01ne the model that makes the most accurate predictions.\nYou could have noticed that the form of our linear model in eq. 1 is very similar to the form\nof the SVM model. The only di\ufb00erence is the missing signoperator. The two models are\nindeed similar. However, the hyperplane in the SVM plays the role of the decision boundary:\nit\u2019s used to separate two groups of examples from one another. As such, it has to be as far\nfrom each group as possible.\nOn the other hand, the hyperplane in linear regression is chosen to be as close to all training\nexamples as possible.\nYou can see why this latter requirement is essential by looking at the illustration in Figure\n1. It displays the regression line (in red) for one-dimensional examples (blue dots). We\ncan use this line to predict the value of the target ynewfor a new unlabeled input example\nxnew. If our examples are D-dimensional feature vectors (for D > 1), the only di\ufb00erence\n1To say that yiis real-valued, we write yi\u2208R, where Rdenotes the set of all real numbers, an in\ufb01nite set\nof numbers from minus in\ufb01nity to plus in\ufb01nity.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 3", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2348, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8390815e-ba1b-49cf-a931-f9d8039fd2f2": {"__data__": {"id_": "8390815e-ba1b-49cf-a931-f9d8039fd2f2", "embedding": null, "metadata": {"page_label": "23", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6391795a-8c64-4f4c-a62e-922cfd00a928", "node_type": "4", "metadata": {"page_label": "23", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "a86812a40d79aa0af45d7477316b44a70728d50d24e4e0f0529c09278e8c594e", "class_name": "RelatedNodeInfo"}}, "text": "Figure 1: Linear Regression for one-dimensional examples.\nwith the one-dimensional case is that the regression model is not a line but a plane (for two\ndimensions) or a hyperplane (for D> 2).\nNow you see why it\u2019s essential to have the requirement that the regression hyperplane lies as\nclose to the training examples as possible: if the red line in Figure 1 was far from the blue\ndots, the prediction ynewwould have fewer chances to be correct.\n3.1.2 Solution\nTo get this latter requirement satis\ufb01ed, the optimization procedure which we use to \ufb01nd the\noptimal values for w\u2217andb\u2217tries to minimize the following expression:\n1\nN\u2211\ni=1...N(fw,b(xi)\u2212yi)2. (2)\nIn mathematics, the expression we minimize or maximize is called an objective function, or,\nsimply, an objective. The expression (fw,b(xi)\u2212yi)2in the above objective is called the loss\nfunction . It\u2019s a measure of penalty for misclassi\ufb01cation of example i. This particular choice\nof the loss function is called squared error loss . All model-based learning algorithms have\na loss function and what we do to \ufb01nd the best model is we try to minimize the objective\nknown as the cost function . In linear regression, the cost function is given by the average\nloss, also called the empirical risk . The average loss, or empirical risk, for a model, is the\naverage of all penalties obtained by applying the model to the training data.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 4", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1445, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "378763e0-901e-4403-b815-db458afaf31e": {"__data__": {"id_": "378763e0-901e-4403-b815-db458afaf31e", "embedding": null, "metadata": {"page_label": "24", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "92edad98-3281-46f2-968f-5a6a78cde3e2", "node_type": "4", "metadata": {"page_label": "24", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "c1bcf00b4fded462af7490fc98798018f56a88990b353a7a9f25128f96667edb", "class_name": "RelatedNodeInfo"}}, "text": "Why is the loss in linear regression a quadratic function? Why couldn\u2019t we get the absolute\nvalue of the di\ufb00erence between the true target yiand the predicted value f(xi)and use that\nas a penalty? We could. Moreover, we also could use a cube instead of a square.\nNow you probably start realizing how many seemingly arbitrary decisions are made when we\ndesign a machine learning algorithm: we decided to use the linear combination of features to\npredict the target. However, we could use a square or some other polynomial to combine the\nvalues of features. We could also use some other loss function that makes sense: the absolute\ndi\ufb00erence between f(xi)andyimakes sense, the cube of the di\ufb00erence too; the binary loss\n(1whenf(xi)andyiare di\ufb00erent and 0when they are the same) also makes sense, right?\nIf we made di\ufb00erent decisions about the form of the model, the form of the loss function,\nand about the choice of the algorithm that minimizes the average loss to \ufb01nd the best values\nof parameters, we would end up inventing a di\ufb00erent machine learning algorithm. Sounds\neasy, doesn\u2019t it? However, do not rush to invent a new learning algorithm. The fact that it\u2019s\ndi\ufb00erent doesn\u2019t mean that it will work better in practice.\nPeople invent new learning algorithms for one of the two main reasons:\n1.Thenewalgorithmsolvesaspeci\ufb01cpracticalproblembetterthantheexistingalgorithms.\n2.The new algorithm has better theoretical guarantees on the quality of the model it\nproduces.\nOne practical justi\ufb01cation of the choice of the linear form for the model is that it\u2019s simple.\nWhy use a complex model when you can use a simple one? Another consideration is that\nlinear models rarely over\ufb01t. Over\ufb01tting is the property of a model such that the model\npredicts very well labels of the examples used during training but frequently makes errors\nwhen applied to examples that weren\u2019t seen by the learning algorithm during training.\nAn example of over\ufb01tting in regression is shown in Figure 2. The data used to build the\nred regression line is the same as in Figure 1. The di\ufb00erence is that this time, this is the\npolynomial regression with a polynomial of degree 10. The regression line predicts almost\nperfectly the targets almost all training examples, but will likely make signi\ufb01cant errors on\nnew data, as you can see in Figure 1 for xnew. We talk more about over\ufb01tting and how to\navoid it Chapter 5.\nNow you know why linear regression can be useful: it doesn\u2019t over\ufb01t much. But what\nabout the squared loss? Why did we decide that it should be squared? In 1705, the French\nmathematician Adrien-Marie Legendre, who \ufb01rst published the sum of squares method for\ngauging the quality of the model stated that squaring the error before summing is convenient .\nWhy did he say that? The absolute value is not convenient, because it doesn\u2019t have a\ncontinuous derivative, which makes the function not smooth. Functions that are not smooth\ncreate unnecessary di\ufb03culties when employing linear algebra to \ufb01nd closed form solutions\nto optimization problems. Closed form solutions to \ufb01nding an optimum of a function are\nsimple algebraic expressions and are often preferable to using complex numerical optimization\nmethods, such as gradient descent (used, among others, to train neural networks).\nIntuitively, squared penalties are also advantageous because they exaggerate the di\ufb00erence\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 5", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3421, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "171aaa29-6868-4a13-9a86-eecd0dac2811": {"__data__": {"id_": "171aaa29-6868-4a13-9a86-eecd0dac2811", "embedding": null, "metadata": {"page_label": "25", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "529005d4-ff38-410a-8c57-399b16b82484", "node_type": "4", "metadata": {"page_label": "25", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "9c3c48b13aa81b1cfd403852424b32acb3342b7b3844e208ce844d23cc0c8e44", "class_name": "RelatedNodeInfo"}}, "text": "Figure 2: Over\ufb01tting.\nbetween the true target and the predicted one according to the value of this di\ufb00erence. We\nmight also use the powers 3 or 4, but their derivatives are more complicated to work with.\nFinally, why do we care about the derivative of the average loss? If we can calculate the\ngradient of the function in eq. 2, we can then set this gradient to zero2and \ufb01nd the solution\nto a system of equations that gives us the optimal values w\u2217andb\u2217.\n3.2 Logistic Regression\nThe \ufb01rst thing to say is that logistic regression is not a regression, but a classi\ufb01cation learning\nalgorithm. The name comes from statistics and is due to the fact that the mathematical\nformulation of logistic regression is similar to that of linear regression.\nI explain logistic regression on the case of binary classi\ufb01cation. However, it can naturally be\nextended to multiclass classi\ufb01cation.\n3.2.1 Problem Statement\nIn logistic regression, we still want to model yias a linear function of xi, however, with a\nbinaryyithis is not straightforward. The linear combination of features such as wxi+bis a\nfunction that spans from minus in\ufb01nity to plus in\ufb01nity, while yihas only two possible values.\n2To \ufb01nd the minimum or the maximum of a function, we set the gradient to zero because the value of the\ngradient at extrema of a function is always zero. In 2D, the gradient at an extremum is a horizontal line.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 6", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1449, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b721aae6-72f1-4b58-bc87-b6109094c0e1": {"__data__": {"id_": "b721aae6-72f1-4b58-bc87-b6109094c0e1", "embedding": null, "metadata": {"page_label": "26", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3ae2fc96-95f8-4af2-b2d5-1f343bbd6cd8", "node_type": "4", "metadata": {"page_label": "26", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "a846215a5505f84579c1a4a390914d48356cda71d79b93fc22fab9c8ce772874", "class_name": "RelatedNodeInfo"}}, "text": "At the time where the absence of computers required scientists to perform manual calculations,\nthey were eager to \ufb01nd a linear classi\ufb01cation model. They \ufb01gured out that if we de\ufb01ne a\nnegative label as 0and the positive label as 1, we would just need to \ufb01nd a simple continuous\nfunction whose codomain is (0,1). In such a case, if the value returned by the model for\ninput xis closer to 0, then we assign a negative label to x; otherwise, the example is labeled\nas positive. One function that has such a property is the standard logistic function (also\nknown as the sigmoid function ):\nf(x) =1\n1 +e\u2212x,\nwhereeis the base of the natural logarithm (also called Euler\u2019s number ;exis also known as\ntheexp(x)function in programming languages). Its graph is depicted in Figure 3.\nThe logistic regression model looks like this:\nfw,b(x)def=1\n1 +e\u2212(wx+b). (3)\nYou can see the familiar term wx+bfrom linear regression.\nBy looking at the graph of the standard logistic function, we can see how well it \ufb01ts our\nclassi\ufb01cation purpose: if we optimize the values of wandbappropriately, we could interpret\nthe output of f(x)as the probability of yibeing positive. For example, if it\u2019s higher than or\nequal to the threshold 0.5we would say that the class of xis positive; otherwise, it\u2019s negative.\nIn practice, the choice of the threshold could be di\ufb00erent depending on the problem. We\nreturn to this discussion in Chapter 5 when we talk about model performance assessment.\nNow, how do we \ufb01nd optimal w\u2217andb\u2217? In linear regression, we minimized the empirical\nrisk which was de\ufb01ned as the average squared error loss, also known as the mean squared\nerroror MSE.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1703, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5043bf18-90e8-4d1c-bf6a-6915f07b3627": {"__data__": {"id_": "5043bf18-90e8-4d1c-bf6a-6915f07b3627", "embedding": null, "metadata": {"page_label": "27", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "05c91038-c4fd-45dc-bb30-fe11f6af60d8", "node_type": "4", "metadata": {"page_label": "27", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "fe2da8d451bff20ab434296b05c95712cb086718d7dc8e186edffaca63205c3b", "class_name": "RelatedNodeInfo"}}, "text": "6\n 4\n 2\n 0 2 4 6\nx0.00.20.40.60.81.0f(x)Figure 3: Standard logistic function.\n3.2.2 Solution\nIn logistic regression, on the other hand, we maximize the likelihood of our training set\naccording to the model. In statistics, the likelihood function de\ufb01nes how likely the observation\n(an example) is according to our model.\nFor instance, let\u2019s have a labeled example (xi,yi)in our training data. Assume also that we\nfound (guessed) some speci\ufb01c values \u02c6 wand\u02c6bof our parameters. If we now apply our model\nf\u02c6w,\u02c6btoxiusing eq. 3 we will get some value 0<p< 1as output. If yiis the positive class,\nthe likelihood of yibeing the positive class, according to our model, is given by p. Similarly,\nifyiis the negative class, the likelihood of it being the negative class is given by 1\u2212p.\nThe optimization criterion in logistic regression is called maximum likelihood . Instead of\nminimizing the average loss, like in linear regression, we now maximize the likelihood of the\ntraining data according to our model:\nLw,bdef=\u220f\ni=1...Nfw,b(xi)yi(1\u2212fw,b(xi))(1\u2212yi). (4)\nThe expression fw,b(x)yi(1\u2212fw,b(x))(1\u2212yi)may look scary but it\u2019s just a fancy mathematical\nway of saying: \u201c fw,b(x)whenyi= 1and(1\u2212fw,b(x))otherwise\u201d. Indeed, if yi= 1, then\n(1\u2212fw,b(x))(1\u2212yi)equals 1because (1\u2212yi) = 0and we know that anything power 0equals\n1. On the other hand, if yi= 0, thenfw,b(x)yiequals 1for the same reason.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 8", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1444, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f62ce4f-1e0f-4597-81df-aa14959f78f8": {"__data__": {"id_": "9f62ce4f-1e0f-4597-81df-aa14959f78f8", "embedding": null, "metadata": {"page_label": "28", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f62527f3-faf3-4cb1-a502-bcb3672b72b2", "node_type": "4", "metadata": {"page_label": "28", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "37e44519da2fd3e08b46f72721e8df1dceb7b55f5bec1f0282d233a22a690352", "class_name": "RelatedNodeInfo"}}, "text": "You may have noticed that we used the product operator\u220fin the objective function instead\nof the sum operator\u2211which was used in linear regression. It\u2019s because the likelihood of\nobservingNlabels forNexamples is the product of likelihoods of each observation (assuming\nthat all observations are independent of one another, which is the case). You can draw\na parallel with the multiplication of probabilities of outcomes in a series of independent\nexperiments in the probability theory.\nBecause of the expfunction used in the model, in practice, it\u2019s more convenient to maximize\nthelog-likelihood instead of likelihood. The log-likelihood is de\ufb01ned like follows:\nLogL w,bdef= ln(L(w,b(x)) =N\u2211\ni=1yilnfw,b(x) + (1\u2212yi) ln (1\u2212fw,b(x)).\nBecause lnis astrictly increasing function , maximizing this function is the same as maximizing\nits argument, and the solution to this new optimization problem is the same as the solution\nto the original problem.\nContrary to linear regression, there\u2019s no closed form solution to the above optimization\nproblem. A typical numerical optimization procedure used in such cases is gradient descent .\nWe talk about it in the next chapter.\n3.3 Decision Tree Learning\nA decision tree is an acyclic graphthat can be used to make decisions. In each branching\nnode of the graph, a speci\ufb01c feature jof the feature vector is examined. If the value of the\nfeature is below a speci\ufb01c threshold, then the left branch is followed; otherwise, the right\nbranch is followed. As the leaf node is reached, the decision is made about the class to which\nthe example belongs.\nAs the title of the section suggests, a decision tree can be learned from data.\n3.3.1 Problem Statement\nLike previously, we have a collection of labeled examples; labels belong to the set {0,1}. We\nwant to build a decision tree that would allow us to predict the class given a feature vector.\n3.3.2 Solution\nThere are various formulations of the decision tree learning algorithm. In this book, we\nconsider just one, called ID3.\nThe optimization criterion, in this case, is the average log-likelihood:\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 9", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2144, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b96b7a7b-a003-4363-8382-49651fd7e713": {"__data__": {"id_": "b96b7a7b-a003-4363-8382-49651fd7e713", "embedding": null, "metadata": {"page_label": "29", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "896b708d-b98a-4393-bde9-b572afdd793f", "node_type": "4", "metadata": {"page_label": "29", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "04e60b2a8b30a329a9d23933c8a2d753fb980a1a734ba4f0b4834eb88de2d961", "class_name": "RelatedNodeInfo"}}, "text": "1\nNN\u2211\ni=1yilnfID3(xi) + (1\u2212yi) ln (1\u2212fID3(xi)), (5)\nwherefID3is a decision tree.\nBy now, it looks very similar to logistic regression. However, contrary to the logistic regression\nlearning algorithm which builds a parametric model fw\u2217,b\u2217by \ufb01nding an optimal solution\nto the optimization criterion, the ID3 algorithm optimizes it approximately by constructing a\nnonparametric model fID3(x)def= Pr(y= 1|x).\nS={( x1, \u00a0 y1), \u00a0( x2, \u00a0 y2),\u00a0( x3,\u00a0 y3),\n( x4,\u00a0 y4),\u00a0( x5,\u00a0 y5),\u00a0( x6,\u00a0 y6),\n( x7,\u00a0 y7),\u00a0( x8, \u00a0 y8),\u00a0( x9, \u00a0 y9),\n( x1 0,\u00a0 y1 0),\u00a0( x1 1,\u00a0y1 1),\u00a0( x1 2,\u00a0 y1 2)}x\nPr( y\u00a0=\u00a01 | x) \u00a0=\u00a0( y1 +y2 +y3 +y4 +y5 \u00a0\n+y6 +y7 +y8 +y9 +y1 0 +y1 1 +y1 2)/12\nPr( y\u00a0=\u00a01 | x)\n(a)\nx\nPr( y\u00a0=\u00a0 1 | x ) \u00a0=\u00a0 ( y1 +y2 +y4 \u00a0\n+y6 +y7 +y8 +y9 )/7\nPr( y\u00a0=\u00a0 1 | x )x( 3 ) \u00a0<\u00a018.3?\nS\u00ad \u00a0 =\u00a0{ ( x1 , \u00a0 y1 ) , \u00a0 ( x2 , \u00a0 y2 ),\n( x4 , \u00a0 y4 ),\u00a0 ( x6 , \u00a0 y6 ),\u00a0( x7 , \u00a0 y7 ),\n( x8 , \u00a0 y8 ), \u00a0( x9 , \u00a0 y9 ) } \u00a0\nPr( y\u00a0=\u00a0 1 | x ) \u00a0=\n( y3 +y5 +y1 0 + y1 1 + y1 2 )/5\nPr( y\u00a0=\u00a0 1 | x )S+ \u00a0 =\u00a0{ ( x3 , \u00a0 y3 ),\u00a0 ( x5 , \u00a0 y5 ),\u00a0( x1 0 , \u00a0 y1 0 ),\n( x1 1 ,\u00a0 y1 1 ),\u00a0( x1 2 ,\u00a0 y1 2 ) } \u00a0Y es No (b)\nFigure 4: An illustration of a decision tree building algorithm. The set Scontains 12labeled\nexamples. (a) In the beginning, the decision tree only contains the start node; it makes the\nsame prediction for any input. (b) The decision tree after the \ufb01rst split; it tests whether\nfeature 3is less than 18.3and, depending on the result, the prediction is made in one of the\ntwo leaf nodes.\nThe ID3 learning algorithm works as follows. Let Sdenote a set of labeled examples. In the\nbeginning, thedecisiontreeonlyhasastartnodethatcontainsallexamples: Sdef={(xi,yi)}N\ni=1.\nStart with a constant model fS\nID3de\ufb01ned as,\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 10", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1732, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "160e5079-55d8-4dd2-8bf5-c61f2331bcf3": {"__data__": {"id_": "160e5079-55d8-4dd2-8bf5-c61f2331bcf3", "embedding": null, "metadata": {"page_label": "30", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d714854-2f05-4db6-a0ae-010d0c7207dd", "node_type": "4", "metadata": {"page_label": "30", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "29593a5c02372efb8e575002be886e8f580cb429ed243511abfbe0fe9ed3c33d", "class_name": "RelatedNodeInfo"}}, "text": "fS\nID3def=1\n|S|\u2211\n(x,y)\u2208Sy. (6)\nThe prediction given by the above model, fS\nID3(x), would be the same for any input x. The\ncorresponding decision tree built using a toy dataset of twelve labeled examples is shown in\n\ufb01g 4a.\nThen we search through all features j= 1,...,Dand all thresholds t, and split the set S\ninto two subsets: S\u2212def={(x,y)|(x,y)\u2208S,x(j)<t}andS+def={(x,y)|(x,y)\u2208S,x(j)\u2265t}.\nThe two new subsets would go to two new leaf nodes, and we evaluate, for all possible pairs\n(j,t)how good the split with pieces S\u2212andS+is. Finally, we pick the best such values (j,t),\nsplitSintoS+andS\u2212, form two new leaf nodes, and continue recursively on S+andS\u2212(or\nquit if no split produces a model that\u2019s su\ufb03ciently better than the current one). A decision\ntree after one split is illustrated in \ufb01g 4b.\nNow you should wonder what do the words \u201cevaluate how good the split is\u201d mean. In ID3, the\ngoodness of a split is estimated by using the criterion called entropy. Entropy is a measure of\nuncertainty about a random variable. It reaches its maximum when all values of the random\nvariables are equiprobable. Entropy reaches its minimum when the random variable can have\nonly one value. The entropy of a set of examples Sis given by,\nH(S)def=\u2212fS\nID3lnfS\nID3\u2212(1\u2212fS\nID3) ln(1\u2212fS\nID3).\nWhen we split a set of examples by a certain feature jand a threshold t, the entropy of a\nsplit,H(S\u2212,S+), is simply a weighted sum of two entropies:\nH(S\u2212,S+)def=|S\u2212|\n|S|H(S\u2212) +|S+|\n|S|H(S+). (7)\nSo, in ID3, at each step, at each leaf node, we \ufb01nd a split that minimizes the entropy given\nby eq. 7 or we stop at this leaf node.\nThe algorithm stops at a leaf node in any of the below situations:\n\u2022All examples in the leaf node are classi\ufb01ed correctly by the one-piece model (eq. 6).\n\u2022We cannot \ufb01nd an attribute to split upon.\n\u2022The split reduces the entropy less than some \u03f5(the value for which has to be found\nexperimentally3).\n\u2022The tree reaches some maximum depth d(also has to be found experimentally).\nBecause in ID3, the decision to split the dataset on each iteration is local (doesn\u2019t depend\non future splits), the algorithm doesn\u2019t guarantee an optimal solution. The model can be\n3In Chapter 5, I show how to do that in the section on hyperparameter tuning.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 11", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2300, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b5b4c5d0-d013-4f2c-971a-dbc62905ea04": {"__data__": {"id_": "b5b4c5d0-d013-4f2c-971a-dbc62905ea04", "embedding": null, "metadata": {"page_label": "31", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cac1dd84-9e09-4d57-8b7e-9f26057bd77e", "node_type": "4", "metadata": {"page_label": "31", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "896a22abdba986104f6c47ded1b656d552f9b6ea1c01a02bd952542fd352eecf", "class_name": "RelatedNodeInfo"}}, "text": "improved by using techniques like backtracking during the search for the optimal decision\ntree at the cost of possibly taking longer to build a model.\nThe most widely used formulation of a decision tree learning algorithm is called C4.5. It has\nseveral additional features as compared to ID3:\n\u2022it accepts both continuous and discrete features;\n\u2022it handles incomplete examples;\n\u2022it solves over\ufb01tting problem by using a bottom-up technique known as \u201cpruning\u201d.\nPruning consists of going back through the tree once it\u2019s been\ncreated and removing branches that don\u2019t contribute signi\ufb01cantly\nenough to the error reduction by replacing them with leaf nodes.\nThe entropy-based split criterion intuitively makes sense: entropy\nreaches its minimum of 0when all examples in Shave the same\nlabel; on the other hand, the entropy is at its maximum of 1when\nexactly one-half of examples in Sis labeled with 1, making such a\nleaf useless for classi\ufb01cation. The only remaining question is how\nthis algorithm approximately maximizes the average log-likelihood\ncriterion. I leave it for further reading.\n3.4 Support Vector Machine\nI already presented SVM in the introduction, so this section only \ufb01lls a couple of blanks.\nTwo critical questions need to be answered:\n1.What if there\u2019s noise in the data and no hyperplane can perfectly separate positive\nexamples from negative ones?\n2.What if the data cannot be separated using a plane, but could be separated by a\nhigher-order polynomial?\nYou can see both situations depicted in Figure 5. In the left case, the data could be separated\nby a straight line if not for the noise (outliers or examples with wrong labels). In the right\ncase, the decision boundary is a circle and not a straight line.\nRemember that in SVM, we want to satisfy the following constraints:\nwxi\u2212b\u2265+1ifyi= +1,\nwxi\u2212b\u2264\u22121ifyi=\u22121.(8)\nWe also want to minimize \u2225w\u2225so that the hyperplane is equally distant from the closest\nexamples of each class. Minimizing \u2225w\u2225is equivalent to minimizing1\n2||w||2, and the use of\nthis term makes it possible to perform quadratic programming optimization later on. The\noptimization problem for SVM, therefore, looks like this:\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 12", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2218, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5b7ea7af-c383-4743-815e-c56b6fab8543": {"__data__": {"id_": "5b7ea7af-c383-4743-815e-c56b6fab8543", "embedding": null, "metadata": {"page_label": "32", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e255d32d-2304-43e8-a73a-84dc3a995e17", "node_type": "4", "metadata": {"page_label": "32", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "6324d6aa39ec2d8510d529c46b83f84da0a426b2bb1ded8f729bbeadea8d31c4", "class_name": "RelatedNodeInfo"}}, "text": "0 2 4 6 8 10 1202468\n10\n 5\n 0 5 1010\n5\n0510Figure 5: Linearly non-separable cases. Left: the presence of noise. Right: inherent\nnonlinearity.\nmin1\n2||w||2,such thatyi(xiw\u2212b)\u22121\u22650,i= 1,...,N. (9)\n3.4.1 Dealing with Noise\nTo extend SVM to cases in which the data is not linearly separable, we introduce the hinge\nlossfunction: max (0,1\u2212yi(wxi\u2212b)).\nThe hinge loss function is zero if the constraints in 8 are satis\ufb01ed; in other words, if wxi\nlies on the correct side of the decision boundary. For data on the wrong side of the decision\nboundary, the function\u2019s value is proportional to the distance from the decision boundary.\nWe then wish to minimize the following cost function,\nC\u2225w\u22252+1\nNN\u2211\ni=1max (0,1\u2212yi(wxi\u2212b)),\nwhere the hyperparameter Cdetermines the tradeo\ufb00 between increasing the size of the\ndecision boundary and ensuring that each xilies on the correct side of the decision boundary.\nThe value of Cis usually chosen experimentally, just like ID3\u2019s hyperparameters \u03f5andd.\nSVMs that optimize hinge loss are called soft-margin SVMs, while the original formulation is\nreferred to as a hard-margin SVM.\nAs you can see, for su\ufb03ciently high values of C, the second term in the cost function will\nbecome negligible, so the SVM algorithm will try to \ufb01nd the highest margin by completely\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 13", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1348, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d28791c8-40ec-42d5-b583-c80855cf254e": {"__data__": {"id_": "d28791c8-40ec-42d5-b583-c80855cf254e", "embedding": null, "metadata": {"page_label": "33", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3a49a07b-e5f0-4d89-9143-35441564041b", "node_type": "4", "metadata": {"page_label": "33", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "0e6819eaa1b49283e83a6c683dd3a60a62c9ea96a1b379b9cf2f6b96c36c305b", "class_name": "RelatedNodeInfo"}}, "text": "ignoring misclassi\ufb01cation. As we decrease the value of C, making classi\ufb01cation errors is\nbecoming more costly, so the SVM algorithm tries to make fewer mistakes by sacri\ufb01cing\nthe margin size. As we have already discussed, a larger margin is better for generalization.\nTherefore, Cregulates the tradeo\ufb00 between classifying the training data well (minimizing\nempirical risk) and classifying future examples well (generalization).\n3.4.2 Dealing with Inherent Non-Linearity\nSVM can be adapted to work with datasets that cannot be separated by a hyperplane in\nits original space. Indeed, if we manage to transform the original space into a space of\nhigher dimensionality, we could hope that the examples will become linearly separable in this\ntransformed space. In SVMs, using a function to implicitly transform the original space into\na higher dimensional space during the cost function optimization is called the kernel trick .\n0 20 40 60 80 10075\n075020406080100120\nFigure 6: The data from Figure 5 (right) becomes linearly separable after a transformation\ninto a three-dimensional space.\nThe e\ufb00ect of applying the kernel trick is illustrated in Figure 6. As you can see, it\u2019s possible\nto transform a two-dimensional non-linearly-separable data into a linearly-separable three-\ndimensional data using a speci\ufb01c mapping \u03c6:x\u21a6\u2192\u03c6(x), where\u03c6(x)is a vector of higher\ndimensionality than x. For the example of 2D data in Figure 5 (right), the mapping \u03c6\nfor that projects a 2D example x= [q,p]into a 3D space (Figure 6) would look like this:\n\u03c6([q,p])def=(q2,\u221a\n2qp,p2), where\u00b72means\u00b7squared. You see now that the data becomes\nlinearly separable in the transformed space.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 14", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1723, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "be60ad8c-2f1f-442d-9516-2264a1301be3": {"__data__": {"id_": "be60ad8c-2f1f-442d-9516-2264a1301be3", "embedding": null, "metadata": {"page_label": "34", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e6252d24-3ec6-433c-bf6c-71387eb1ef4c", "node_type": "4", "metadata": {"page_label": "34", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "fb8fa0d788bd76854a4c934f4c98950d36bfcd971f625948dbf7cc7727b0eedc", "class_name": "RelatedNodeInfo"}}, "text": "However, we don\u2019t know a priori which mapping \u03c6would work for our data. If we \ufb01rst\ntransform all our input examples using some mapping into very high dimensional vectors and\nthen apply SVM to this data, and we try all possible mapping functions, the computation\ncould become very ine\ufb03cient, and we would never solve our classi\ufb01cation problem.\nFortunately, scientists \ufb01gured out how to use kernel functions (or, simply, kernels) to\ne\ufb03ciently work in higher-dimensional spaces without doing this transformation explicitly . To\nunderstand how kernels work, we have to see \ufb01rst how the optimization algorithm for SVM\n\ufb01nds the optimal values for wandb.\nThe method traditionally used to solve the optimization problem in eq. 9 is the method of\nLagrange multipliers . Instead of solving the original problem from eq. 9, it is convenient to\nsolve an equivalent problem formulated like this:\nmax\n\u03b11...\u03b1NN\u2211\ni=1\u03b1i\u22121\n2N\u2211\ni=1N\u2211\nk=1yi\u03b1i(xixk)yk\u03b1ksubject toN\u2211\ni=1\u03b1iyi= 0and\u03b1i\u22650,i= 1,...,N,\nwhere\u03b1iare called Lagrange multipliers. When formulated like this, the optimization\nproblem becomes a convex quadratic optimization problem, e\ufb03ciently solvable by quadratic\nprogramming algorithms.\nNow, you could have noticed that in the above formulation, there is a term xixk, and this is\nthe only place where the feature vectors are used. If we want to transform our vector space\ninto higher dimensional space, we need to transform xiinto\u03c6(xi)andxkinto\u03c6(xk)and\nthen multiply \u03c6(xi)and\u03c6(xk). Doing so would be very costly.\nOn the other hand, we are only interested in the result of the dot-product xixk, which, as\nwe know, is a real number. We don\u2019t care how this number was obtained as long as it\u2019s\ncorrect. By using the kernel trick, we can get rid of a costly transformation of original\nfeature vectors into higher-dimensional vectors and avoid computing their dot-product. We\nreplace that by a simple operation on the original feature vectors that gives the same\nresult. For example, instead of transforming (q1,p1)into (q2\n1,\u221a\n2q1p1,p2\n1)and(q2,p2)into\n(q2\n2,\u221a\n2q2p2,p2\n2)and then computing the dot-product of (q2\n1,\u221a\n2q1p1,p2\n1)and(q2\n2,\u221a\n2q2p2,p2\n2)\nto obtain (q2\n1q2\n2+2q1q2p1p2+p2\n1p2\n2)we could \ufb01nd the dot-product between (q1,p1)and(q2,p2)\nto get (q1q2+p1p2)and then square it to get exactly the same result (q2\n1q2\n2+2q1q2p1p2+p2\n1p2\n2).\nThat was an example of the kernel trick, and we used the quadratic kernel k(xi,xk)def=(xixk)2.\nMultiple kernel functions exist, the most widely used of which is the RBF kernel :\nk(x,x\u2032) = exp(\n\u2212\u2225x\u2212x\u2032\u22252\n2\u03c32)\n,\nwhere\u2225x\u2212x\u2032\u22252is the squared Euclidean distance between two feature vectors. The\nEuclidean distance is given by the following equation:\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2731, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b80e2dda-07a0-445c-a92c-c45d51495ad5": {"__data__": {"id_": "b80e2dda-07a0-445c-a92c-c45d51495ad5", "embedding": null, "metadata": {"page_label": "35", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "231d88f9-4a99-41a2-9b70-2686e067d0d4", "node_type": "4", "metadata": {"page_label": "35", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "da89afaf1ab0362abcafcb75f9ee66fcfccbbe59701d4281a9ecdada4e134e42", "class_name": "RelatedNodeInfo"}}, "text": "d(xi,xk)def=\u220f(\nx(1)\ni\u2212x(1)\nk)2\n+(\nx(2)\ni\u2212x(2)\nk)2\n+\u00b7\u00b7\u00b7+(\nx(N)\ni\u2212x(N)\nk)2\n=\ued6a\ued6b\ued6b\u221aD\u2211\nj=1(\nx(j)\ni\u2212x(j)\nk)2\n.\nIt can be shown that the feature space of the RBF (for \u201cradial basis function\u201d) kernel has\nan in\ufb01nite number of dimensions. By varying the hyperparameter \u03c3, the data analyst can\nchoose between getting a smooth or curvy decision boundary in the original space.\n3.5 k-Nearest Neighbors\nk-Nearest Neighbors (kNN) is a non-parametric learning algorithm. Contrary to other\nlearning algorithms that allow discarding the training data after the model is built, kNN\nkeeps all training examples in memory. Once a new, previously unseen example xcomes in,\nthe kNN algorithm \ufb01nds ktraining examples closest to xand returns the majority label, in\ncase of classi\ufb01cation, or the average label, in case of regression.\nThe closeness of two examples is given by a distance function. For example, Euclidean\ndistance seen above is frequently used in practice. Another popular choice of the distance\nfunction is the negative cosine similarity . Cosine similarity de\ufb01ned as,\ns(xi,xk)def= cos( \u2220(xi,xk)) =\u2211D\nj=1x(j)\nix(j)\nk\u220f\n\u2211D\nj=1(\nx(j)\ni)2\u220f\n\u2211D\nj=1(\nx(j)\nk)2,\nis a measure of similarity of the directions of two vectors. If the angle between two vectors\nis0degrees, then two vectors point to the same direction, and cosine similarity is equal to\n1. If the vectors are orthogonal, the cosine similarity is 0. For vectors pointing in opposite\ndirections, the cosine similarity is \u22121. If we want to use cosine similarity as a distance metric,\nwe need to multiply it by \u22121. Other popular distance metrics include Chebychev distance,\nMahalanobis distance, and Hamming distance. The choice of the distance metric, as well as\nthe value for k, are the choices the analyst makes before running the algorithm. So these\nare hyperparameters. The distance metric could also be learned from data (as opposed to\nguessing it). We talk about that in Chapter 10.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 16", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1991, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c902e9a2-14e3-4f69-85c5-01da062b492d": {"__data__": {"id_": "c902e9a2-14e3-4f69-85c5-01da062b492d", "embedding": null, "metadata": {"page_label": "36", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "77bca458-a819-4b0e-ac9b-740545893ab4", "node_type": "4", "metadata": {"page_label": "36", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "35c02fff9a4b0184ea2e7b67b8630a13da95a68cd244e92cec1cebe01c5c23b4", "class_name": "RelatedNodeInfo"}}, "text": "4 Anatomy of a Learning Algorithm\n4.1 Building Blocks of a Learning Algorithm\nYou may have noticed by reading the previous chapter that each learning algorithm we saw\nconsisted of three parts:\n1) a loss function;\n2)an optimization criterion based on the loss function (a cost function, for example); and\n3)an optimization routine leveraging training data to \ufb01nd a solution to the optimization\ncriterion.\nThese are the building blocks of any learning algorithm. You saw in the previous chapter\nthat some algorithms were designed to explicitly optimize a speci\ufb01c criterion (both linear and\nlogistic regressions, SVM). Some others, including decision tree learning and kNN, optimize\nthe criterion implicitly. Decision tree learning and kNN are among the oldest machine\nlearning algorithms and were invented experimentally based on intuition, without a speci\ufb01c\nglobal optimization criterion in mind, and (like it often happened in scienti\ufb01c history) the\noptimization criteria were developed later to explain why those algorithms work.\nBy reading modern literature on machine learning, you often encounter references to gradient\ndescent orstochastic gradient descent . These are two most frequently used optimization\nalgorithms used in cases where the optimization criterion is di\ufb00erentiable.\nGradient descent is an iterative optimization algorithm for \ufb01nding the minimum of a function.\nTo \ufb01nd a localminimum of a function using gradient descent, one starts at some random\npoint and takes steps proportional to the negative of the gradient (or approximate gradient)\nof the function at the current point.\nGradient descent can be used to \ufb01nd optimal parameters for linear and logistic regression,\nSVM and also neural networks which we consider later. For many models, such as logistic\nregression or SVM, the optimization criterion is convex. Convex functions have only one\nminimum, which is global. Optimization criteria for neural networks are not convex, but in\npractice even \ufb01nding a local minimum su\ufb03ces.\nLet\u2019s see how gradient descent works.\n4.2 Gradient Descent\nIn this section, I demonstrate how gradient descent \ufb01nds the solution to a linear regression\nproblem1. I illustrate my description with Python code as well as with plots that show how\nthe solution improves after some iterations of gradient descent. I use a dataset with only\n1As you know, linear regression has a closed form solution. That means that gradient descent is not\nneeded to solve this speci\ufb01c type of problem. However, for illustration purposes, linear regression is a perfect\nproblem to explain gradient descent.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 3", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2648, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4802546c-8ee0-4757-abe6-33c7b68425af": {"__data__": {"id_": "4802546c-8ee0-4757-abe6-33c7b68425af", "embedding": null, "metadata": {"page_label": "37", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1aa2b63d-c630-48f0-9759-a1a20df01941", "node_type": "4", "metadata": {"page_label": "37", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "5004cc66518cc7e20942a0f9b6eade6cf799acc02797fdb624f9f5c03a5cad36", "class_name": "RelatedNodeInfo"}}, "text": "one feature. However, the optimization criterion will have two parameters: wandb. The\nextension to multi-dimensional training data is straightforward: you have variables w(1),w(2),\nandbfor two-dimensional data, w(1),w(2),w(3), andbfor three-dimensional data and so on.\n0 10 20 30 40 50\nSpendings, M$510152025Sales, UnitsSales as a function of radio ad spendings.\nFigure 1: The original data. The Y-axis corresponds to the sales in units (the quantity we\nwant to predict), the X-axis corresponds to our feature: the spendings on radio ads in M$.\nTo give a practical example, I use the real dataset (can be found on the book\u2019s wiki) with the\nfollowing columns: the Spendings of various companies on radio advertising each year and\ntheir annual Sales in terms of units sold. We want to build a regression model that we can\nuse to predict units sold based on how much a company spends on radio advertising. Each\nrow in the dataset represents one speci\ufb01c company:\nCompany Spendings, M$ Sales, Units\n1 37.8 22.1\n2 39.3 10.4\n3 45.9 9.3\n4 41.3 18.5\n.. .. ..\nWe have data for 200 companies, so we have 200 training examples in the form (xi,yi) =\n(Spendings i,Sales i). Figure 1 shows all examples on a 2D plot.\nRemember that the linear regression model looks like this: f(x) =wx+b. We don\u2019t know\nwhat the optimal values for wandbare and we want to learn them from data. To do that,\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 4", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1435, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c5f6b640-7eb9-407c-9380-262be5fcc394": {"__data__": {"id_": "c5f6b640-7eb9-407c-9380-262be5fcc394", "embedding": null, "metadata": {"page_label": "38", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "57646547-3bed-458b-9a97-9d67154c7286", "node_type": "4", "metadata": {"page_label": "38", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "67a8263d278e18cd7e06ad932dcc37df49818c231791b9fbd5d064aad77be7f7", "class_name": "RelatedNodeInfo"}}, "text": "we look for such values for wandbthat minimize the mean squared error:\nldef=1\nNN\u2211\ni=1(yi\u2212(wxi+b))2.\nGradient descent starts with calculating the partial derivative for every parameter:\n\u2202l\n\u2202w=1\nNN\u2211\ni=1\u22122xi(yi\u2212(wxi+b));\n\u2202l\n\u2202b=1\nNN\u2211\ni=1\u22122(yi\u2212(wxi+b)).(1)\nTo \ufb01nd the partial derivative of the term (yi\u2212(wx+b))2with respect to wI applied the\nchain rule . Here, we have the chain f=f2(f1)wheref1=yi\u2212(wx+b)andf2=f2\n1. To \ufb01nd\na partial derivative of fwith respect to wwe have to \ufb01rst \ufb01nd the partial derivative of fwith\nrespect tof2which is equal to 2(yi\u2212(wx+b))(from calculus, we know that the derivative\n\u2202\n\u2202xx2= 2x) and then we have to multiply it by the partial derivative of yi\u2212(wx+b)with\nrespect towwhich is equal to\u2212x. So overall\u2202l\n\u2202w=1\nN\u2211N\ni=1\u22122xi(yi\u2212(wxi+b)). In a similar\nway, the partial derivative of lwith respect to b,\u2202l\n\u2202b, was calculated.\nGradient descent proceeds in epochs. An epoch consists of using the training set entirely to\nupdate each parameter. In the beginning, the \ufb01rst epoch, we initialize2w\u21900andb\u21900.\nThe partial derivatives,\u2202l\n\u2202wand\u2202l\n\u2202bgiven by eq. 1 equal, respectively,\u22122\nN\u2211N\ni=1xiyiand\n\u22122\nN\u2211N\ni=1yi. At each epoch, we update wandbusing partial derivatives. The learning rate \u03b1\ncontrols the size of an update:\nw\u2190w\u2212\u03b1\u2202l\n\u2202w;\nb\u2190b\u2212\u03b1\u2202l\n\u2202b.(2)\nWe subtract (as opposed to adding) partial derivatives from the values of parameters because\nderivatives are indicators of growth of a function. If a derivative is positive at some point3,\nthen the function grows at this point. Because we want to minimize the objective function,\n2In complex models, such as neural networks, which have thousands of parameters, the initialization of\nparameters may signi\ufb01cantly a\ufb00ect the solution found using gradient descent. There are di\ufb00erent initialization\nmethods (at random, with all zeroes, with small values around zero, and others) and it is an important choice\nthe data analyst has to make.\n3A point is given by the current values of parameters.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 5", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2012, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "552b268a-2e01-49f8-90c3-ce7149701707": {"__data__": {"id_": "552b268a-2e01-49f8-90c3-ce7149701707", "embedding": null, "metadata": {"page_label": "39", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "22ee1e88-5647-465a-b62c-61af6cdad8b4", "node_type": "4", "metadata": {"page_label": "39", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "eac08a297169a896aceeddf052d46bf59225887b0428ce6b4a86b884755c8c69", "class_name": "RelatedNodeInfo"}}, "text": "when the derivative is positive we know that we need to move our parameter in the opposite\ndirection (to the left on the axis of coordinates). When the derivative is negative (function is\ndecreasing), we need to move our parameter to the right to decrease the value of the function\neven more. Subtracting a negative value from a parameter moves it to the right.\nAt the next epoch, we recalculate partial derivatives using eq. 1 with the updated values of\nwandb; we continue the process until convergence. Typically, we need many epochs until we\nstart seeing that the values for wandbdon\u2019t change much after each epoch; then we stop.\nIt\u2019s hard to imagine a machine learning engineer who doesn\u2019t use Python. So, if you waited\nfor the right moment to start learning Python, this is that moment. Below, I show how to\nprogram gradient descent in Python.\nThe function that updates the parameters wandbduring one epoch is shown below:\n1def update_w_and_b(spendings, sales, w, b, alpha):\n2 dl_dw = 0.0\n3 dl_db = 0.0\n4 N = len(spendings)\n5\n6 for iinrange(N):\n7 dl_dw += -2*spendings[i]*(sales[i] - (w*spendings[i] + b))\n8 dl_db += -2*(sales[i] - (w*spendings[i] + b))\n9\n10 # update w and b\n11 w = w - (1/float(N))*dl_dw*alpha\n12 b = b - (1/float(N))*dl_db*alpha\n13\n14 return w, b\nThe function that loops over multiple epochs is shown below:\n15def train(spendings, sales, w, b, alpha, epochs):\n16 for einrange(epochs):\n17 w, b = update_w_and_b(spendings, sales, w, b, alpha)\n18\n19 # log the progress\n20 ife % 400 == 0:\n21 print(\"epoch:\", e, \"loss: \", avg_loss(spendings, sales, w, b))\n22\n23 return w, b\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 6", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1655, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fb43f2b9-c458-4c8a-9d71-e10e350b13c0": {"__data__": {"id_": "fb43f2b9-c458-4c8a-9d71-e10e350b13c0", "embedding": null, "metadata": {"page_label": "40", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8b6851e6-3986-426b-a835-95a001537067", "node_type": "4", "metadata": {"page_label": "40", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "29dfecc692e7cd59cc783d5c0b495d78faf32403f0158b3e41f9fd5150dde1ac", "class_name": "RelatedNodeInfo"}}, "text": "0 10 20 30 40 50051015202530Epoch 0\n0 10 20 30 40 50051015202530 Epoch 400\n0 10 20 30 40 50051015202530 Epoch 800\n0 10 20 30 40 50051015202530\nEpoch 1200\n0 10 20 30 40 50051015202530 Epoch 1600\n0 10 20 30 40 50051015202530 Epoch 3000\nFigure 2: The evolution of the regression line through gradient descent epochs.\nThe function avg_loss in the above code snippet is a function that computes the mean\nsquared error. It is de\ufb01ned as:\n25def avg_loss(spendings, sales, w, b):\n26 N = len(spendings)\n27 total_error = 0.0\n28 for iinrange(N):\n29 total_error += (sales[i] - (w*spendings[i] + b))**2\n30 return total_error / float(N)\nIf we run the trainfunction for \u03b1= 0.001,w= 0.0,b= 0.0, and 15,000 epochs, we will see\nthe following output (shown partially):\nepoch: 0 loss: 92.32078294903626\nepoch: 400 loss: 33.79131790081576\nepoch: 800 loss: 27.9918542960729\nepoch: 1200 loss: 24.33481690722147\nepoch: 1600 loss: 22.028754937538633\n...\nepoch: 2800 loss: 19.07940244306619\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1026, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd3adb36-d586-4f85-a2a0-6863cf86b86f": {"__data__": {"id_": "bd3adb36-d586-4f85-a2a0-6863cf86b86f", "embedding": null, "metadata": {"page_label": "41", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eca461b2-31dd-49f4-8e89-684463bb7d30", "node_type": "4", "metadata": {"page_label": "41", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "a17e651f185b04e2bf219687706c92f96a09a13006b56cef2e9b0149677c3f54", "class_name": "RelatedNodeInfo"}}, "text": "You can see that the average loss decreases as the trainfunction loops through epochs. Figure\n2 shows the evolution of the regression line through epochs.\nFinally, once we have found the optimal values of parameters wandb, the only missing piece\nis a function that makes predictions:\n31def predict(x, w, b):\n32 return w*x + b\nTry to execute the following code:\n33w, b = train(x, y, 0.0, 0.0, 0.001, 15000)\n34x_new = 23.0\n35y_new = predict(x_new, w, b)\n36print(y_new)\nThe output is 13.97.\nGradient descent is sensitive to the choice of the learning rate \u03b1. It is also slow for large\ndatasets. Fortunately, several signi\ufb01cant improvements to this algorithm have been proposed.\nMinibatch stochastic gradient descent (minibatch SGD) is a version of the algorithm\nthat speeds up the computation by approximating the gradient using smaller batches (subsets)\nof the training data. SGD itself has various \u201cupgrades\u201d. Adagrad is a version of SGD that\nscales\u03b1for each parameter according to the history of gradients. As a result, \u03b1is reduced\nfor very large gradients and vice-versa. Momentum is a method that helps accelerate SGD\nby orienting the gradient descent in the relevant direction and reducing oscillations. In neural\nnetwork training, variants of SGD such as RMSprop andAdam, are very frequently used.\nNotice that gradient descent and its variants are not machine learning algorithms. They are\nsolvers of minimization problems in which the function to minimize has a gradient (in most\npoints of its domain).\n4.3 How Machine Learning Engineers Work\nUnless you are a research scientist or work for a huge corporation with a large R&D budget,\nyou usually don\u2019t implement machine learning algorithms yourself. You don\u2019t implement\ngradient descent or some other solver either. You use libraries, most of which are open\nsource. A library is a collection of algorithms and supporting tools implemented with stability\nand e\ufb03ciency in mind. The most frequently used in practice open-source machine learning\nlibrary is scikit-learn . It\u2019s written in Python and C. Here\u2019s how you do linear regression in\nscikit-learn:\n1def train(x, y):\n2 from sklearn.linear_model import LinearRegression\n3 model = LinearRegression().fit(x,y)\n4 return model\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 8", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2292, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5c2684d3-8221-4508-8569-270867c838be": {"__data__": {"id_": "5c2684d3-8221-4508-8569-270867c838be", "embedding": null, "metadata": {"page_label": "42", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ff04144a-ff21-400c-a57a-dc4a8e83c8dc", "node_type": "4", "metadata": {"page_label": "42", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "936ce594b501a79b54d8f0eb3c83758a7b42f67688f4cd0859b3f8aa6470ee2f", "class_name": "RelatedNodeInfo"}}, "text": "5\n6model = train(x,y)\n7\n8x_new = 23.0\n9y_new = model.predict(x_new)\n10print(y_new)\nThe output will, again, be 13.97. Easy, right? You can replace LinearRegression with some\nother type of regression learning algorithm without modifying anything else. It just works.\nThe same can be said about classi\ufb01cation. You can easily replace LogisticRegression algorithm\nwithSVCalgorithm (this is scikit-learn\u2019s name for the Support Vector Machine algorithm),\nDecisionTreeClassi\ufb01er ,NearestNeighbors or many other classi\ufb01cation learning algorithms\nimplemented in scikit-learn.\n4.4 Learning Algorithms\u2019 Particularities\nHere, I outline some practical particularities that can di\ufb00erentiate one learning algorithm\nfrom another. You already know that di\ufb00erent learning algorithms can have di\ufb00erent\nhyperparameters ( Cin SVM,\u03f5anddin ID3). Solvers such as gradient descent can also have\nhyperparameters, like \u03b1for example.\nSome algorithms, like decision tree learning, can accept categorical features. For example, if\nyou have a feature \u201ccolor\u201d that can take values \u201cred\u201d, \u201cyellow\u201d, or \u201cgreen\u201d, you can keep\nthis feature as is. SVM, logistic and linear regression, as well as kNN (with cosine similarity\nor Euclidean distance metrics), expect numerical values for all features. All algorithms\nimplemented in scikit-learn expect numerical features. In the next chapter, I show how to\nconvert categorical features into numerical ones.\nSome algorithms, like SVM, allow the data analyst to provide weightings for each class.\nThese weightings in\ufb02uence how the decision boundary is drawn. If the weight of some class\nis high, the learning algorithm tries to not make errors in predicting training examples of\nthis class (typically, for the cost of making an error elsewhere). That could be important if\ninstances of some class are in the minority in your training data, but you would like to avoid\nmisclassifying examples of that class as much as possible.\nSome classi\ufb01cation models, like SVM and kNN, given a feature vector only output the class.\nOthers, like logistic regression or decision trees, can also return the score between 0and 1\nwhich can be interpreted as either how con\ufb01dent the model is about the prediction or as the\nprobability that the input example belongs to a certain class4.\nSomeclassi\ufb01cationalgorithms(likedecisiontreelearning, logisticregression, orSVM)buildthe\nmodel using the whole dataset at once. If you have got additional labeled examples, you have\n4If it\u2019s really necessary, the score for SVM and kNN predictions could be synthetically created using simple\ntechniques.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 9", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2637, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "70bde962-238f-4dc4-8114-9cc0201d4853": {"__data__": {"id_": "70bde962-238f-4dc4-8114-9cc0201d4853", "embedding": null, "metadata": {"page_label": "43", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bee74d58-9e81-4711-ade1-92cc2922b28d", "node_type": "4", "metadata": {"page_label": "43", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "dfd0a9495d1a67767988e83d0a540d53610b03f2afeb1b17bc21b263a8e3d0d1", "class_name": "RelatedNodeInfo"}}, "text": "to rebuild the model from scratch. Other algorithms (such as Na\u00efve Bayes, multilayer percep-\ntron, SGDClassi\ufb01er/SGDRegressor, PassiveAggressiveClassi\ufb01er/PassiveAggressiveRegressor\nin scikit-learn) can be trained iteratively, one batch at a time. Once new training examples\nare available, you can update the model using only the new data.\nFinally, some algorithms, like decision tree learning, SVM, and kNN can be used for both clas-\nsi\ufb01cation and regression, while others can only solve one type of problem: either classi\ufb01cation\nor regression, but not both.\nUsually, each library provides the documentation that explains what kind of problem each\nalgorithm solves, what input values are allowed and what kind of output the model produces.\nThe documentation also provides information on hyperparameters.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 10", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 866, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ec72f00c-b906-488c-ab23-0f243b45ea9d": {"__data__": {"id_": "ec72f00c-b906-488c-ab23-0f243b45ea9d", "embedding": null, "metadata": {"page_label": "44", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "166efef4-6439-467e-bbbf-bc4c9959597d", "node_type": "4", "metadata": {"page_label": "44", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "a6725ca2d2f9a903e99e8b202f79e6f80157c379f1361427dda0d246d853e580", "class_name": "RelatedNodeInfo"}}, "text": "5 Basic Practice\nUntil now, I only mentioned in passing some issues that a data analyst needs to consider when\nworking on a machine learning problem: feature engineering, over\ufb01tting, and hyperparameter\ntuning. In this chapter, we talk about these and other challenges that have to be addressed\nbefore you can type model = LogisticRegression().\ufb01t(x,y) in scikit-learn.\n5.1 Feature Engineering\nWhen a product manager tells you \u201cWe need to be able to predict whether a particular\ncustomer will stay with us. Here are the logs of customers\u2019 interactions with our product for\n\ufb01ve years.\u201d you cannot just grab the data, load it into a library and get a prediction. You\nneed to build a dataset \ufb01rst.\nRemember from the \ufb01rst chapter that the dataset is the collection of labeled examples\n{(xi,yi)}N\ni=1. Each element xiamongNis called a feature vector . A feature vector is a\nvector in which each dimension j= 1,...,Dcontains a value that describes the example\nsomehow. That value is called a feature and is denoted as x(j).\nThe problem of transforming raw data into a dataset is called feature engineering . For\nmost practical problems, feature engineering is a labor-intensive process that demands from\nthe data analyst a lot of creativity and, preferably, domain knowledge.\nFor example, to transform the logs of user interaction with a computer system, one could\ncreate features that contain information about the user and various statistics extracted from\nthe logs. For each user, one feature would contain the price of the subscription; other features\nwould contain the frequency of connections per day, week and year. Another feature would\ncontain the average session duration in seconds or the average response time for one request,\nand so on. Everything measurable can be used as a feature. The role of the data analyst is to\ncreate informative features: those would allow the learning algorithm to build a model that\npredicts well labels of the data used for training. Highly informative features are also called\nfeatures with high predictive power . For example, the average duration of a user\u2019s session\nhas high predictive power for the problem of predicting whether the user will keep using the\napplication in the future.\nWe say that a model has a low bias when it predicts the training data well. That is, the\nmodel makes few mistakes when we use it to predict labels of the examples used to build the\nmodel.\n5.1.1 One-Hot Encoding\nSome learning algorithms only work with numerical feature vectors. When some feature in\nyour dataset is categorical, like \u201ccolors\u201d or \u201cdays of the week,\u201d you can transform such a\ncategorical feature into several binary ones.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 3", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2723, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ff02cec2-25b4-4f21-b701-dae6fb6d791c": {"__data__": {"id_": "ff02cec2-25b4-4f21-b701-dae6fb6d791c", "embedding": null, "metadata": {"page_label": "45", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "16f5ca3f-72d2-42e7-a58b-5efa196c2798", "node_type": "4", "metadata": {"page_label": "45", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "8a696dab9e378edbabfba0c17a42de5641f668fd98f6225af1ddfe922738b6c0", "class_name": "RelatedNodeInfo"}}, "text": "If your example has a categorical feature \u201ccolors\u201d and this feature has three possible values:\n\u201cred,\u201d \u201cyellow,\u201d \u201cgreen,\u201d you can transform this feature into a vector of three numerical\nvalues:\nred= [1,0,0]\nyellow = [0,1,0]\ngreen = [0,0,1](1)\nBy doing so, you increase the dimensionality of your feature vectors. You should not transform\nred into 1, yellow into 2, and green into 3to avoid increasing the dimensionality because that\nwould imply that there\u2019s an order among the values in this category and this speci\ufb01c order is\nimportant for the decision making. If the order of a feature\u2019s values is not important, using\nordered numbers as values is likely to confuse the learning algorithm,1because the algorithm\nwill try to \ufb01nd a regularity where there\u2019s no one, which may potentially lead to over\ufb01tting.\n5.1.2 Binning\nAn opposite situation, occurring less frequently in practice, is when you have a numerical\nfeature but you want to convert it into a categorical one. Binning (also called bucketing )\nis the process of converting a continuous feature into multiple binary features called bins or\nbuckets, typically based on value range. For example, instead of representing age as a single\nreal-valued feature, the analyst could chop ranges of age into discrete bins: all ages between\n0and5years-old could be put into one bin, 6to10years-old could be in the second bin, 11\nto15years-old could be in the third bin, and so on.\nLet feature j= 4represent age. By applying binning, we replace this feature with the\ncorresponding bins. Let the three new bins, \u201cage_bin1\u201d, \u201cage_bin2\u201d and \u201cage_bin3\u201d be\nadded with indexes j= 123,j= 124andj= 125respectively (by default the values of these\nthree new features are 0). Now ifx(4)\ni= 7for some example xi, then we set feature x(124)\nito\n1; ifx(4)\ni= 13, then we set feature x(125)\nito1, and so on.\nIn some cases, a carefully designed binning can help the learning algorithm to learn using\nfewer examples. It happens because we give a \u201chint\u201d to the learning algorithm that if the\nvalue of a feature falls within a speci\ufb01c range, the exact value of the feature doesn\u2019t matter.\n1When the ordering of values of some categorical variable matters, we can replace those values by numbers\nby keeping only one variable. For example, if our variable represents the quality of an article, and the\nvalues are {poor, decent, good, excellent }, then we could replace those categories by numbers, for example,\n{1,2,3,4}.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 4", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2508, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "84547772-9245-4382-8c65-836bd343c163": {"__data__": {"id_": "84547772-9245-4382-8c65-836bd343c163", "embedding": null, "metadata": {"page_label": "46", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "952adb7b-a96b-47ae-838a-13381806395a", "node_type": "4", "metadata": {"page_label": "46", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "032e560a50ca1f7d58ca378f282f03dd1f0b99bf9c08ee2484fb8913c062f752", "class_name": "RelatedNodeInfo"}}, "text": "5.1.3 Normalization\nNormalization is the process of converting an actual range of values which a numerical\nfeature can take, into a standard range of values, typically in the interval [\u22121,1]or[0,1].\nFor example, suppose the natural range of a particular feature is 350to1450. By subtracting\n350from every value of the feature, and dividing the result by 1100, one can normalize those\nvalues into the range [0,1].\nMore generally, the normalization formula looks like this:\n\u00afx(j)=x(j)\u2212min(j)\nmax(j)\u2212min(j),\nwheremin(j)andmax(j)are, respectively, the minimum and the maximum value of the\nfeaturejin the dataset.\nWhy do we normalize? Normalizing the data is not a strict requirement. However, in practice,\nit can lead to an increased speed of learning. Remember the gradient descent example from\nthe previous chapter. Imagine you have a two-dimensional feature vector. When you update\nthe parameters of w(1)andw(2), you use partial derivatives of the mean squared error with\nrespect tow(1)andw(2). Ifx(1)is in the range [0,1000]andx(2)the range [0,0.0001], then\nthe derivative with respect to a larger feature will dominate the update.\nAdditionally, it\u2019s useful to ensure that our inputs are roughly in the same relatively small\nrange to avoid problems which computers have when working with very small or very big\nnumbers (known as numerical over\ufb02ow).\n5.1.4 Standardization\nStandardization (orz-score normalization ) is the procedure during which the feature\nvalues are rescaled so that they have the properties of a standard normal distribution with\n\u00b5= 0and\u03c3= 1, where\u00b5is the mean (the average value of the feature, averaged over all\nexamples in the dataset) and \u03c3is the standard deviation from the mean.\nStandard scores (or z-scores) of features are calculated as follows:\n\u02c6x(j)=x(j)\u2212\u00b5(j)\n\u03c3(j).\nYou may ask when you should use normalization and when standardization. There\u2019s no\nde\ufb01nitive answer to this question. Usually, if your dataset is not too big and you have time,\nyou can try both and see which one performs better for your task.\nIf you don\u2019t have time to run multiple experiments, as a rule of thumb:\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 5", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2171, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "12ba2ef1-b1c8-4169-ac3d-0f26ea3b916b": {"__data__": {"id_": "12ba2ef1-b1c8-4169-ac3d-0f26ea3b916b", "embedding": null, "metadata": {"page_label": "47", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "72f70266-cc7c-4783-bd8f-54e3da726b64", "node_type": "4", "metadata": {"page_label": "47", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "142b91c0f7e47ec90be5c07052f9825cb00b34bafb4bef59607f83089ffb7697", "class_name": "RelatedNodeInfo"}}, "text": "\u2022unsupervised learning algorithms, in practice, more often bene\ufb01t from standardization\nthan from normalization;\n\u2022standardization is also preferred for a feature if the values this feature takes are\ndistributed close to a normal distribution (so-called bell curve);\n\u2022again, standardization is preferred for a feature if it can sometimes have extremely high\nor low values (outliers); this is because normalization will \u201csqueeze\u201d the normal values\ninto a very small range;\n\u2022in all other cases, normalization is preferable.\nFeature rescaling is usually bene\ufb01cial to most learning algorithms. However, modern imple-\nmentations of the learning algorithms, which you can \ufb01nd in popular libraries, are robust to\nfeatures lying in di\ufb00erent ranges.\n5.1.5 Dealing with Missing Features\nIn some cases, the data comes to the analyst in the form of a dataset with features already\nde\ufb01ned. In some examples, values of some features can be missing. That often happens when\nthe dataset was handcrafted, and the person working on it forgot to \ufb01ll some values or didn\u2019t\nget them measured at all.\nThe typical approaches of dealing with missing values for a feature include:\n\u2022removing the examples with missing features from the dataset (that can be done if your\ndataset is big enough so you can sacri\ufb01ce some training examples);\n\u2022using a learning algorithm that can deal with missing feature values (depends on the\nlibrary and a speci\ufb01c implementation of the algorithm);\n\u2022using adata imputation technique.\n5.1.6 Data Imputation Techniques\nOne data imputation technique consists in replacing the missing value of a feature by an\naverage value of this feature in the dataset:\n\u02c6x(j)\u21901\nNx(j).\nAnother technique is to replace the missing value with a value outside the normal range of\nvalues. For example, if the normal range is [0,1], then you can set the missing value to 2or\n\u22121. The idea is that the learning algorithm will learn what is best to do when the feature has\na value signi\ufb01cantly di\ufb00erent from regular values. Alternatively, you can replace the missing\nvalue by a value in the middle of the range. For example, if the range for a feature is [\u22121,1],\nyou can set the missing value to be equal to 0. Here, the idea is that the value in the middle\nof the range will not signi\ufb01cantly a\ufb00ect the prediction.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 6", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2352, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "daea85b6-9b80-452a-b059-9882c8c05669": {"__data__": {"id_": "daea85b6-9b80-452a-b059-9882c8c05669", "embedding": null, "metadata": {"page_label": "48", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dad7c135-9a03-4c63-aaa8-e6c056775732", "node_type": "4", "metadata": {"page_label": "48", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "c4a92abf50b94fc8b1c7a9565e0c665456875a30d287209c3af84d604581a881", "class_name": "RelatedNodeInfo"}}, "text": "A more advanced technique is to use the missing value as the target variable for a regression\nproblem. You can use all remaining features [x(1)\ni,x(2)\ni,...,x(j\u22121)\ni,x(j+1)\ni,...,x(D)\ni]to form a\nfeature vector \u02c6xi, set \u02c6yi\u2190x(j), wherejis the feature with a missing value. Then you build\na regression model to predict \u02c6yfrom \u02c6x. Of course, to build training examples (\u02c6x,\u02c6y), you only\nuse those examples from the original dataset, in which the value of feature jis present.\nFinally, if you have a signi\ufb01cantly large dataset and just a few features with missing values,\nyou can increase the dimensionality of your feature vectors by adding a binary indicator\nfeature for each feature with missing values. Let\u2019s say feature j= 12in yourD-dimensional\ndataset has missing values. For each feature vector x, you then add the feature j=D+ 1\nwhich is equal to 1if the value of feature 12is present in xand0otherwise. The missing\nfeature value then can be replaced by 0or any number of your choice.\nAt prediction time, if your example is not complete, you should use the same data imputation\ntechnique to \ufb01ll the missing features as the technique you used to complete the training data.\nBefore you start working on the learning problem, you cannot tell which data imputation\ntechnique will work the best. Try several techniques, build several models and select the one\nthat works the best.\n5.2 Learning Algorithm Selection\nChoosing a machine learning algorithm can be a di\ufb03cult task. If you have much time, you\ncan try all of them. However, usually the time you have to solve a problem is limited. You\ncan ask yourself several questions before starting to work on the problem. Depending on\nyour answers, you can shortlist some algorithms and try them on your data.\n\u2022Explainability\nDoes your model have to be explainable to a non-technical audience? Most very accurate\nlearning algorithms are so-called \u201cblack boxes.\u201d They learn models that make very few errors,\nbut why a model made a speci\ufb01c prediction could be very hard to understand and even\nharder to explain. Examples of such models are neural networks or ensemble models.\nOn the other hand, kNN, linear regression, or decision tree learning algorithms produce\nmodels that are not always the most accurate, however, the way they make their prediction\nis very straightforward.\n\u2022In-memory vs. out-of-memory\nCan your dataset be fully loaded into the RAM of your server or personal computer? If\nyes, then you can choose from a wide variety of algorithms. Otherwise, you would prefer\nincremental learning algorithms that can improve the model by adding more data\ngradually.\n\u2022Number of features and examples\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2712, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "77ebf0ba-23af-4770-a416-870596cbcb94": {"__data__": {"id_": "77ebf0ba-23af-4770-a416-870596cbcb94", "embedding": null, "metadata": {"page_label": "49", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a05dd0f5-f3e3-4614-b721-c22c184e4171", "node_type": "4", "metadata": {"page_label": "49", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "4929b881d8e67c6ac7f596dac15dc02df1f8dac8f89cb007646350dd92dd8a0d", "class_name": "RelatedNodeInfo"}}, "text": "How many training examples do you have in your dataset? How many features does each\nexample have? Some algorithms, including neural networks andgradient boosting (we\nconsider both later), can handle a huge number of examples and millions of features. Others,\nlike SVM, can be very modest in their capacity.\n\u2022Categorical vs. numerical features\nIs your data composed of categorical only, or numerical only features, or a mix of both?\nDepending on your answer, some algorithms cannot handle your dataset directly, and you\nwould need to convert your categorical features into numerical ones.\n\u2022Nonlinearity of the data\nIs your data linearly separable or can it be modeled using a linear model? If yes, SVM with\nthe linear kernel, logistic or linear regression can be good choices. Otherwise, deep neural\nnetworks or ensemble algorithms, discussed in Chapters 6 and 7, might work better.\n\u2022Training speed\nHow much time is a learning algorithm allowed to use to build a model? Neural networks are\nknown to be slow to train. Simple algorithms like logistic and linear regression or decision\ntrees are much faster. Specialized libraries contain very e\ufb03cient implementations of some\nalgorithms; you may prefer to do research online to \ufb01nd such libraries. Some algorithms,\nsuch as random forests, bene\ufb01t from the availability of multiple CPU cores, so their model\nbuilding time can be signi\ufb01cantly reduced on a machine with dozens of cores.\n\u2022Prediction speed\nHow fast does the model have to be when generating predictions? Will your model be used\nin production where very high throughput is required? Algorithms like SVMs, linear and\nlogistic regression, and (some types of) neural networks, are extremely fast at the prediction\ntime. Others, like kNN, ensemble algorithms, and very deep or recurrent neural networks,\nare slower2.\nIf you don\u2019t want to guess the best algorithm for your data, a popular way to choose one\nis by testing it on the validation set . We talk about that below. Alternatively, if you use\nscikit-learn, you could try their algorithm selection diagram shown in Figure 1.\n5.3 Three Sets\nUntil now, I used the expressions \u201cdataset\u201d and \u201ctraining set\u201d interchangeably. However, in\npractice data analysts work with three distinct sets of labeled examples:\n1) training set,\n2) validation set, and\n2The prediction speed of kNN and ensemble methods implemented in the modern libraries are still pretty\nfast. Don\u2019t be afraid of using these algorithms in your practice.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 8", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2534, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dc09360e-c541-48ca-b29d-e975d0220eed": {"__data__": {"id_": "dc09360e-c541-48ca-b29d-e975d0220eed", "embedding": null, "metadata": {"page_label": "50", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3b66cd3f-b0a9-46b2-9169-efd9f5ab88b3", "node_type": "4", "metadata": {"page_label": "50", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "d9809f47829aedac6e8676ec3c18cd98674f1a356ce79eaff7aff9ccbc39263f", "class_name": "RelatedNodeInfo"}}, "text": "3) test set.\nOnce you have got your annotated dataset, the \ufb01rst thing you do is you shu\ufb04e the examples\nand split the dataset into three subsets: training, validation, and test. The training set is\nusually the biggest one; you use it to build the model. The validation and test sets are\nroughly the same sizes, much smaller than the size of the training set. The learning algorithm\ncannotuse examples from these two subsets to build the model. That is why those two sets\nare often called holdout sets .\nThere\u2019s no optimal proportion to split the dataset into these three subsets. In the past, the\nrule of thumb was to use 70% of the dataset for training, 15% for validation and 15% for\ntesting. However, in the age of big data, datasets often have millions of examples. In such\ncases, it could be reasonable to keep 95% for training and 2.5%/2.5% for validation/testing.\nYou may wonder, what is the reason to have three sets and not one. The answer is simple:\nwhen we build a model, what we do not want is for the model to only do well at predicting\nlabels of examples the learning algorithms has already seen. A trivial algorithm that simply\nmemorizes all training examples and then uses the memory to \u201cpredict\u201d their labels will make\nno mistakes when asked to predict the labels of the training examples, but such an algorithm\nwould be useless in practice. What we really want is a model that is good at predicting\nexamples that the learning algorithm didn\u2019t see: we want good performance on a holdout set.\nWhy do we need two holdout sets and not one? We use the validation set to 1) choose the\nlearning algorithm and 2) \ufb01nd the best values of hyperparameters. We use the test set to\nassess the model before delivering it to the client or putting it in production.\n5.4 Under\ufb01tting and Over\ufb01tting\nI mentioned above the notion of bias. I said that a model has a low bias if it predicts well\nthe labels of the training data. If the model makes many mistakes on the training data, we\nsay that the model has a high bias or that the model under\ufb01ts . So, under\ufb01tting is the\ninability of the model to predict well the labels of the data it was trained on. There could be\nseveral reasons for under\ufb01tting, the most important of which are:\n\u2022your model is too simple for the data (for example a linear model can often under\ufb01t);\n\u2022the features you engineered are not informative enough.\nThe \ufb01rst reason is easy to illustrate in the case of one-dimensional regression: the dataset can\nresemble a curved line, but our model is a straight line. The second reason can be illustrated\nlike this: let\u2019s say you want to predict whether a patient has cancer, and the features you\nhave are height, blood pressure, and heart rate. These three features are clearly not good\npredictors for cancer so our model will not be able to learn a meaningful relationship between\nthese features and the label.\nThe solution to the problem of under\ufb01tting is to try a more complex model or to engineer\nfeatures with higher predictive power.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 9", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3064, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6230913a-b347-4e8a-99d3-14b30e9476f0": {"__data__": {"id_": "6230913a-b347-4e8a-99d3-14b30e9476f0", "embedding": null, "metadata": {"page_label": "51", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8337b4ab-37fd-4c5c-94dd-e73ae77bf82b", "node_type": "4", "metadata": {"page_label": "51", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "6c61d331178525a4ab2295933e1866db97aaa6642f492aab1100b7b11e91c07e", "class_name": "RelatedNodeInfo"}}, "text": "Figure 1: Machine learning algorithm selection diagram for scikit-learn.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 10", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 136, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d92335ae-48ba-4e2e-999e-f956a6c15a95": {"__data__": {"id_": "d92335ae-48ba-4e2e-999e-f956a6c15a95", "embedding": null, "metadata": {"page_label": "52", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b7cb74b-91ca-4cba-9b0a-4ea45ec07018", "node_type": "4", "metadata": {"page_label": "52", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "61b771387aa7f5734ccee679ee82883002e6e0e838499402ddf7355e7a5bae89", "class_name": "RelatedNodeInfo"}}, "text": "4\n 2\n 0 210\n0102030\ndegree 1 (underfit)\ntraining examplesUnder\ufb01tting\n4\n 2\n 0 210\n0102030\ndegree 2 (fit)\ntraining examples Good \ufb01t\n4\n 2\n 0 210\n0102030\ndegree 15 (overfit)\ntraining examples Over\ufb01tting\nFigure 2: Examples of under\ufb01tting (linear model), good \ufb01t (quadratic model), and over\ufb01tting\n(polynomial of degree 15).\nOver\ufb01tting is another problem a model can exhibit. The model that over\ufb01ts predicts very\nwell the training data but poorly the data from at least one of the two holdout sets. I already\ngave an illustration of over\ufb01tting in Chapter 3. Several reasons can lead to over\ufb01tting, the\nmost important of which are:\n\u2022your model is too complex for the data (for example a very tall decision tree or a very\ndeep or wide neural network often over\ufb01t);\n\u2022you have too many features but a small number of training examples.\nIn the literature, you can \ufb01nd another name for the problem of over\ufb01tting: the problem of\nhigh variance . This term comes from statistics. The variance is an error of the model due\nto its sensitivity to small \ufb02uctuations in the training set. It means that if your training data\nwas sampled di\ufb00erently, the learning would result in a signi\ufb01cantly di\ufb00erent model. Which\nis why the model that over\ufb01ts performs poorly on the test data: test and training data are\nsampled from the dataset independently of one another.\nEven the simplest model, such as linear, can over\ufb01t the data. That usually happens when the\ndata is high-dimensional, but the number of training examples is relatively low. In fact, when\nfeature vectors are very high-dimensional, the linear learning algorithm can build a model\nthat assigns non-zero values to most parameters w(j)in the parameter vector w, trying to\n\ufb01nd very complex relationships between all available features to predict labels of training\nexamples perfectly.\nSuch a complex model will most likely predict poorly the labels of the holdout examples.\nThis is because by trying to perfectly predict labels of all training examples, the model will\nalso learn the idiosyncrasies of the training set: the noise in the values of features of the\ntraining examples, the sampling imperfection due to the small dataset size, and other artifacts\nextrinsic to the decision problem at hand but present in the training set.\nFigure 2 illustrates a one-dimensional dataset for which a regression model under\ufb01ts, \ufb01ts well\nand over\ufb01ts the data.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 11", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2447, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8819ea56-9a5e-4ae5-a4dd-d5a772861dd4": {"__data__": {"id_": "8819ea56-9a5e-4ae5-a4dd-d5a772861dd4", "embedding": null, "metadata": {"page_label": "53", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "17700155-9b95-4044-8bdd-56f6a25b41c5", "node_type": "4", "metadata": {"page_label": "53", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "aa34b8a7ee63e6995b025cf39a340a1634f76c8f8d4f749c80960bca59bb4b7a", "class_name": "RelatedNodeInfo"}}, "text": "Several solutions to the problem of over\ufb01tting are possible:\n1.Try a simpler model (linear instead of polynomial regression, or SVM with a linear\nkernel instead of RBF, a neural network with fewer layers/units).\n2.Reduce the dimensionality of examples in the dataset (for example, by using one of the\ndimensionality reduction techniques discussed in Chapter 9).\n3. Add more training data, if possible.\n4. Regularize the model.\nRegularization is the most widely used approach to prevent over\ufb01tting.\n5.5 Regularization\nRegularization is an umbrella-term that encompasses methods that force the learning al-\ngorithm to build a less complex model. In practice, that often leads to slightly higher\nbias but signi\ufb01cantly reduces the variance. This problem is known in the literature as the\nbias-variance tradeo\ufb00 .\nThe two most widely used types of regularization are called L1andL2 regularization . The\nidea is quite simple. To create a regularized model, we modify the objective function by\nadding a penalizing term whose value is higher when the model is more complex.\nFor simplicity, I illustrate regularization using the example of linear regression. The same\nprinciple can be applied to a wide variety of models.\nRecall the linear regression objective:\nmin\nw,b1\nNN\u2211\ni=1(fw,b(xi)\u2212yi)2. (2)\nAn L1-regularized objective looks like this:\nmin\nw,b[\nC|w|+1\nNN\u2211\ni=1(fw,b(xi)\u2212yi)2]\n, (3)\nwhere|w|def=\u2211D\nj=1|w(j)|andCis a hyperparameter that controls the importance of regular-\nization. If we set Cto zero, the model becomes a standard non-regularized linear regression\nmodel. On the other hand, if we set to Cto a high value, the learning algorithm will try to\nset mostw(j)to a very small value or zero to minimize the objective, the model will become\nvery simple which can lead to under\ufb01tting. Your role as the data analyst is to \ufb01nd such\na value of the hyperparameter Cthat doesn\u2019t increase the bias too much but reduces the\nvariance to a level reasonable for the problem at hand. In the next section, I will show how\nto do that.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 12", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2085, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a3d8a86-f212-4b9b-afa1-4eae5e2f8459": {"__data__": {"id_": "3a3d8a86-f212-4b9b-afa1-4eae5e2f8459", "embedding": null, "metadata": {"page_label": "54", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "04f346ca-c21f-4cbc-b462-70b60b75c872", "node_type": "4", "metadata": {"page_label": "54", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "79179e9cf6b937607f938098b7591e48b608b51f236704a9417ad762724286e2", "class_name": "RelatedNodeInfo"}}, "text": "An L2-regularized objective looks like this:\nmin\nw,b[\nC\u2225w\u22252+1\nNN\u2211\ni=1(fw,b(xi)\u2212yi)2]\n,where\u2225w\u22252def=D\u2211\nj=1(w(j))2. (4)\nIn practice, L1 regularization produces a sparse model , a model that has most of its\nparameters (in case of linear models, most of w(j)) equal to zero, provided the hyperparameter\nCis large enough. So L1 makes feature selection by deciding which features are essential\nfor prediction and which are not. That can be useful in case you want to increase model\nexplainability. However, if your only goal is to maximize the performance of the model on\nthe holdout data, then L2 usually gives better results. L2 also has the advantage of being\ndi\ufb00erentiable, so gradient descent can be used for optimizing the objective function.\nL1 and L2 regularization methods were also combined in what is called elastic net regu-\nlarization with L1 and L2 regularizations being special cases. You can \ufb01nd in the literature\nthe name ridge regularization for L2 and lassofor L1.\nIn addition to being widely used with linear models, L1 and L2 regularization are also\nfrequently used with neural networks and many other types of models, which directly\nminimize an objective function.\nNeural networks also bene\ufb01t from two other regularization techniques: dropout andbatch-\nnormalization . There are also non-mathematical methods that have a regularization e\ufb00ect:\ndata augmentation andearly stopping . We talk about these techniques in Chapter 8.\n5.6 Model Performance Assessment\nOnce you have a model which our learning algorithm has built using the training set, how\ncan you say how good the model is? You use the test set to assess the model.\nThe test set contains the examples that the learning algorithm has never seen before, so if\nour model performs well on predicting the labels of the examples from the test set, we say\nthat our model generalizes well or, simply, that it\u2019s good.\nTo be more rigorous, machine learning specialists use various formal metrics and tools to\nassess the model performance. For regression, the assessment of the model is quite simple. A\nwell-\ufb01tting regression model results in predicted values close to the observed data values. The\nmean model , which always predicts the average of the labels in the training data, generally\nwould be used if there were no informative features. The \ufb01t of a regression model being\nassessed should, therefore, be better than the \ufb01t of the mean model. If this is the case, then\nthe next step is to compare the performances of the model on the training and the test data.\nTo do that, we compute the mean squared error3(MSE) for the training, and, separately,\nfor the test data. If the MSE of the model on the test data is substantially higher than\n3Or any other type of average loss function that makes sense.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 13", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2833, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c100e789-01e9-4636-8efd-ad7064a5b64c": {"__data__": {"id_": "c100e789-01e9-4636-8efd-ad7064a5b64c", "embedding": null, "metadata": {"page_label": "55", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f8564556-755a-4f62-a64a-b923a7b64262", "node_type": "4", "metadata": {"page_label": "55", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "7d40f3716e08f2a0218068058c0bb827d02b0994361b3cc5485cd547361e1e49", "class_name": "RelatedNodeInfo"}}, "text": "the MSE obtained on the training data, this is a sign of over\ufb01tting. Regularization or a\nbetter hyperparameter tuning could solve the problem. The meaning of \u201csubstantially higher\u201d\ndepends on the problem at hand and has to be decided by the data analyst jointly with the\ndecision maker/product owner who ordered the model.\nFor classi\ufb01cation, things are a little bit more complicated. The most widely used metrics and\ntools to assess the classi\ufb01cation model are:\n\u2022confusion matrix,\n\u2022accuracy,\n\u2022cost-sensitive accuracy,\n\u2022precision/recall, and\n\u2022area under the ROC curve.\nTo simplify the illustration, I use a binary classi\ufb01cation problem. Where necessary, I show\nhow to extend the approach to the multiclass case.\n5.6.1 Confusion Matrix\nTheconfusion matrix is a table that summarizes how successful the classi\ufb01cation model\nis at predicting examples belonging to various classes. One axis of the confusion matrix\nis the label that the model predicted, and the other axis is the actual label. In a binary\nclassi\ufb01cation problem, there are two classes. Let\u2019s say, the model predicts two classes: \u201cspam\u201d\nand \u201cnot_spam\u201d:\nspam (predicted) not_spam (predicted)\nspam (actual) 23 (TP) 1 (FN)\nnot_spam (actual) 12 (FP) 556 (TN)\nThe above confusion matrix shows that of the 24 examples that actually were spam, the\nmodel correctly classi\ufb01ed 23as spam. In this case, we say that we have 23true positives\nor TP = 23. The model incorrectly classi\ufb01ed 1example as not_spam. In this case, we have 1\nfalse negative , or FN = 1. Similarly, of 568examples that actually were not spam, 556were\ncorrectly classi\ufb01ed ( 556true negatives or TN = 556), and 12were incorrectly classi\ufb01ed ( 12\nfalse positives , FP = 12).\nThe confusion matrix for multiclass classi\ufb01cation has as many rows and columns as there are\ndi\ufb00erent classes. It can help you to determine mistake patterns. For example, a confusion\nmatrix could reveal that a model trained to recognize di\ufb00erent species of animals tends to\nmistakenly predict \u201ccat\u201d instead of \u201cpanther,\u201d or \u201cmouse\u201d instead of \u201crat.\u201d In this case, you\ncan decide to add more labeled examples of these species to help the learning algorithm\nto \u201csee\u201d the di\ufb00erence between them. Alternatively, you might add additional features the\nlearning algorithm can use to build a model that would better distinguish between these\nspecies.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 14", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2394, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6504eceb-ebe5-4edd-98f4-5cd88b2e0691": {"__data__": {"id_": "6504eceb-ebe5-4edd-98f4-5cd88b2e0691", "embedding": null, "metadata": {"page_label": "56", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "62936da7-cbd5-44bd-8952-1899d9bd1736", "node_type": "4", "metadata": {"page_label": "56", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "8c642f970560ca8bf3df45b9c6cab6b98ef1709aebc6b583eb15595a4f968ed6", "class_name": "RelatedNodeInfo"}}, "text": "Confusion matrix is used to calculate two other performance metrics: precision andrecall.\n5.6.2 Precision/Recall\nThe two most frequently used metrics to assess the model are precision andrecall. Precision\nis the ratio of correct positive predictions to the overall number of positive predictions:\nprecisiondef=TP\nTP+FP.\nRecall is the ratio of correct positive predictions to the overall number of positive examples\nin the dataset:\nrecalldef=TP\nTP+FN.\nTo understand the meaning and importance of precision and recall for the model assessment it\nis often useful to think about the prediction problem as the problem of research of documents\nin the database using a query. The precision is the proportion of relevant documents in the\nlist of all returned documents. The recall is the ratio of the relevant documents returned\nby the search engine to the total number of the relevant documents that could have been\nreturned.\nIn the case of the spam detection problem, we want to have high precision (we want to avoid\nmaking mistakes by detecting that a legitimate message is spam) and we are ready to tolerate\nlower recall (we tolerate some spam messages in our inbox).\nAlmost always, in practice, we have to choose between a high precision or a high recall. It\u2019s\nusually impossible to have both. We can achieve either of the two by various means:\n\u2022by assigning a higher weighting to the examples of a speci\ufb01c class (the SVM algorithm\naccepts weightings of classes as input);\n\u2022by tuning hyperparameters to maximize precision or recall on the validation set;\n\u2022by varying the decision threshold for algorithms that return probabilities of classes;\nfor instance, if we use logistic regression or decision tree, to increase precision (at the\ncost of a lower recall), we can decide that the prediction will be positive only if the\nprobability returned by the model is higher than 0.9.\nEven if precision and recall are de\ufb01ned for the binary classi\ufb01cation case, you can always use\nit to assess a multiclass classi\ufb01cation model. To do that, \ufb01rst select a class for which you\nwant to assess these metrics. Then you consider all examples of the selected class as positives\nand all examples of the remaining classes as negatives.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2276, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d3061b7-a8a4-4983-a95a-13ed81e98d18": {"__data__": {"id_": "0d3061b7-a8a4-4983-a95a-13ed81e98d18", "embedding": null, "metadata": {"page_label": "57", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eac05390-70d0-49cc-be8e-3c0c617a38ca", "node_type": "4", "metadata": {"page_label": "57", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "acb4e35ec8dc73988ba79d5d3eaac6e1c7cc8ac3e8bd3da705e851f5751ccbe9", "class_name": "RelatedNodeInfo"}}, "text": "5.6.3 Accuracy\nAccuracy is given by the number of correctly classi\ufb01ed examples divided by the total number\nof classi\ufb01ed examples. In terms of the confusion matrix, it is given by:\naccuracydef=TP+TN\nTP+TN+FP+FN. (5)\nAccuracy is a useful metric when errors in predicting all classes are equally important. In\ncase of the spam/not spam, this may not be the case. For example, you would tolerate false\npositives less than false negatives. A false positive in spam detection is the situation in which\nyour friend sends you an email, but the model labels it as spam and doesn\u2019t show you. On\nthe other hand, the false negative is less of a problem: if your model doesn\u2019t detect a small\npercentage of spam messages, it\u2019s not a big deal.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 16", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 792, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "54371945-0542-4347-9775-9123bf3fa463": {"__data__": {"id_": "54371945-0542-4347-9775-9123bf3fa463", "embedding": null, "metadata": {"page_label": "58", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "11c5b1e0-d02a-48eb-ab94-04a17e350133", "node_type": "4", "metadata": {"page_label": "58", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "ebac0a5cd8519a5fb4aa931b342af4d94e67d07efd459647c6f960007a5b2292", "class_name": "RelatedNodeInfo"}}, "text": "Figure 3: The area under the ROC curve (shown on gray).\n5.6.4 Cost-Sensitive Accuracy\nFor dealing with the situation in which di\ufb00erent classes have di\ufb00erent importance, a useful\nmetric iscost-sensitive accuracy . To compute a cost-sensitive accuracy, you \ufb01rst assign a\ncost (a positive number) to both types of mistakes: FP and FN. You then compute the counts\nTP, TN, FP, FN as usual and multiply the counts for FP and FN by the corresponding cost\nbefore calculating the accuracy using eq. 5.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 17", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 556, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b9dd4991-8ee5-4f26-b5c0-6585063741fb": {"__data__": {"id_": "b9dd4991-8ee5-4f26-b5c0-6585063741fb", "embedding": null, "metadata": {"page_label": "59", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "82b97858-dab1-4527-bc5f-56061962431e", "node_type": "4", "metadata": {"page_label": "59", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "4cac14dd19bd3a0186cdef32defb608b437feede10eeef557abbef401590219d", "class_name": "RelatedNodeInfo"}}, "text": "5.6.5 Area under the ROC Curve (AUC)\nThe ROC curve (stands for \u201creceiver operating characteristic,\u201d the term comes from radar\nengineering) is a commonly used method to assess the performance of classi\ufb01cation models.\nROC curves use a combination of the true positive rate (de\ufb01ned exactly as recall) and\nfalse positive rate (the proportion of negative examples predicted incorrectly) to build up a\nsummary picture of the classi\ufb01cation performance.\nThe true positive rate (TPR) and the false positive rate (FPR) are respectively de\ufb01ned as,\nTPRdef=TP\nTP+FNand FPRdef=FP\nFP+TN.\nROC curves can only be used to assess classi\ufb01ers that return some con\ufb01dence score (or a\nprobability) of prediction. For example, logistic regression, neural networks, and decision\ntrees (and ensemble models based on decision trees) can be assessed using ROC curves.\nTo draw a ROC curve, you \ufb01rst discretize the range of the con\ufb01dence score. If this range for\na model is [0,1], then you can discretize it like this: [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1].\nThen, you use each discrete value as the prediction threshold and predict the labels of\nexamples in your dataset using the model and this threshold. For example, if you want to\ncompute TPR and FPR for the threshold equal to 0.7, you apply the model to each example,\nget the score, and, if the score is higher than or equal to 0.7, you predict the positive class;\notherwise, you predict the negative class.\nLook at the illustration in Figure 3. It\u2019s easy to see that if the threshold is 0, all our\npredictions will be positive, so both TPR and FPR will be 1(the upper right corner). On\nthe other hand, if the threshold is 1, then no positive prediction will be made, both TPR\nand FPR will be 0which corresponds to the lower left corner.\nThe higher the area under the ROC curve (AUC), the better the classi\ufb01er. A classi\ufb01er\nwith an AUC higher than 0.5is better than a random classi\ufb01er. If AUC is lower than 0.5,\nthen something is wrong with your model. A perfect classi\ufb01er would have an AUC of 1.\nUsually, if your model behaves well, you obtain a good classi\ufb01er by selecting the value of the\nthreshold that gives TPR close to 1while keeping FPR near 0.\nROC curves are popular because they are relatively simple to understand, they capture more\nthan one aspect of the classi\ufb01cation (by taking both false positives and negatives into account)\nand allow visually and with low e\ufb00ort comparing the performance of di\ufb00erent models.\n5.7 Hyperparameter Tuning\nWhen I presented learning algorithms, I mentioned that you as a data analyst have to select\ngood values for the algorithm\u2019s hyperparameters, such as \u03f5anddfor ID3,Cfor SVM, or \u03b1\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 18", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2719, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "94dc798e-abf9-4c56-ac23-42d5448aa4e2": {"__data__": {"id_": "94dc798e-abf9-4c56-ac23-42d5448aa4e2", "embedding": null, "metadata": {"page_label": "60", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "beebe7be-037d-4c1f-abfa-6a5b6f3cf974", "node_type": "4", "metadata": {"page_label": "60", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "2644f5d679643dd76ca9ec45129cd54e846c20ba418060b0c17ae42873dd8251", "class_name": "RelatedNodeInfo"}}, "text": "for gradient descent. But what does that exactly mean? Which value is the best and how to\n\ufb01nd it? In this section, I answer these essential questions.\nAs you already know, hyperparameters aren\u2019t optimized by the learning algorithm itself. The\ndata analyst has to \u201ctune\u201d hyperparameters by experimentally \ufb01nding the best combination\nof values, one per hyperparameter.\nOne typical way to do that, when you have enough data to have a decent validation set (in\nwhich each class is represented by at least a couple of dozen examples) and the number of\nhyperparameters and their range is not too large is to use grid search .\nGrid search is the most simple hyperparameter tuning technique. Let\u2019s say you train an\nSVM and you have two hyperparameters to tune: the penalty parameter C(a positive real\nnumber) and the kernel (either \u201clinear\u201d or \u201crbf\u201d).\nIf it\u2019s the \ufb01rst time you are working with this particular dataset, you don\u2019t know what is the\npossible range of values for C. The most common trick is to use a logarithmic scale. For\nexample, for Cyou can try the following values: [0.001, 0.01, 0.1, 1, 10, 100, 1000]. In this\ncase you have 14 combinations of hyperparameters to try: [(0.001, \u201clinear\u201d), (0.01, \u201clinear\u201d),\n(0.1, \u201clinear\u201d), (1, \u201clinear\u201d), (10, \u201clinear\u201d), (100, \u201clinear\u201d), (1000, \u201clinear\u201d), (0.001, \u201crbf\u201d),\n(0.01, \u201crbf\u201d), (0.1, \u201crbf\u201d), (1, \u201crbf\u201d), (10, \u201crbf\u201d), (100, \u201crbf\u201d), (1000, \u201crbf\u201d)].\nYou use the training set and train 14 models, one for each combination of hyperparameters.\nThen you assess the performance of each model on the validation data using one of the\nmetrics we discussed in the previous section (or some other metric that matters to you).\nFinally, you keep the model that performs the best according to the metric.\nOnce the best pair of hyperparameters is found, you can try to explore the values close to the\nbest ones in some region around them. Sometimes, this can result in an even better model.\nFinally, you assess the selected model using the test set.\nAs you could notice, trying all combinations of hyperparameters, especially if there are\nmore than a couple of them, could be time-consuming, especially for large datasets. There\nare more e\ufb03cient techniques, such as random search andBayesian hyperparameter\noptimization .\nRandom search di\ufb00ers from grid search in that you no longer\nprovide a discrete set of values to explore for each hyperparameter;\ninstead, you provide a statistical distribution for each hyperparam-\neter from which values are randomly sampled and set the total\nnumber of combinations you want to try.\nBayesian techniques di\ufb00er from random or grid search in that they\nuse past evaluation results to choose the next values to evaluate.\nThe idea is to limit the number of expensive optimizations of the\nobjective function by choosing the next hyperparameter values\nbased on those that have done well in the past.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2930, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "51dd81db-2ce1-4c68-93fc-df03f52c2d15": {"__data__": {"id_": "51dd81db-2ce1-4c68-93fc-df03f52c2d15", "embedding": null, "metadata": {"page_label": "61", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f685bc46-fec8-417a-b9f1-5363da5466f9", "node_type": "4", "metadata": {"page_label": "61", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "1851fd09b66277ab9e32b9c571841832f9be5a434e85adab63592b8c139ab168", "class_name": "RelatedNodeInfo"}}, "text": "There are also gradient-based techniques ,evolutionary optimization techniques ,\nand other algorithmic hyperparameter tuning techniques. Most modern machine learning\nlibraries implement one or more such techniques. There are also hyperparameter tuning\nlibraries that can help you to tune hyperparameters of virtually any learning algorithm,\nincluding ones you programmed yourself.\n5.7.1 Cross-Validation\nWhen you don\u2019t have a decent validation set to tune your hyperparameters on, the common\ntechniquethatcanhelpyouiscalled cross-validation . Whenyouhavefewtrainingexamples,\nit could be prohibitive to have both validation and test set. You would prefer to use more\ndata to train the model. In such a case, you only split your data into a training and a test\nset. Then you use cross-validation on the training set to simulate a validation set.\nCross-validation works like follows. First, you \ufb01x the values of the hyperparameters you want\nto evaluate. Then you split your training set into several subsets of the same size. Each\nsubset is called a fold. Typically, \ufb01ve-fold cross-validation is used in practice. With \ufb01ve-fold\ncross-validation, you randomly split your training data into \ufb01ve folds: {F1,F2,...,F 5}. Each\nFk,k= 1,..., 5contains 20% of your training data. Then you train \ufb01ve models as follows.\nTo train the \ufb01rst model, f1, you use all examples from folds F2,F3,F4, andF5as the training\nset and the examples from F1as the validation set. To train the second model, f2, you\nuse the examples from folds F1,F3,F4, andF5to train and the examples from F2as the\nvalidation set. You continue building models iteratively like this and compute the value of\nthe metric of interest on each validation set, from F1toF5. Then you average the \ufb01ve values\nof the metric to get the \ufb01nal value.\nYou can use grid search with cross-validation to \ufb01nd the best values of hyperparameters for\nyour model. Once you have found these values, you use the entire training set to build the\nmodel with these best values of hyperparameters you have found via cross-validation. Finally,\nyou assess the model using the test set.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 20", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2170, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f8e27359-9c32-4dea-88fc-ba574ed60b37": {"__data__": {"id_": "f8e27359-9c32-4dea-88fc-ba574ed60b37", "embedding": null, "metadata": {"page_label": "62", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ab2e30c6-ceb5-48e0-bb32-0bb147deec11", "node_type": "4", "metadata": {"page_label": "62", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "6bcf8522af70572ea41cc421923d27be415b6a5ecbac832dc76dd79997d369be", "class_name": "RelatedNodeInfo"}}, "text": "6 Neural Networks and Deep Learning\nFirst of all, you already know what a neural network is, and you already know how to build\nsuch a model. Yes, it\u2019s logistic regression! As a matter of fact, the logistic regression model,\nor rather its generalization for multiclass classi\ufb01cation, called the softmax regression model,\nis a standard unit in a neural network.\n6.1 Neural Networks\nIf you understood linear regression, logistic regression, and gradient descent, understanding\nneural networks should not be a problem.\nA neural network (NN), just like a regression or an SVM model, is a mathematical function:\ny=fNN(x).\nThe function fNNhas a particular form: it\u2019s a nested function . You have probably already\nheard of neural network layers. So, for a 3-layer neural network that returns a scalar, fNN\nlooks like this:\ny=fNN(x) =f3(f2(f1(x))).\nIn the above equation, f1andf2are vector functions of the following form:\nfl(z)def=gl(Wlz+bl), (1)\nwherelis called the layer index and can span from 1to any number of layers. The function\nglis called an activation function . It is a \ufb01xed, usually nonlinear function chosen by the\ndata analyst before the learning is started. The parameters Wl(a matrix) and bl(a vector)\nfor each layer are learned using the familiar gradient descent by optimizing, depending on the\ntask, a particular cost function (such as MSE). Compare eq. 1 with the equation for logistic\nregression, where you replace glby the sigmoid function, and you will not see any di\ufb00erence.\nThe function f3is a scalar function for the regression task, but can also be a vector function\ndepending on your problem.\nYou may probably wonder why a matrix Wlis used and not a vector wl. The reason is\nthatglis a vector function. Each row wl,u(ufor unit) of the matrix Wlis a vector of\nthe same dimensionality as z. Letal,u=wl,uz+bl,u. The output of fl(z)is a vector\n[gl(al,1),gl(al,2),...,g l(al,size l)], whereglis some scalar function1, andsizelis the number of\nunits in layer l. To make it more concrete, let\u2019s consider one architecture of neural networks\ncalled multilayer perceptron and often referred to as a vanilla neural network .\n1A scalar function outputs a scalar, that is a simple number and not a vector.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 3", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2275, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a7023ac-ef10-4246-9abf-ae044c30701c": {"__data__": {"id_": "5a7023ac-ef10-4246-9abf-ae044c30701c", "embedding": null, "metadata": {"page_label": "63", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "93c3cf00-5dab-4807-b566-2173f8e32b19", "node_type": "4", "metadata": {"page_label": "63", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "c8ad9e3354e5a3484e7fd2736ed8cad0e0b4abf7742ec7d975a6e4ec605c26d1", "class_name": "RelatedNodeInfo"}}, "text": "6.1.1 Multilayer Perceptron Example\nWe have a closer look at one particular con\ufb01guration of neural networks called feed-forward\nneural networks (FFNN), and more speci\ufb01cally the architecture called a multilayer\nperceptron (MLP). As an illustration, let\u2019s consider an MLP with three layers. Our network\ntakes a two-dimensional feature vector as input and outputs a number. This FFNN can be a\nregression or a classi\ufb01cation model, depending on the activation function used in the third,\noutput layer.\nOur MLP is depicted in Figure 1. The neural network is represented graphically as a\nconnected combination of unitslogically organized into one or more layers. Each unit is\nrepresented by either a circle or a rectangle. The inbound arrow represents an input of a unit\nand indicates where this input came from. The outbound arrow indicates the output of a\nunit.\nThe output of each unit is the result of the mathematical operation written inside the\nrectangle. Circle units don\u2019t do anything with the input; they just send their input directly\nto the output.\nThe following happens in each rectangle unit. Firstly, all inputs of the unit are joined together\nto form an input vector. Then the unit applies a linear transformation to the input vector,\nexactly like linear regression model does with its input feature vector. Finally, the unit\napplies an activation function gto the result of the linear transformation and obtains the\noutput value, a real number. In a vanilla FFNN, the output value of a unit of some layer\nbecomes an input value of each of the units of the subsequent layer.\nIn Figure 1, the activation function glhas one index: l, the index of the layer the unit belongs\nto. Usually, all units of a layer use the same activation function, but it\u2019s not a rule. Each\nlayer can have a di\ufb00erent number of units. Each unit has its parameters wl,uandbl,u, where\nuis the index of the unit, and lis the index of the layer. The vector yl\u22121in each unit is\nde\ufb01ned as [y(1)\nl\u22121,y(2)\nl\u22121,y(3)\nl\u22121,y(4)\nl\u22121]. The vector xin the \ufb01rst layer is de\ufb01ned as [x(1),...,x(D)].\nAs you can see in Figure 1, in multilayer perceptron all outputs of one layer are connected to\neach input of the succeeding layer. This architecture is called fully-connected . A neural\nnetwork can contain fully-connected layers . Those are the layers whose units receive as\ninputs the outputs of each of the units of the previous layer.\n6.1.2 Feed-Forward Neural Network Architecture\nIf we want to solve a regression or a classi\ufb01cation problem discussed in previous chapters, the\nlast (the rightmost) layer of a neural network usually contains only one unit. If the activation\nfunctionglastof the last unit is linear, then the neural network is a regression model. If the\nglastis a logistic function, the neural network is a binary classi\ufb01cation model.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 4", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2880, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4344d8c1-7033-4e68-bfc9-11ebc7912263": {"__data__": {"id_": "4344d8c1-7033-4e68-bfc9-11ebc7912263", "embedding": null, "metadata": {"page_label": "64", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c5948655-dac7-4e52-91a7-9184c2c27339", "node_type": "4", "metadata": {"page_label": "64", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "dba8f6fe650735ae23f52945a029c204d1583c9d94a7163cdcb6e6504067c6cf", "class_name": "RelatedNodeInfo"}}, "text": "x(1)\nx(2)y 1(1) \u00a0 \u2190\u00a0 g 1 ( w 1,1 x\u00a0+\u00a0b 1,1 )\nx(1)\nx(2)y 1(2)\u00a0 \u2190\u00a0g 1 ( w 1,2 x\u00a0+\u00a0b 1,2 )\ny 1(3) \u00a0\u2190\u00a0g 1 ( w 1,3 x\u00a0+\u00a0b 1,3 )\ny 1(4)\u00a0\u2190\u00a0g 1 ( w 1,4 x\u00a0+\u00a0b 1,4 )y 2(1)\u00a0\u2190\u00a0g 2 ( w 2,1 y 1 \u00a0+\u00a0b 2,1 )\ny 2(2)\u00a0\u2190\u00a0g 2 ( w 2,2 y 1 \u00a0+\u00a0b 2,2 )\ny 2(3)\u00a0\u2190\u00a0g 2 ( w 2,3 y 1 \u00a0+\u00a0b 2,3 )\ny 2(4)\u00a0\u2190\u00a0g 2 ( w 2,4 y 1 \u00a0+\u00a0b 2,4 )y \u00a0\u2190\u00a0g 3 ( w 3,1 y 2 \u00a0+\u00a0b 3,1 )ylayer\u00a03\u00a0 (f 3 ) \u00a0 layer\u00a02\u00a0 ( f 2 ) \u00a0 layer\u00a01\u00a0 ( f 1 ) \u00a0\ny 1(1)\ny 1(4)y 2(4)y 2(3)y 2(2)y 2(1)\nFigure 1: A multilayer perceptron with two-dimensional input, two layers with four units and one output layer with one\nunit.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 5", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 610, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8788a6ea-b5c8-4b29-8336-415514881aff": {"__data__": {"id_": "8788a6ea-b5c8-4b29-8336-415514881aff", "embedding": null, "metadata": {"page_label": "65", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dbf10e25-8ce0-4cb4-bdd8-bea644d79cf6", "node_type": "4", "metadata": {"page_label": "65", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "6864eede04f945939581da24e43a82d4ed9f92dbd7ed00afcdb9931915a6de35", "class_name": "RelatedNodeInfo"}}, "text": "The data analyst can choose any mathematical function as gl,u, assuming it\u2019s di\ufb00erentiable2.\nThe latter property is essential for gradient descent used to \ufb01nd the values of the parameters\nwl,uandbl,ufor alllandu. The primary purpose of having nonlinear components in the\nfunctionfNNis to allow the neural network to approximate nonlinear functions. Without\nnonlinearities, fNNwould be linear, no matter how many layers it has. The reason is that\nWlz+blis a linear function and a linear function of a linear function is also linear.\nPopular choices of activation functions are the logistic function, already known to you, as well\nasTanHandReLU. The former is the hyperbolic tangent function, similar to the logistic\nfunction but ranging from \u22121to1(without reaching them). The latter is the recti\ufb01ed linear\nunit function, which equals to zero when its input zis negative and to zotherwise:\ntanh(z) =ez\u2212e\u2212z\nez+e\u2212z,\nrelu(z) ={\n0ifz<0\nzotherwise.\nAs I said above, Wlin the expression Wlz+bl, is a matrix, while blis a vector. That looks\ndi\ufb00erent from linear regression\u2019s wz+b. In matrix Wl, each rowucorresponds to a vector of\nparameters wl,u. The dimensionality of the vector wl,uequals to the number of units in the\nlayerl\u22121. The operation Wlzresults in a vector aldef=[wl,1z,wl,2z,...,wl,size lz]. Then\nthe sum al+blgives asizel-dimensional vector cl. Finally, the function gl(cl)produces the\nvector yldef= [y(1)\nl,y(2)\nl,...,y(size l)\nl]as output.\n6.2 Deep Learning\nDeep learning refers to training neural networks with more than two non-output layers. In the\npast, it became more di\ufb03cult to train such networks as the number of layers grew. The two\nbiggest challenges were referred to as the problems of exploding gradient andvanishing\ngradient as gradient descent was used to train the network parameters.\nWhile the problem of exploding gradient was easier to deal with by applying simple techniques\nlikegradient clipping and L1 or L2 regularization, the problem of vanishing gradient\nremained intractable for decades.\nWhat is vanishing gradient and why does it arise? To update the values of the parameters in\nneural networks the algorithm called backpropagation is typically used. Backpropagation\nis an e\ufb03cient algorithm for computing gradients on neural networks using the chain rule. In\nChapter 4, we have already seen how the chain rule is used to calculate partial derivatives of\na complex function. During gradient descent, the neural network\u2019s parameters receive an\n2The function has to be di\ufb00erentiable across its whole domain or in the majority of the points of its\ndomain. For example, ReLU is not di\ufb00erentiable at 0.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 6", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2692, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0e466327-b3fb-4d2f-b7ef-2caea8fb064a": {"__data__": {"id_": "0e466327-b3fb-4d2f-b7ef-2caea8fb064a", "embedding": null, "metadata": {"page_label": "66", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a618400-0ee9-45f9-a8ae-5d06524711ac", "node_type": "4", "metadata": {"page_label": "66", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "0d8fd36eed20f500528b0492b662bca7058a12c5512535470b719a1786d95374", "class_name": "RelatedNodeInfo"}}, "text": "update proportional to the partial derivative of the cost function with respect to the current\nparameter in each iteration of training. The problem is that in some cases, the gradient will\nbe vanishingly small, e\ufb00ectively preventing some parameters from changing their value. In\nthe worst case, this may completely stop the neural network from further training.\nTraditional activation functions, such as the hyperbolic tangent function I mentioned above,\nhave gradients in the range (0,1), and backpropagation computes gradients by the chain rule.\nThat has the e\ufb00ect of multiplying nof these small numbers to compute gradients of the earlier\n(leftmost) layers in an n-layer network, meaning that the gradient decreases exponentially\nwithn. That results in the e\ufb00ect that the earlier layers train very slowly, if at all.\nHowever, the modern implementations of neural network learning algorithms allow you to\ne\ufb00ectively train very deep neural networks (up to hundreds of layers). This is due to several\nimprovements combined together, including ReLU, LSTM (and other gated units; we consider\nthem below), as well as techniques such as skip connections used in residual neural\nnetworks , as well as advanced modi\ufb01cations of the gradient descent algorithm.\nTherefore, today, since the problems of vanishing and exploding gradient are mostly solved\n(or their e\ufb00ect diminished) to a great extent, the term \u201cdeep learning\u201d refers to training\nneural networks using the modern algorithmic and mathematical toolkit independently of\nhow deep the neural network is. In practice, many business problems can be solved with\nneural networks having 2-3 layers between the input and output layers. The layers that are\nneither input nor output are often called hidden layers .\n6.2.1 Convolutional Neural Network\nYou may have noticed that the number of parameters an MLP can have grows very fast as you\nmake your network bigger. More speci\ufb01cally, as you add one layer, you add (sizel\u22121+1)\u00b7sizel\nparameters (our matrix Wlplus the vector bl). That means that if you add another 1000-unit\nlayer to an existing neural network, then you add more than 1 million additional parameters\nto your model. Optimizing such big models is a very computationally intensive problem.\nWhen our training examples are images, the input is very high-dimensional3. If you want\nto learn to classify images using an MLP, the optimization problem is likely to become\nintractable.\nAconvolutional neural network (CNN) is a special kind of FFNN that signi\ufb01cantly\nreduces the number of parameters in a deep neural network with many units without losing\ntoo much in the quality of the model. CNNs have found applications in image and text\nprocessing where they beat many previously established benchmarks.\nBecause CNNs were invented with image processing in mind, I explain them on the image\nclassi\ufb01cation example.\n3Each pixel of an image is a feature. If our image is 100 by 100 pixels, then there are 10,000 features.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3030, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c2bd1df4-508e-4184-857b-2fbdaa7f8812": {"__data__": {"id_": "c2bd1df4-508e-4184-857b-2fbdaa7f8812", "embedding": null, "metadata": {"page_label": "67", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4aba9541-cccd-49e5-b2ac-e250522d9618", "node_type": "4", "metadata": {"page_label": "67", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "46a4e43192d992ed9ea4a65d3117ec2bc026d6cb2097d0676b8894f46ce1eb45", "class_name": "RelatedNodeInfo"}}, "text": "You may have noticed that in images, pixels that are close to one another usually represent\nthe same type of information: sky, water, leaves, fur, bricks, and so on. The exception from\nthe rule are the edges: the parts of an image where two di\ufb00erent objects \u201ctouch\u201d one another.\nIf we can train the neural network to recognize regions of the same information as well as the\nedges, this knowledge would allow the neural network to predict the object represented in the\nimage. For example, if the neural network detected multiple skin regions and edges that look\nlike parts of an oval with skin-like tone on the inside and bluish tone on the outside, then it\nis likely that it\u2019s a face on the sky background. If our goal is to detect people on pictures,\nthe neural network will most likely succeed in predicting a person in this picture.\nHaving in mind that the most important information in the image is local, we can split the\nimage into square patches using a moving window approach4. We can then train multiple\nsmaller regression models at once, each small regression model receiving a square patch as\ninput. The goal of each small regression model is to learn to detect a speci\ufb01c kind of pattern\nin the input patch. For example, one small regression model will learn to detect the sky;\nanother one will detect the grass, the third one will detect edges of a building, and so on.\nIn CNNs, a small regression model looks like the one in Figure 1, but it only has the layer 1\nand doesn\u2019t have layers 2and3. To detect some pattern, a small regression model has to\nlearn the parameters of a matrix F(for \u201c\ufb01lter\u201d) of size p\u00d7p, wherepis the size of a patch.\nLet\u2019s assume, for simplicity, that the input image is black and white, with 1representing\nblack and 0representing white pixels. Assume also that our patches are 3by3pixels (p= 3).\nSome patch could then look like the following matrix P(for \u201cpatch\u201d):\nP=\uf8ee\n\uf8f00 1 0\n1 1 1\n0 1 0\uf8f9\n\uf8fb.\nThe above patch represents a pattern that looks like a cross. The small regression model that\nwill detect such patterns (and only them) would need to learn a 3by3parameter matrix F\nwhere parameters at positions corresponding to the 1s in the input patch would be positive\nnumbers, while the parameters in positions corresponding to 0s would be close to zero. If\nwe calculate the convolution of matrices PandF, the value we obtain is higher the more\nsimilar Fis toP. To illustrate the convolution of two matrices, assume that Flooks like this:\nF=\uf8ee\n\uf8f00 2 3\n2 4 1\n0 3 0\uf8f9\n\uf8fb.\nThen convolution operator is only de\ufb01ned for matrices that have the same number of rows\nand columns. For our matrices of PandFit\u2019s calculated as illustrated below:\n4Consider this as if you looked at a dollar bill in a microscope. To see the whole bill you have to gradually\nmove your bill from left to right and from top to bottom. At each moment in time, you see only a part of the\nbill of \ufb01xed dimensions. This approach is called moving window .\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 8", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3010, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ff0a2bfd-77d4-4ed0-88e6-ec1699e4375e": {"__data__": {"id_": "ff0a2bfd-77d4-4ed0-88e6-ec1699e4375e", "embedding": null, "metadata": {"page_label": "68", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fa1c767d-4e6b-4a38-b356-672d353b0e17", "node_type": "4", "metadata": {"page_label": "68", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "61cc6a944159aba960d04fde6c3b744f8ffccc3e1ebae31722e8dda5000938bb", "class_name": "RelatedNodeInfo"}}, "text": "0 1 0\n1 1 1\n0 0 1convP F\noverlay0 2 3\n2 4 1\n0 0 30\n1 1\n00 1\n1\n0 1.\n.\n..\n.\n..\n.\n.sum\n123\n2 1\n0 00 2\n4\n3Figure 2: A convolution between two matrices.\nIf our input patch Phad a di\ufb00erent patten, for example, that of a letter T,\nP=\uf8ee\n\uf8f01 1 1\n0 1 0\n0 1 0\uf8f9\n\uf8fb,\nthen the convolution with Fwould give a lower result: 9. So, you can see the more the patch\n\u201clooks\u201d like the \ufb01lter, the higher the value of the convolution operation is. For convenience,\nthere\u2019s also a bias parameter bassociated with each \ufb01lter Fwhich is added to the result of a\nconvolution before applying the nonlinearity (activation function).\nOne layer of a CNN consists of multiple convolution \ufb01lters (each with its own bias parameter),\njust like one layer in a vanilla FFNN consists of multiple units. Each \ufb01lter of the \ufb01rst\n(leftmost) layer slides \u2014 or convolves \u2014 across the input image, left to right, top to bottom,\nand convolution is computed at each iteration.\nAn illustration of the process is given in Figure 3 where 6 steps of one \ufb01lter convolving across\nan image are shown.\nThe \ufb01lter matrix (one for each \ufb01lter in each layer) and bias values are trainable parameters\nthat are optimized using gradient descent with backpropagation.\nA nonlinearity is applied to the sum of the convolution and the bias term. Typically, the\nReLU activation function is used in all hidden layers. The activation function of the output\nlayer depends on the task.\nSince we can have sizel\ufb01lters in each layer l, the output of the convolution layer lwould\nconsist ofsizelmatrices, one for each \ufb01lter.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 9", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1606, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0af700ff-94de-4a09-acce-ff9436f1e0f1": {"__data__": {"id_": "0af700ff-94de-4a09-acce-ff9436f1e0f1", "embedding": null, "metadata": {"page_label": "69", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4ef05452-1812-44b9-90ae-7fea2cdd255a", "node_type": "4", "metadata": {"page_label": "69", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "4369ae43db7023576b8aca0a1e7c66495bef76ce1f77c6e6d22d03fa9110bd32", "class_name": "RelatedNodeInfo"}}, "text": "1\n1\n1\n1111\n10 0 \n0 \n0 0 0 0 0 -1 2 \n4 -2 ImageFilter  \n4 Output\u00a0before\nnonlinearity  \n1\n1\n1\n1111\n10 0 \n0 \n0 0 0 0 0 -1 2 \n4 -2 \n4 -1 \n1\n1\n1\n1111\n10 0 \n0 \n0 0 0 0 0 -1 2 \n4 -2 \n4 -1 7 Conv\u00a01\nConv\u00a02\nConv\u00a031\n1\n1\n1111\n10 0 \n0 \n0 0 0 0 0 -1 2 \n4 -2 \n4 \n1\n1\n1\n1111\n10 0 \n0 \n0 0 0 0 0 -1 2 \n4 -2 \n4 -1 \n1\n1\n1\n1111\n10 0 \n0 \n0 0 0 0 0 -1 2 \n4 -2 \n4 -1 7 -1 7 \n7 2 Conv\u00a04\n2 7 \n2 7 0 Conv\u00a05\nConv\u00a061Bias\n1\n1\n11\n1Figure 3: A \ufb01lter convolving across an image.\nIf the CNN has one convolution layer following another convolution layer, then the subsequent\nlayerl+ 1treats the output of the preceding layer las a collection of sizelimage matrices.\nSuch a collection is called a volume. The size of that collection is called the volume\u2019s depth.\nEach \ufb01lter of layer l+ 1convolves the whole volume. The convolution of a patch of a volume\nis simply the sum of convolutions of the corresponding patches of individual matrices the\nvolume consists of.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 10", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 991, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "84f69743-60b0-4e4e-8812-87c86274e7e5": {"__data__": {"id_": "84f69743-60b0-4e4e-8812-87c86274e7e5", "embedding": null, "metadata": {"page_label": "70", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3e69a925-8276-4828-8be8-47e81923fca0", "node_type": "4", "metadata": {"page_label": "70", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "bca2e4fb5bba54805042737423b27fadb593e042f0352c5fecca4b292f493140", "class_name": "RelatedNodeInfo"}}, "text": "Figure 4: Convolution of a volume consisting of three matrices.\nAn example of a convolution of a patch of a volume consisting of depth 3is shown in Figure\n4. The value of the convolution, \u22123, was obtained as (\u22122\u00b73 + 3\u00b71 + 5\u00b74 +\u22121\u00b71) + (\u22122\u00b72 +\n3\u00b7(\u22121) + 5\u00b7(\u22123) +\u22121\u00b71) + (\u22122\u00b71 + 3\u00b7(\u22121) + 5\u00b72 +\u22121\u00b7(\u22121)) + (\u22122).\nIn computer vision, CNNs often get volumes as input, since an image is usually represented\nby three channels: R, G, and B, each channel being a monochrome picture.\nTwo important properties of convolution are strideandpadding .\nStride is the step size of the moving window. In Figure 3, the\nstride is 1, that is the \ufb01lter slides to the right and to the bottom\nby one cell at a time. In Figure 5, you can see a partial example\nof convolution with stride 2. You can see that the output matrix\nis smaller when stride is bigger.\nPadding allows getting a larger output matrix; it\u2019s the width of\nthe square of additional cells with which you surround the image\n(or volume) before you convolve it with the \ufb01lter. The cells added\nby padding usually contain zeroes. In Figure 3, the padding is 0, so no additional cells are\nadded to the image. In Figure 6, on the other hand, the stride is 2and padding is 1, so a\nsquare of width 1of additional cells are added to the image. You can see that the output\nmatrix is bigger when padding is bigger5.\n5To save space, in Figure 6, only the \ufb01rst two of the nine convolutions are shown.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 11", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1488, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7c5d768e-c814-4553-9e41-702fc4addd71": {"__data__": {"id_": "7c5d768e-c814-4553-9e41-702fc4addd71", "embedding": null, "metadata": {"page_label": "71", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "73a89634-2cb5-4c16-a73b-89ad22fcb4ed", "node_type": "4", "metadata": {"page_label": "71", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "5165bf34dafe522623231db25e73222292d33aa16dc5c6bbc876801d0b709764", "class_name": "RelatedNodeInfo"}}, "text": "1\n1\n1\n1111\n10 0 \n0 \n0 0 0 0 0 -1 2 \n4 -2 ImageFilter  \n4 Output\u00a0before\nnonlinearity  \n1\n1\n1\n1111\n10 0 \n0 \n0 0 0 0 0 -1 2 \n4 -2 \n4 7 Conv\u00a01\nConv\u00a021Bias\n11\n1\n1\n1111\n10 0 \n0 \n0 0 0 0 0 -1 2 \n4 -2 \n4 \nConv\u00a03\n17 \n0 \n1\n1\n1\n1111\n10 0 \n0 \n0 0 0 0 0 -1 2 \n4 -2 \n4 \nConv\u00a04\n17 \n0 -1 Figure 5: Convolution with stride 2.\n0\n0\n0\n0\n0 0 0 0 00 0 0 0 0 0\n0\n0\n0\n0\n01\n1\n1\n1111\n10 0 \n0 \n0 0 0 0 0 -1 2 \n4 -2 Image\u00a0with\u00a0padding\u00a01Filter  \n-1 Output\u00a0before\nnonlinearity  \nConv\u00a01\n1Bias\n0\n0\n0\n0\n0 0 0 0 00 0 0 0 0 0\n0\n0\n0\n0\n01\n1\n1\n1111\n10 0 \n0 \n0 0 0 0 0 -1 2 \n4 -2 \n-1 0\nConv\u00a02\n1\nFigure 6: Convolution with stride 2and padding 1.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 12", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 669, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d446ef3c-9ec8-4f4d-979e-6d99bac96e05": {"__data__": {"id_": "d446ef3c-9ec8-4f4d-979e-6d99bac96e05", "embedding": null, "metadata": {"page_label": "72", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4b16ebc7-bc2e-47e4-b6cb-043478a5b937", "node_type": "4", "metadata": {"page_label": "72", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "b4331f9982a6b87c83a7ae4f8f9f97708b5da081651c323d1187694912b10dfd", "class_name": "RelatedNodeInfo"}}, "text": "An example of an image with padding 2is shown in Figure 7. Padding is helpful with larger\n\ufb01lters because it allows them to better \u201cscan\u201d the boundaries of the image.\nFigure 7: Image with padding 2.\nThis section would not be complete without presenting pooling , a technique very often used\nin CNNs. Pooling works in a way very similar to convolution, as a \ufb01lter applied using a\nmoving window approach. However, instead of applying a trainable \ufb01lter to an input matrix\nor a volume, pooling layer applies a \ufb01xed operator, usually either maxoraverage. Similarly\nto convolution, pooling has hyperparameters: the size of the \ufb01lter and the stride. An example\nofmaxpooling with \ufb01lter of size 2and stride 2is shown in Figure 8.\nUsually, a pooling layer follows a convolution layer, and it gets the output of convolution\nas input. When pooling is applied to a volume, each matrix in the volume is processed\nindependently of others. Therefore, the output of the pooling layer applied to a volume is a\nvolume of the same depth as the input.\nAs you can see, pooling only has hyperparameters and doesn\u2019t have parameters to learn.\nTypically, the \ufb01lter of size 2or3and stride 2are used in practice. Max pooling is more\npopular than average and often gives better results.\nTypically pooling contributes to the increased accuracy of the model. It also improves the\nspeed of training by reducing the number of parameters of the neural network. (As you can\nsee in Figure 8, with \ufb01lter size 2and stride 2the number of parameters is reduced to 25%,\nthat is to 4parameters instead of 16.)\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 13", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1630, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b6519a99-6eb7-439f-8d2b-f0ae36f643f1": {"__data__": {"id_": "b6519a99-6eb7-439f-8d2b-f0ae36f643f1", "embedding": null, "metadata": {"page_label": "73", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f83e2fd0-70ac-4676-8137-970b4fa47ea0", "node_type": "4", "metadata": {"page_label": "73", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "03ca0f637cf47c1527561a89cf0f10a50349825a8601d82f45ee4aa32ff91584", "class_name": "RelatedNodeInfo"}}, "text": "3\n5\n-3\n5564\n29 -1 \n1 \n7 4 2 1 8 Image\n8 Output  \n8 6 Pool\u00a01\nPool\u00a028 \nPool\u00a036 \n5 \n8 \nPool\u00a046 \n5 9 3\n5\n-3\n5564\n29 -1 \n1 \n7 4 2 1 8 3\n5\n-3\n5564\n29 -1 \n1 \n7 4 2 1 8 \n3\n5\n-3\n5564\n29 -1 \n1 \n7 4 2 1 8 Figure 8: Pooling with \ufb01lter of size 2and stride 2.\n6.2.2 Recurrent Neural Network\nRecurrent neural networks (RNNs) are used to label, classify, or generate sequences. A\nsequence is a matrix, each row of which is a feature vector and the order of rows matters.\nTo label a sequence is to predict a class for each feature vector in a sequence. To classify a\nsequence is to predict a class for the entire sequence. To generate a sequence is to output\nanother sequence (of a possibly di\ufb00erent length) somehow relevant to the input sequence.\nRNNs are often used in text processing because sentences and texts are naturally sequences\nof either words/punctuation marks or sequences of characters. For the same reason, recurrent\nneural networks are also used in speech processing.\nA recurrent neural network is not feed-forward: it contains loops. The idea is that each unit\nuof recurrent layer lhas a real-valued statehl,u. The state can be seen as the memory of\nthe unit. In RNN, each unit uin each layer lreceives two inputs: a vector of states from the\nprevious layer l\u22121and the vector of states from this same layer lfrom the previous time\nstep.\nTo illustrate the idea, let\u2019s consider the \ufb01rst and the second recurrent layers of an RNN. The\n\ufb01rst (leftmost) layer receives a feature vector as input. The second layer receives the output\nof the \ufb01rst layer as input.\nThis situation is schematically depicted in Figure 9. As I said above, each training example\nis a matrix in which each row is a feature vector. For simplicity, let\u2019s illustrate this matrix\nas a sequence of vectors X= [x1,x2,...,xt\u22121,xt,xt+1,...,xlength X], wherelength Xis the\nlength of the input sequence. If our input example Xis a text sentence, then feature vector\nxtfor eacht= 1,...,length Xrepresents a word in the sentence at position t.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 14", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2063, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "158146fd-2925-4430-9c4d-a7d15b06900a": {"__data__": {"id_": "158146fd-2925-4430-9c4d-a7d15b06900a", "embedding": null, "metadata": {"page_label": "74", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8b62eb6c-9ca7-4889-b8e5-96ece790887d", "node_type": "4", "metadata": {"page_label": "74", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "1edebc027e57dc71821fe2aff665b83c4f36d82d68c4606d5b1136cae1292b02", "class_name": "RelatedNodeInfo"}}, "text": "xt\u00a0\u2190\u00a0[x(1),t,x(2),t]x(2),t\nht\u00ad 11,1\nht\u00ad 1l,2ht1,1\nht1,1\u00a0\u2190\u00a0g1( w1,1 xt\u00a0+\u00a0 u 1,1 ht\u00ad 11\u00a0+\u00a0b 1,1)yt\n1\u00a0\u2190\u00a0 g2( V1 ht\n1\u00a0+\u00a0 c1)\nx(1),t\nx(2),t\nht\u00ad 1\n1,1\nht\u00ad 11,2ht\n1,2\nht1,2\u00a0\u2190\u00a0g1( w1,2 xt\u00a0+\u00a0 u 1,2 ht\u00ad 1\n1\u00a0+\u00a0b1,2)x(1),t\nxt\u00a0\u2190\u00a0[x(1),t,x(2),t]layer\u00a01\nht1\u00a0\u2190\u00a0[ht1,1,ht1,2]\nht\u00ad 12,1\nht\u00ad 12,2ht2,1\nht2,1 \u00a0\u2190 \u00a0g 1( w 2,1 h 1t\u00a0+\u00a0 u 2,1 ht\u00ad 12\u00a0+\u00a0b 2,1)\nht\u00ad 1\n2,1\nht\u00ad 12,2ht\n2,2layer\u00a02\nht1\u00a0\u2190\u00a0[ht1,1,ht1,2]\nyt2 \u00a0\u2190 \u00a0 g 2( V 2 ht2\u00a0+\u00a0 c 2)ht2 \u00a0\u2190[ht2,1,ht2,2]yt1 yt2\nht\n1\u00a0\u2190\u00a0[ht\n1,1,ht1,2]\nht\n2,2\u00a0\u2190\u00a0g1( w2,2 ht\n1\u00a0+\u00a0 u 2,2 ht\u00ad 12\u00a0+\u00a0b 2,2)\nFigure 9: The \ufb01rst two layers of an RNN. The input feature vector is two-dimensional; each layer has two units.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 686, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b08db4ef-854f-412b-9eea-6de6b74341f2": {"__data__": {"id_": "b08db4ef-854f-412b-9eea-6de6b74341f2", "embedding": null, "metadata": {"page_label": "75", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f2de2223-c69c-450a-a273-63b345aa8043", "node_type": "4", "metadata": {"page_label": "75", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "6a0d962fd2628fa03a7bb4b76a43f2794bd7247ab3678431b84215e3c5637e64", "class_name": "RelatedNodeInfo"}}, "text": "As depicted in Figure 9, in an RNN, the feature vectors from an input example are \u201cread\u201d by\nthe neural network sequentially in the order of the timesteps. The index tdenotes a timestep.\nTo update the state ht\nl,uat each timestep tin each unit uof each layer lwe \ufb01rst calculate a\nlinear combination of the input feature vector with the state vector ht\u22121\nl,uof this same layer\nfrom the previous timestep, t\u22121. The linear combination of two vectors is calculated using\ntwo parameter vectors wl,u,ul,uand a parameter bl,u. The value of ht\nl,uis then obtained by\napplying activation function g1to the result of the linear combination. A typical choice for\nfunctiong1istanh. The output yt\nlis typically a vector calculated for the whole layer lat\nonce. To obtain yt\nl, we use activation function g2that takes a vector as input and returns a\ndi\ufb00erent vector of the same dimensionality. The function g2is applied to a linear combination\nof the state vector values ht\nl,ucalculated using a parameter matrix Vland a parameter vector\ncl,u. In classi\ufb01cation, a typical choice for g2is the softmax function :\n\u03c3(z)def= [\u03c3(1),...,\u03c3(D)],where\u03c3(j)def=exp(\nz(j))\n\u2211D\nk=1exp(\nz(k)).\nThe softmax function is a generalization of the sigmoid function to multidimensional outputs.\nIt has the property that\u2211D\nj=1\u03c3(j)= 1and\u03c3(j)>0for allj.\nThe dimensionality of Vlis chosen by the data analyst such that multiplication of matrix Vl\nby the vector ht\nlresults in a vector of the same dimensionality as that of the vector cl. This\nchoice depends on the dimensionality for the output label yin your training data. (Until\nnow we only saw one-dimensional labels, but we will see in the future chapters that labels\ncan be multidimensional as well.)\nThe values of wl,u,ul,u,bl,u,Vl,u, andcl,uare computed from the training data using gradient\ndescent with backpropagation. To train RNN models, a special version of backpropagation is\nused called backpropagation through time .\nBoth tanhand softmaxsu\ufb00er from the vanishing gradient problem. Even if our RNN has just\none or two recurrent layers, because of the sequential nature of the input, backpropagation\nhas to \u201cunfold\u201d the network over time. From the point of view of the gradient calculation, in\npractice this means that the longer is the input sequence, the deeper is the unfolded network.\nAnother problem RNNs have is that of handling long-term dependencies. As the length of\nthe input sequence grows, the feature vectors from the beginning of the sequence tend to\nbe \u201cforgotten,\u201d because the state of each unit, which serves as network\u2019s memory, becomes\nsigni\ufb01cantly a\ufb00ected by the feature vectors read more recently. Therefore, in text or speech\nprocessing, the cause-e\ufb00ect link between distant words in a long sentence can be lost.\nThe most e\ufb00ective recurrent neural network models used in practice are gated RNNs . These\ninclude the long short-term memory (LSTM) networks and networks based on the gated\nrecurrent unit (GRU).\nThe beauty of using gated units in RNNs is that such networks can store information in their\nunits for future use, much like bits in a computer\u2019s memory. The di\ufb00erence with the real\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 16", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3197, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7d8f1905-1e73-402c-a430-0a773fe11a12": {"__data__": {"id_": "7d8f1905-1e73-402c-a430-0a773fe11a12", "embedding": null, "metadata": {"page_label": "76", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "02c012ff-ec10-4a3d-8f41-f60e812ac611", "node_type": "4", "metadata": {"page_label": "76", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "64801ef9ca5cc8264daff304ecd0c1d79ed827e3a71208f519e04bfce945d90c", "class_name": "RelatedNodeInfo"}}, "text": "memory is that reading, writing, and erasure of information stored in each unit is controlled\nby activation functions that take values in the range (0,1). The trained neural network can\n\u201cread\u201d the input sequence of feature vectors and decide at some early time step tto keep\nspeci\ufb01c information about the feature vectors. That information about the earlier feature\nvectors can later be used by the model to process the feature vectors from near the end of\nthe input sequence. For example, if the input text starts with the word she, a language\nprocessing RNN model could decide to store the information about the gender to interpret\ncorrectly the word theirseen later in the sentence.\nUnits make decisions about what information to store, and when to allow reads, writes, and\nerasures. Those decisions are learned from data and implemented through the concept of\ngates. There are several architectures of gated units. A simple but e\ufb00ective one is called the\nminimal gated GRU and is composed of a memory cell, and a forget gate.\nLet\u2019s look at the math of a GRU unit on an example of the \ufb01rst layer of the RNN (the one\nthat takes the sequence of feature vectors as input). A minimal gated GRU unit uin layer\nltakes two inputs: the vector of the memory cell values from all units in the same layer\nfrom the previous timestep, ht\u22121\nl, and a feature vector xt. It then uses these two vectors like\nfollows (all operations in the below sequence are executed in the unit one after another):\n\u02dcht\nl,u\u2190g1(wl,uxt+ul,uht\u22121\nl+bl,u),\n\u0393t\nl,u\u2190g2(ml,uxt+ol,uht\u22121+al,u),\nht\nl,u\u2190\u0393t\nl,u\u02dcht\nl+ (1\u2212\u0393t\nl,u)ht\u22121\nl,\nht\nl\u2190[ht\nl,1,...,ht\nl,size l]\nyt\nl\u2190g3(Vlht\nl+cl,u),\nwhereg1is the tanhactivation function, g2is called the gate\nfunction and is implemented as the sigmoid function taking values\nin the range (0,1). If the gate \u0393l,uis close to 0, then the memory\ncell keeps its value from the previous time step, ht\u22121\nl. On the other\nhand, if the gate \u0393l,uis close to 1, the value of the memory cell\nis overwritten by a new value \u02dcht\nl,u(see the third assignment from\nthe top). Just like in standard RNNs, g3is usually softmax.\nA gated unit takes an input and stores it for some time. This\nis equivalent to applying the identity function ( f(x) =x) to the\ninput. Because the derivative of the identity function is constant, when a network with gated\nunits is trained with backpropagation through time, the gradient does not vanish.\nOther important extensions to RNNs include bi-directional RNNs , RNNs with attention\nandsequence-to-sequence RNN models. The latter, in particular, are frequently used to\nbuild neural machine translation models and other models for text to text transformations.\nA generalization of an RNN is a recursive neural network .\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 17", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2789, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e5b1d44-088b-400e-b864-b86d851a2e0e": {"__data__": {"id_": "4e5b1d44-088b-400e-b864-b86d851a2e0e", "embedding": null, "metadata": {"page_label": "77", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "671a4e26-0453-4a71-9a17-08cd9faca12c", "node_type": "4", "metadata": {"page_label": "77", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "03de14ff41951cd3ad503d5f9a733ecd82780c2f467530d6a638bd4610edc0df", "class_name": "RelatedNodeInfo"}}, "text": "7 Problems and Solutions\n7.1 Kernel Regression\nWe talked about linear regression, but what if our data doesn\u2019t have the form of a straight\nline? Polynomial regression could help. Let\u2019s say we have a one-dimensional data {(xi,yi)}N\ni=1.\nWe could try to \ufb01t a quadratic line y=w1xi+w2x2\ni+bto our data. By de\ufb01ning the mean\nsquared error (MSE) cost function, we could apply gradient descent and \ufb01nd the values of\nparameters w1,w2, andbthat minimize this cost function. In one- or two-dimensional space,\nwe can easily see whether the function \ufb01ts the data. However, if our input is a D-dimensional\nfeature vector, with D> 3, \ufb01nding the right polynomial would be hard.\nKernel regression is a non-parametric method. That means that there are no parameters to\nlearn. The model is based on the data itself (like in kNN). In its simplest form, in kernel\nregression we look for a model like this:\nf(x) =1\nNN\u2211\ni=1wiyi,wherewi=Nk(xi\u2212x\nb)\n\u2211N\nl=1k(xl\u2212x\nb). (1)\nThe function k(\u00b7)is called a kernel. The kernel plays the role of a similarity function: the\nvalues of coe\ufb03cients wiare higher when xis similar to xiand lower when they are dissimilar.\nKernels can have di\ufb00erent forms, the most frequently used one is the Gaussian kernel:\nk(z) =1\u221a\n2\u03c0exp(\u2212z2\n2)\n.\nGood \ufb01t\n Slight over\ufb01t\n Strong over\ufb01t\nFigure 1: Example of kernel regression line with a Gaussian kernel for three values of b.\nThe valuebis a hyperparameter that we tune using the validation set (by running the model\nbuilt with a speci\ufb01c value of bon the validation set examples and calculating the MSE). You\ncan see an illustration of the in\ufb02uence bhas on the shape of the regression line in Figure 1.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 3", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1707, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "82d812b8-e543-4d5d-94c5-6891b9c3794c": {"__data__": {"id_": "82d812b8-e543-4d5d-94c5-6891b9c3794c", "embedding": null, "metadata": {"page_label": "78", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "84dc5868-0544-492d-9926-8ebcce33a25d", "node_type": "4", "metadata": {"page_label": "78", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "8fca2c42b48bb27d64162cf4fa5f5985d549bfce3a0ff14fb0dba6b867d512da", "class_name": "RelatedNodeInfo"}}, "text": "If your inputs are multi-dimensional feature vectors, the terms xi\u2212xandxl\u2212xin eq. 1\nhave to be replaced by Euclidean distance \u2225xi\u2212x\u2225and\u2225xl\u2212x\u2225respectively.\n7.2 Multiclass Classi\ufb01cation\nAlthough many classi\ufb01cation problems can be de\ufb01ned using two classes, some are de\ufb01ned\nwith more than two classes, which requires adaptations of our machine learning algorithms.\nIn multiclass classi\ufb01cation, the label can be one of Cclasses:y\u2208{1,...,C}. Many machine\nlearning algorithms are binary; SVM is an example. Some algorithms can naturally be\nextended to handle multiclass problems. ID3 and other decision tree learning algorithms can\nbe simply changed like this:\nfS\nID3def= Pr(yi=c|x) =1\n|S|\u2211\n{y|(x,y)\u2208S,y=c}y,\nfor allc\u2208{1,...,C}, whereSis the leaf node in which the prediction is made.\nLogistic regression can be naturally extended to multiclass learning problems by replacing\nthe sigmoid function with the softmax function which we already saw in Chapter 6.\nThe kNN algorithm is also straightforward to extend to the multiclass case: when we \ufb01nd\nthekclosest examples for the input xand examine them, we return the class that we saw\nthe most among the kexamples.\nSVM cannot be naturally extended to multiclass problems. Other algorithms can be imple-\nmented more e\ufb03ciently in the binary case. What should you do if you have a multiclass\nproblem but a binary classi\ufb01cation learning algorithm? One common strategy is called one\nversus rest . The idea is to transform a multiclass problem into Cbinary classi\ufb01cation\nproblems and build Cbinary classi\ufb01ers. For example, if we have three classes, y\u2208{1,2,3},\nwe create copies of the original datasets and modify them. In the \ufb01rst copy, we replace all\nlabels not equal to 1by0. In the second copy, we replace all labels not equal to 2by0. In the\nthird copy, we replace all labels not equal to 3by0. Now we have three binary classi\ufb01cation\nproblems where we have to learn to distinguish between labels 1and0,2and0, and 3and0.\nOnce we have the three models, to classify the new input feature vector x, we apply the three\nmodels to the input, and we get three predictions. We then pick the prediction of a non-zero\nclass which is the most certain . Remember that in logistic regression, the model returns not\na label but a score (between 0and1) that can be interpreted as the probability that the\nlabel is positive. We can also interpret this score as the certainty of prediction. In SVM, the\nanalog of certainty is the distance dfrom the input xto the decision boundary given by,\nddef=w\u2217x+b\u2217\n\u2225w\u2225.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 4", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2590, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4a8af527-96b6-4b97-a8c9-00dcaee75ec0": {"__data__": {"id_": "4a8af527-96b6-4b97-a8c9-00dcaee75ec0", "embedding": null, "metadata": {"page_label": "79", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a2b046e6-16b2-4143-ac1a-e10772a9b55a", "node_type": "4", "metadata": {"page_label": "79", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "a864b07f328e4a96e8756da7d93273d44922d1c25c66f6af1d0e5827202166e2", "class_name": "RelatedNodeInfo"}}, "text": "The larger the distance, the more certain is the prediction. Most learning algorithm either\ncan be naturally converted to a multiclass case, or they return a score we can use in the one\nversus rest strategy.\n7.3 One-Class Classi\ufb01cation\nSometimes we only have examples of one class and we want to train a model that would\ndistinguish examples of this class from everything else.\nOne-class classi\ufb01cation , also known as unary classi\ufb01cation orclass modeling , tries\nto identify objects of a speci\ufb01c class among all objects, by learning from a training set\ncontaining only the objects of that class. That is di\ufb00erent from and more di\ufb03cult than the\ntraditional classi\ufb01cation problem, which tries to distinguish between two or more classes\nwith the training set containing objects from all classes. A typical one-class classi\ufb01cation\nproblem is the classi\ufb01cation of the tra\ufb03c in a secure computer network as normal. In this\nscenario, there are few, if any, examples of the tra\ufb03c under an attack or during an intrusion.\nHowever, the examples of normal tra\ufb03c are often in abundance. One-class classi\ufb01cation\nlearning algorithms are used for outlier detection, anomaly detection, and novelty detection.\nThere are several one-class learning algorithms. The most widely used in practice are\none-class Gaussian ,one-class k-means ,one-class kNN , andone-class SVM .\nThe idea behind the one-class gaussian is that we model our data as if it came from a Gaussian\ndistribution, more precisely multivariate normal distribution (MND). The probability density\nfunction (pdf) for MND is given by the following equation:\nf\u00b5,\u03a3(x) =exp(\n\u22121\n2(x\u2212\u00b5)\u22a4\u03a3\u22121(x\u2212\u00b5))\n\u221a\n(2\u03c0)D|\u03a3|,\nwheref\u00b5,\u03a3(x)returns the probability density corresponding to the input feature vector x.\nProbability density can be interpreted as the likelihood that example xwas drawn from the\nprobability distribution we model as an MND. Values \u00b5(a vector) and \u03a3(a matrix) are\nthe parameters we have to learn. The maximum likelihood criterion (similarly to how we\nsolved the logistic regression learning problem) is optimized to \ufb01nd the optimal values for\nthese two parameters. |\u03a3|def= det \u03a3is the determinant of the matrix \u03a3; the notation \u03a3\u22121\nmeans the inverseof the matrix \u03a3.\nIf the terms determinant andinverseare new to you, don\u2019t worry. These are standard\noperations on vector and matrices from the branch of mathematics called matrix theory . If\nyou feel the need to know what they are, Wikipedia explains well these concepts.\nIn practice, the numbers in the vector \u00b5determine the place where the curve of our Gaussian\ndistribution is centered, while the numbers in \u03a3determine the shape of the curve. For\na training set consisting of two-dimensional feature vectors, an example of the one-class\nGaussian model is given in Figure 2.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 5", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2831, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "280f327d-d51f-443c-abd9-7c6dd0a88929": {"__data__": {"id_": "280f327d-d51f-443c-abd9-7c6dd0a88929", "embedding": null, "metadata": {"page_label": "80", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb66a87e-c56c-40a2-b8a1-4a75ea693a26", "node_type": "4", "metadata": {"page_label": "80", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "ef4951e8ce567f6afc1c55d83f72090c394ab4d47d11161277806ca02f71bd10", "class_name": "RelatedNodeInfo"}}, "text": "Figure 2: One-class classi\ufb01cation solved using the one-class gaussian method. Left: two-\ndimensional feature vectors. Right: the MND curve that maximizes the likelihood of the\nexamples on the left.\nOnce we have our model parametrized by \u00b5and\u03a3learned from data, we predict the likelihood\nof every input xby usingf\u00b5,\u03a3(x). Only if the likelihood is above a certain threshold, we\npredict that the example belongs to our class; otherwise, it is classi\ufb01ed as the outlier. The\nvalue of the threshold is found experimentally or using an \u201ceducated guess.\u201d\nWhen the data has a more complex shape, a more advanced algorithm can use a combination\nof several Gaussians (called a mixture of Gaussians). In this case, there are more parameters\nto learn from data: one \u00b5and one \u03a3for each Gaussian as well as the parameters that allow\ncombining multiple Gaussians to form one pdf. In Chapter 9, we consider a mixture of\nGaussians with an application to clustering.\nOne-class k-means and one-class kNN are based on a similar prin-\nciple as that of one-class Gaussian: build some model of the data\nand then de\ufb01ne a threshold to decide whether our new feature\nvector looks similar to other examples according to the model. In\nthe former, all training examples are clustered using the k-means\nclustering algorithm and, when a new example xis observed, the\ndistanced(x)is calculated as the minimum distance between x\nand the center of each cluster. If d(x)is less than a particular\nthreshold, then xbelongs to the class.\nOne-class SVM, depending on formulation, tries either 1) to separate all training examples\nfrom the origin (in the feature space) and maximize the distance from the hyperplane to\nthe origin, or 2) to obtain a spherical boundary around the data by minimizing the volume\nof this hypersphere. I leave the description of the one-class kNN algorithm, as well as the\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 6", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1922, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29586fdf-d098-40a1-9119-5ba5880435d1": {"__data__": {"id_": "29586fdf-d098-40a1-9119-5ba5880435d1", "embedding": null, "metadata": {"page_label": "81", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "83f9605f-2565-4e30-8d4e-14d2d3b22650", "node_type": "4", "metadata": {"page_label": "81", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "f1169922e06d835c6ea22cc123d34906a286c709527a14361e8ecb4c436daa06", "class_name": "RelatedNodeInfo"}}, "text": "details of the one-class k-means and one-class SVM for the complementary reading.\n7.4 Multi-Label Classi\ufb01cation\nIn some situations, more than one label is appropriate to describe an example from the\ndataset. In this case, we talk about the multi-label classi\ufb01cation .\nFor instance, if we want to describe an image, we could assign several labels to it: \u201cconifer,\u201d\n\u201cmountain,\u201d \u201croad,\u201d all three at the same time (Figure 3).\nFigure 3: A picture labeled as \u201cconifer,\u201d \u201cmountain,\u201d and \u201croad.\u201d Photo: Cate Lagadia.\nIf the number of possible values for labels is high, but they are all of the same nature, like\ntags, we can transform each labeled example into several labeled examples, one per label.\nThese new examples all have the same feature vector and only one label. That becomes a\nmulticlass classi\ufb01cation problem. We can solve it using the one versus rest strategy. The\nonly di\ufb00erence with the usual multiclass problem is that now we have a new hyperparameter:\nthreshold. If the prediction score for some label is above the threshold, this label is predicted\nfor the input feature vector. In this scenario, multiple labels can be predicted for one feature\nvector. The value of the threshold is chosen using the validation set.\nAnalogously, algorithms that naturally can be made multiclass (decision trees, logistic\nregression and neural networks among others) can be applied to multi-label classi\ufb01cation\nproblems. Because they return the score for each class, we can de\ufb01ne a threshold and then\nassign multiple labels to one feature vector if the threshold is above some value.\nNeural networks algorithms can naturally train multi-label classi\ufb01cation models by using the\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1734, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8b6d9912-c11b-4511-a5b2-69e2d609a437": {"__data__": {"id_": "8b6d9912-c11b-4511-a5b2-69e2d609a437", "embedding": null, "metadata": {"page_label": "82", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "500061dc-92f7-4365-8406-99234abea413", "node_type": "4", "metadata": {"page_label": "82", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "f61bdce7a6efdd6beac82353a28f3540b38db2c7bf2b2519846ed887d61fe46f", "class_name": "RelatedNodeInfo"}}, "text": "binary cross-entropy cost function. The output layer of the neural network, in this case,\nhas one unit per label. Each unit of the output layer has the sigmoid activation function.\nAccordingly, each label lis binary (yi,l\u2208{0,1}), wherel= 1,...,Landi= 1,...,N. The\nbinary cross-entropy of predicting the probability \u02c6yi,lthat example xihas labellis de\ufb01ned\nas,\n\u2212(yi,lln(\u02c6yi,l) + (1\u2212yi,l) ln(1\u2212\u02c6yi,l)).\nThe minimization criterion is simply the average of all binary cross-entropy terms across all\ntraining examples and all labels of those examples.\nIn cases where the number of possible values each label can take is small, one can convert\nmultilabel into a multiclass problem using a di\ufb00erent approach. Imagine the following problem.\nWe want to label images and labels can be of two types. The \ufb01rst type of label can have\ntwo possible values: {photo,painting}; the label of the second type can have three possible\nvalues{portrait,paysage,other }. We can create a new fake class for each combination of\nthe two original classes, like this:\nFake Class Real Class 1 Real Class 2\n1 photo portrait\n2 photo paysage\n3 photo other\n4 painting portrait\n5 painting paysage\n6 painting other\nNow we have the same labeled examples, but we replace real multi-labels with one fake label\nwith values from 1to6. This approach works well in practice when there are not too many\npossible combinations of classes. Otherwise, you need to use much more training data to\ncompensate for an increased set of classes.\nThe primary advantage of this latter approach is that you keep your labels correlated,\ncontrary to the previously seen methods that predict each label independently of one another.\nCorrelation between labels can be essential in many problems. For example, if you want to\npredict whether an email is spamornot_spam at the same time as predicting whether it\u2019s\nordinary orpriorityemail. You would like to avoid predictions like [spam,priority ].\n7.5 Ensemble Learning\nThe fundamental algorithms that we considered in Chapter 3 have their limitations. Because\nof their simplicity, sometimes they cannot produce a model accurate enough for your problem.\nYou could try using deep neural networks. However, in practice, deep neural networks require\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 8", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2293, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "664ad79e-23ce-4054-9fb4-3a96e2927e86": {"__data__": {"id_": "664ad79e-23ce-4054-9fb4-3a96e2927e86", "embedding": null, "metadata": {"page_label": "83", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "63e3323b-29c5-437a-a896-71540232be79", "node_type": "4", "metadata": {"page_label": "83", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "66d9ec1ce9dfbe74652a4fe3d4d3b88948a73017a16e4940d46973e940232e69", "class_name": "RelatedNodeInfo"}}, "text": "a signi\ufb01cant amount of labeled data which you might not have. Another approach to boost\nthe performance of simple learning algorithms is ensemble learning .\nEnsemble learning is a learning paradigm that, instead of trying to learn one super-accurate\nmodel, focuses on training a large number of low-accuracy models and then combining the\npredictions given by those weakmodels to obtain a high-accuracy meta-model .\nLow-accuracy models are usually learned by weak learners , that is learning algorithms that\ncannot learn complex models, and thus are typically fast at the training and at the prediction\ntime. The most frequently used weak learner is a decision tree learning algorithm in which\nwe often stop splitting the training set after just a few iterations. The obtained trees are\nshallow and not particularly accurate, but the idea behind ensemble learning is that if the\ntrees are not identical and each tree is at least slightly better than random guessing, then we\ncan obtain high accuracy by combining a large number of such trees.\nTo obtain the prediction for input x, the predictions of each weak model are combined using\nsome sort of weighted voting. The speci\ufb01c form of vote weighting depends on the algorithm,\nbut, independently of the algorithm, the idea is the same: if the council of weak models\npredicts that the message is spam, then we assign the label spamtox.\nTwo principal ensemble learning methods are boosting andbagging .\n7.5.1 Boosting and Bagging\nBoosting consists of using the original training data and iteratively create multiple models by\nusing a weak learner. Each new model would be di\ufb00erent from the previous ones in the sense\nthat the weak learner, by building each new model tries to \u201c\ufb01x\u201d the errors which previous\nmodels make. The \ufb01nal ensemble model is a certain combination of those multiple weak\nmodels built iteratively.\nBagging consists of creating many \u201ccopies\u201d of the training data (each copy is slightly di\ufb00erent\nfrom another) and then apply the weak learner to each copy to obtain multiple weak models\nand then combine them. A widely used and e\ufb00ective machine learning algorithm based on\nthe idea of bagging is random forest .\n7.5.2 Random Forest\nThe \u201cvanilla\u201d bagging algorithm works like follows. Given a training set, we create Brandom\nsamplesSb(for eachb= 1,...,B) of the training set and build a decision tree model fb\nusing each sample Sbas the training set. To sample Sbfor someb, we do the sampling with\nreplacement . This means that we start with an empty set, and then pick at random an\nexample from the training set and put its exact copy to Sbby keeping the original example\nin the original training set. We keep picking examples at random until the |Sb|=N.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 9", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2780, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "db0c9cee-c83e-4a0e-9e1d-cce392154337": {"__data__": {"id_": "db0c9cee-c83e-4a0e-9e1d-cce392154337", "embedding": null, "metadata": {"page_label": "84", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "921bca7a-668b-409a-ad25-e3f04b3cc4ec", "node_type": "4", "metadata": {"page_label": "84", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "21b83ee0834eb47bf551e064f509919f1423caeccad0225db57f28f23492fcd7", "class_name": "RelatedNodeInfo"}}, "text": "After training, we have Bdecision trees. The prediction for a new example xis obtained as\nthe average of Bpredictions:\ny\u2190\u02c6f(x)def=1\nBB\u2211\nb=1fb(x),\nin the case of regression, or by taking the majority vote in the case of classi\ufb01cation.\nRandom forest is di\ufb00erent from the vanilla bagging in just one way. It uses a modi\ufb01ed tree\nlearning algorithm that inspects, at each split in the learning process, a random subset of\nthe features. The reason for doing this is to avoid the correlation of the trees: if one or a\nfew features are very strong predictors for the target, these features will be selected to split\nexamples in many trees. This would result in many correlated trees in our \u201cforest.\u201d Correlated\npredictors cannot help in improving the accuracy of prediction. The main reason behind a\nbetter performance of model ensembling is that models that are good will likely agree on\nthe same prediction, while bad models will likely disagree on di\ufb00erent ones. Correlation will\nmake bad models more likely to agree, which will hamper the majority vote or the average.\nThe most important hyperparameters to tune are the number of trees, B, and the size of the\nrandom subset of the features to consider at each split.\nRandom forest is one of the most widely used ensemble learning algorithms. Why is it so\ne\ufb00ective? The reason is that by using multiple samples of the original dataset, we reduce\nthevariance of the \ufb01nal model. Remember that the low variance means low over\ufb01tting .\nOver\ufb01ttinghappenswhenourmodeltriestoexplainsmallvariationsinthedatasetbecauseour\ndataset is just a small sample of the population of all possible examples of the phenomenon we\ntry to model. If we were unlucky with how our training set was sampled, then it could contain\nsome undesirable (but unavoidable) artifacts: noise, outliers and over- or underrepresented\nexamples. By creating multiple random samples with replacement of our training set, we\nreduce the e\ufb00ect of these artifacts.\n7.5.3 Gradient Boosting\nAnother e\ufb00ective ensemble learning algorithm, based on the idea of boosting, is gradient\nboosting . Let\u2019s \ufb01rst look at gradient boosting for regression. To build a strong regressor,\nwe start with a constant model f=f0(just like we did in ID3):\nf=f0(x)def=1\nNN\u2211\ni=1yi.\nThen we modify labels of each example i= 1,...,Nin our training set like follows:\n\u02c6yi\u2190yi\u2212f(xi), (2)\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 10", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2417, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "17ab93d6-753c-401e-82fe-1e291fc890f9": {"__data__": {"id_": "17ab93d6-753c-401e-82fe-1e291fc890f9", "embedding": null, "metadata": {"page_label": "85", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "013a546c-781c-4734-b316-8f1b3271edb8", "node_type": "4", "metadata": {"page_label": "85", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "5904c3c49a3e66844720f2fabd8a77ffce56eafa7e85c6ffc903b5634c9b304f", "class_name": "RelatedNodeInfo"}}, "text": "where \u02c6yi, called the residual , is the new label for example xi.\nNow we use the modi\ufb01ed training set, with residuals instead of original labels, to build a new\ndecision tree model, f1. The boosting model is now de\ufb01ned as fdef=f0+\u03b1f1, where\u03b1is the\nlearning rate (a hyperparameter).\nThen we recompute the residuals using eq. 2 and replace the labels in the training data once\nagain, train the new decision tree model f2, rede\ufb01ne the boosting model as fdef=f0+\u03b1f1+\u03b1f2\nand the process continues until the prede\ufb01ned maximum Mof trees are combined.\nIntuitively, what\u2019s happening here? By computing the residuals, we \ufb01nd how well (or poorly)\nthe target of each training example is predicted by the current model f. We then train\nanother tree to \ufb01x the errors of the current model (this is why we use residuals instead of\nreal labels) and add this new tree to the existing model with some weight \u03b1. Therefore, each\nadditional tree added to the model partially \ufb01xes the errors made by the previous trees until\nthe maximum number M(another hyperparameter) of trees are combined.\nNow you should reasonably ask why the algorithm is called gradient boosting? In gradient\nboosting, we don\u2019t calculate any gradient contrary to what we did in Chapter 4 for linear\nregression. To see the similarity between gradient boosting and gradient descent remember\nwhy we calculated the gradient in linear regression: we did that to get an idea on where we\nshould move the values of our parameters so that the MSE cost function reaches its minimum.\nThe gradient showed the direction, but we didn\u2019t know how far we should go in this direction,\nso we used a small step \u03b1at each iteration and then reevaluated our direction. The same\nhappens in gradient boosting. However, instead of getting the gradient directly, we use its\nproxy in the form of residuals: they show us how the model has to be adjusted so that the\nerror (the residual) is reduced.\nThe three principal hyperparameters to tune in gradient boosting are the number of trees,\nthe learning rate, and the depth of trees \u2014 all three a\ufb00ect model accuracy. The depth of\ntrees also a\ufb00ects the speed of training and prediction: the shorter, the faster.\nIt can be shown that training on residuals optimizes the overall model ffor the mean squared\nerror criterion. You can see the di\ufb00erence with bagging here: boosting reduces the bias (or\nunder\ufb01tting) instead of the variance. As such, boosting can over\ufb01t. However, by tuning the\ndepth and the number of trees, over\ufb01tting can be largely avoided.\nGradient boosting for classi\ufb01cation is similar, but the steps are slightly di\ufb00erent. Let\u2019s\nconsider the binary case. Assume we have Mregression decision trees. Similarly to logistic\nregression, the prediction of the ensemble of decision trees is modeled using the sigmoid\nfunction:\nPr(y= 1|x,f)def=1\n1 +e\u2212f(x),\nwheref(x)def=\u2211M\nm=1fm(x)andfmis a regression tree.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 11", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2947, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4c8c2f8a-d6a9-46f1-8755-67c16356aa44": {"__data__": {"id_": "4c8c2f8a-d6a9-46f1-8755-67c16356aa44", "embedding": null, "metadata": {"page_label": "86", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "57b8399e-2bed-45ed-bc3e-57bb94ea2fc8", "node_type": "4", "metadata": {"page_label": "86", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "875dc91f516a9fecad1e994ac3d5a5f1335193777092bc2f0aacb5c54728679d", "class_name": "RelatedNodeInfo"}}, "text": "Again, like in logistic regression, we apply the maximum likelihood principle by trying to\n\ufb01nd such an fthat maximizes Lf=\u2211N\ni=1ln [Pr(yi= 1|xi,f)]. Again, to avoid numerical\nover\ufb02ow, we maximize the sum of log-likelihoods rather than the product of likelihoods.\nThe algorithm starts with the initial constant model f=f0=p\n1\u2212p, wherep=1\nN\u2211N\ni=1yi.\n(It can be shown that such initialization is optimal for the sigmoid function.) Then at each\niterationm, a new tree fmis added to the model. To \ufb01nd the best fm, \ufb01rst the partial\nderivativegiof the current model is calculated for each i= 1,...,N:\ngi=dLf\ndf,\nwherefis the ensemble classi\ufb01er model built at the previous iteration m\u22121. To calculate gi\nwe need to \ufb01nd the derivatives of ln [Pr(yi= 1|xi,f)]with respect to ffor alli. Notice that\nln [Pr(yi= 1|xi,f)]def= ln[\n1\n1+e\u2212f(xi)]\n. The derivative of the right-hand term in the previous\nequation with respect to fequals to1\nef(xi)+1.\nWe then transform our training set by replacing the original label yiwith the corresponding\npartial derivative gi, and build a new tree fmusing the transformed training set. Then we\n\ufb01nd the optimal update step \u03c1mas:\n\u03c1m\u2190arg max\n\u03c1Lf+\u03c1fm.\nAt the end of iteration m, we update the ensemble model fby adding the new tree fm:\nf\u2190f+\u03b1\u03c1mfm.\nWe iterate until m=M, then we stop and return the ensemble model f.\nGradient boosting is one of the most powerful machines learning algorithms. Not just because\nit creates very accurate models, but also because it is capable of handling huge datasets with\nmillions of examples and features. It usually outperforms random forest in accuracy but,\nbecause of its sequential nature, can be signi\ufb01cantly slower in training.\n7.6 Learning to Label Sequences\nSequence is one the most frequently observed types of structured data. We communicate\nusing sequences of words and sentences, we execute tasks in sequences, our genes, the music\nwe listen and videos we watch, our observations of a continuous process, such as a moving\ncar or the price of a stock are all sequential.\nSequence labeling is the problem of automatically assigning a label to each element of a\nsequence. A labeled sequential training example in sequence labeling is a pair of lists (X,Y),\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 12", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2277, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6e67c100-6106-4f4b-ae96-7c43c6a418f3": {"__data__": {"id_": "6e67c100-6106-4f4b-ae96-7c43c6a418f3", "embedding": null, "metadata": {"page_label": "87", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "97d240b7-4ed6-4279-98e0-2cc3afa86524", "node_type": "4", "metadata": {"page_label": "87", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "0240aa5765808cfdbae5a4e8e90cdf446a4a1ce8a4e652bb55c0f218a751a7b4", "class_name": "RelatedNodeInfo"}}, "text": "where Xis a list of feature vectors, one per time step, Yis a list of the same length of labels.\nFor example, Xcould represent words in a sentence such as [\u201cbig\u201d, \u201cbeautiful\u201d, \u201ccar\u201d], and\nYwould be the list of the corresponding parts of speech, such as [\u201cadjective\u201d, \u201cadjective\u201d,\n\u201cnoun\u201d]). More formally, in an example i,Xi= [x1\ni,x2\ni,...,xsize i\ni], wheresizeiis the length\nof the sequence of the example i,Yi= [y1\ni,y2\ni,...,ysize i\ni]andyi\u2208{1,2,...,C}.\nYou have already seen that an RNN can be used to label a sequence. At each time step t, it\nreads an input feature vector x(t)\ni, and the last recurrent layer outputs a label y(t)\nlast(in the\ncase of binary labeling) or y(t)\nlast(in the case of multiclass or multilabel labeling).\nHowever, RNN is not the only possible model for sequence labeling. The model called\nConditional Random Fields (CRF) is a very e\ufb00ective alternative that often performs well\nin practice for the feature vectors that have many informative features. For example, imagine\nwe have the task of named entity extraction and we want to build a model that would\nlabel each word in the sentence such as \u201cI go to San Francisco\u201d with one of the following\nclasses:{location,name,company _name,other}. If our feature vectors (which represent\nwords) contain such binary features as \u201cwhether or not the word starts with a capital letter\u201d\nand \u201cwhether or not the word can be found in the list of locations,\u201d such features would be\nvery informative and help to classify the words SanandFrancisco aslocation.\nBuilding handcrafted features is known to be a labor-intensive\nprocess that requires a signi\ufb01cant level of domain expertise.\nCRF is an interesting model and can be seen as a generalization of\nlogistic regression to sequences. However, in practice, for sequence\nlabeling tasks, it has been outperformed by bidirectional deep\ngated RNN. CRFs are also signi\ufb01cantly slower in training which\nmakes them di\ufb03cult to apply to large training sets (with hundreds\nof thousands of examples). Additionally, a large training set is\nwhere a deep neural network thrives.\n7.7 Sequence-to-Sequence Learning\nSequence-to-sequence learning (often abbreviated as seq2seq learning) is a generalization\nof the sequence labeling problem. In seq2seq, XiandYican have di\ufb00erent lengths. seq2seq\nmodels have found application in machine translation (where, for example, the input is\nan English sentence, and the output is the corresponding French sentence), conversational\ninterfaces (where the input is a question typed by the user, and the output is the answer\nfrom the machine), text summarization, spelling correction, and many others.\nMany but not all seq2seq learning problems are currently best solved by neural networks.\nThe network architectures used in seq2seq all have two parts: an encoder and adecoder .\nIn seq2seq neural network learning, the encoder is a neural network that accepts sequential\ninput. It can be an RNN, but also a CNN or some other architecture. The role of the encoder\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 13", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3059, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "614cff0d-f071-40c4-9704-2cfbbbce903f": {"__data__": {"id_": "614cff0d-f071-40c4-9704-2cfbbbce903f", "embedding": null, "metadata": {"page_label": "88", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8d812b50-3a71-491e-b361-a30b1036a4a9", "node_type": "4", "metadata": {"page_label": "88", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "86636f19086c0d7fcbc20938b2e47233bbac3d534b0267040daaa44801e3754d", "class_name": "RelatedNodeInfo"}}, "text": "is to read the input and generate some sort of state (similar to the state in RNN) that can be\nseen as a numerical representation of the meaning of the input the machine can work with.\nThe meaning of some entity, whether it be an image, a text or a video, is usually a vector\nor a matrix that contains real numbers. This vector (or matrix) is called in the machine\nlearning jargon the embedding of the input.\nThe decoder is another neural network that takes an embedding as input and is capable of\ngenerating a sequence of outputs. As you could have already guessed, that embedding comes\nfrom the encoder. To produce a sequence of outputs, the decoder takes a start of sequence\ninput feature vector x(0)(typically all zeroes), produces the \ufb01rst output y(1), updates its\nstate by combining the embedding and the input x(0), and then uses the output y(1)as\nits next input x(1). For simplicity, the dimensionality of y(t)can be the same as that of\nx(t); however, it is not strictly necessary. As we saw in Chapter 6, each layer of an RNN\ncan produce many simultaneous outputs: one can be used to generate the label y(t), while\nanother one, of di\ufb00erent dimensionality, can be used as the x(t).\nEncoder Decoder\nThe weather is fine <\u00a0start\u00a0>Il fait beau\nt\u00a0=1 2 3 4 1 2 3\nFigure 4: A traditional seq2seq architecture. The embedding, usually given by the state of\nthe last layer of the encoder, is passed from the blue to the purple subnetwork.\nBoth encoder and decoder are trained simultaneously using the training data. The errors at\nthe decoder output are propagated to the encoder via backpropagation.\nA traditional seq2seq architecture is illustrated in Figure 4. More accurate predictions can\nbe obtained using an architecture with attention . Attention mechanism is implemented by\nan additional set of parameters that combine some information from the encoder (in RNNs,\nthis information is the list of state vectors of the last recurrent layer from all encoder time\nsteps) and the current state of the decoder to generate the label. That allows for even better\nretention of long-term dependencies than provided by gated units and bidirectional RNN.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 14", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2211, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3cab4973-9e01-40af-ba9e-882bdd6ed504": {"__data__": {"id_": "3cab4973-9e01-40af-ba9e-882bdd6ed504", "embedding": null, "metadata": {"page_label": "89", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e2a67bef-a949-46fc-ac65-7bdcc16566d3", "node_type": "4", "metadata": {"page_label": "89", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "87c6c0608d96ca99545e6e9b7efb45e559972eaa0182ec884c2e5b9ac1033cc8", "class_name": "RelatedNodeInfo"}}, "text": "The weather is fine <\u00a0start\u00a0>Il fait beau\nt\u00a0=1 2 3 4 1 2 3AttentionFigure 5: A seq2seq architecture with attention.\nA seq2seq architecture with attention is illustrated in Figure 5.\nseq2seq learning is a relatively new research domain. Novel network\narchitectures are regularly discovered and published. Training such\narchitectures can be challenging as the number of hyperparameters\nto tune and other architectural decisions can be overwhelming.\nConsult the book\u2019s wiki for the state of the art material, tutorials\nand code samples.\n7.8 Active Learning\nActive learning is an interesting supervised learning paradigm.\nIt is usually applied when obtaining labeled examples is costly. That is often the case in the\nmedical or \ufb01nancial domains, where the opinion of an expert may be required to annotate\npatients\u2019 or customers\u2019 data. The idea is to start learning with relatively few labeled examples,\nand a large number of unlabeled ones, and then label only those examples that contribute\nthe most to the model quality.\nThere are multiple strategies of active learning. Here, we discuss only the following two:\n1) data density and uncertainty based, and\n2) support vector-based.\nThe former strategy applies the current model f, trained using the existing labeled examples,\nto each of the remaining unlabelled examples (or, to save the computing time, to some\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1421, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "903d10a6-d5dd-408e-9833-8f04cb28c663": {"__data__": {"id_": "903d10a6-d5dd-408e-9833-8f04cb28c663", "embedding": null, "metadata": {"page_label": "90", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "02083bcc-7bb1-43f8-bfb1-f61efafa959a", "node_type": "4", "metadata": {"page_label": "90", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "c8edd7d4f481bbd48470a0126b552d8496e662c4aee0ac86739a94cb97e7a606", "class_name": "RelatedNodeInfo"}}, "text": "random sample of them). For each unlabeled example x, the following importance score is\ncomputed: density (x)\u00b7uncertainty f(x). Density re\ufb02ects how many examples surround xin\nits close neighborhood, while uncertainty f(x)re\ufb02ects how uncertain the prediction of the\nmodelfis for x. In binary classi\ufb01cation with sigmoid, the closer the prediction score is to\n0.5, the more uncertain is the prediction. In SVM, the closer the example is to the decision\nboundary, the most uncertain is the prediction.\nIn multiclass classi\ufb01cation, entropycan be used as a typical measure of uncertainty:\nHf(x) =\u2212C\u2211\nc=1Pr(y(c);f(x)) ln[\nPr(y(c);f(x))]\n,\nwhere Pr(y(c);f(x))is the probability score the model fassigns to class y(c)when classifying\nx. You can see that if for each y(c),f(y(c)) =1\nCthen the model is the most uncertain and\nthe entropy is at its maximum of 1; on the other hand, if for some y(c),f(y(c)) = 1, then the\nmodel is certain about the class y(c)and the entropy is at its minimum of 0.\nDensity for the example xcan be obtained by taking the average\nof the distance from xto each of its knearest neighbors (with k\nbeing a hyperparameter).\nOnce we know the importance score of each unlabeled example,\nwe pick the one with the highest importance score and ask the\nexpert to annotate it. Then we add the new annotated example\nto the training set, rebuild the model and continue the process\nuntil some stopping criterion is satis\ufb01ed. A stopping criterion can\nbe chosen in advance (the maximum number of requests to the\nexpert based on the available budget) or depend on how well our model performs according\nto some metric.\nThe support vector-based active learning strategy consists in building an SVM model using\nthe labeled data. We then ask our expert to annotate the unlabeled example that lies the\nclosest to the hyperplane that separates the two classes. The idea is that if the example lies\nclosest to the hyperplane, then it is the least certain and would contribute the most to the\nreduction of possible places where the true (the one we look for) hyperplane could lie.\nSome active learning strategies can incorporate the cost of asking an expert for a label.\nOthers learnto ask expert\u2019s opinion. The \u201cquery by committee\u201d strategy consists of training\nmultiple models using di\ufb00erent methods and then asking an expert to label example on which\nthose models disagree the most. Some strategies try to select examples to label so that the\nvariance or the bias of the model are reduced the most.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 16", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2558, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f1e70401-da95-483f-98d7-c56e4cd89f67": {"__data__": {"id_": "f1e70401-da95-483f-98d7-c56e4cd89f67", "embedding": null, "metadata": {"page_label": "91", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ccea3dca-d0a4-4c92-bef0-34cd17edca09", "node_type": "4", "metadata": {"page_label": "91", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "9190a2705677aceedb9758b34b76e50c792a7a5e53550dba752e1e347f0c8416", "class_name": "RelatedNodeInfo"}}, "text": "7.9 Semi-Supervised Learning\nInsemi-supervised learning (SSL) we also have labeled a small fraction of the dataset;\nmost of the remaining examples are unlabeled. Our goal is to leverage a large number of\nunlabeled examples to improve the model performance without asking for additional labeled\nexamples.\nHistorically, there were multiple attempts at solving this problem. None of them could be\ncalled universally acclaimed and frequently used in practice. For example, one frequently\ncited SSL method is called self-learning . In self-learning, we use a learning algorithm to\nbuild the initial model using the labeled examples. Then we apply the model to all unlabeled\nexamples and label them using the model. If the con\ufb01dence score of prediction for some\nunlabeled example xis higher than some threshold (chosen experimentally), then we add this\nlabeled example to our training set, retrain the model and continue like this until a stopping\ncriterion is satis\ufb01ed. We could stop, for example, if the accuracy of the model has not been\nimproved during the last miterations.\nThe above method can bring some improvement to the model compared to just using\nthe initially labeled dataset, but the increase in performance usually is not impressive.\nFurthermore, in practice, the quality of the model could even decrease. That depends on the\nproperties of the statistical distribution the data was drawn from, which is usually unknown.\nOn the other hand, the recent advancements in neural network learning brought some\nimpressive results. For example, it was shown that for some datasets, such as MNIST (a\nfrequent testbench in computer vision that consists of labeled images of handwritten digits\nfrom 0 to 9) the model trained in a semi-supervised way has an almost perfect performance\nwith just 10 labeled examples per class (100 labeled examples overall). For comparison,\nMNIST contains 70,000 labeled examples (60,000 for training and 10,000 for test). The\nneural network architecture that attained such a remarkable performance is called a ladder\nnetwork . To understand ladder networks you have to understand what an autoencoder is.\nAn autoencoder is a feed-forward neural network with an encoder-decoder architecture. It\nis trained to reconstruct its input. So the training example is a pair (x,x). We want the\noutput \u02c6xof the model f(x)to be as similar to the input xas possible.\nAn important detail here is that an autoencoder\u2019s network looks like an hourglass with a\nbottleneck layer in the middle that contains the embedding of the D-dimensional input\nvector; the embedding layer usually has much fewer units than D. The goal of the decoder is\nto reconstruct the input feature vector from this embedding. Theoretically, it is su\ufb03cient\nto have 10units in the bottleneck layer to successfully encode MNIST images. In a typical\nautoencoder schematically depicted in Figure 6, the cost function is usually either the mean\nsquared error (when features can be any number) or the binary cross-entropy (when features\nare binary and the units of the last layer of the decoder have the sigmoid activation function).\nIf the cost is the mean squared error, then it is given by:\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 17", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3234, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "98749495-f5fc-465f-a4e8-b2d791d58b3a": {"__data__": {"id_": "98749495-f5fc-465f-a4e8-b2d791d58b3a", "embedding": null, "metadata": {"page_label": "92", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "43ba82ed-d9c5-48e5-a435-11251cd31cf0", "node_type": "4", "metadata": {"page_label": "92", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "9824d7ddf0b4b4343d287be422fc00dcf54aea9e91263e81738ef27b570e55a9", "class_name": "RelatedNodeInfo"}}, "text": "1\nNN\u2211\ni=1\u2225xi\u2212f(xi)\u22252,\nwhere\u2225xi\u2212f(xi)\u2225is the Euclidean distance between two vectors.\nFigure 6: Autoencoder.\nAdenoising autoencoder corrupts the left-hand side xin the training example (x,x)by\nadding some random perturbation to the features. If our examples are grayscale images with\npixels represented as values between 0and1, usually a Gaussian noise is added to each\nfeature. For each feature jof the input feature vector xthe noise value n(j)is sampled from\ntheGaussian distribution :\nn(j)\u223cN(\u00b5,\u03c32),\nwhere the notation \u223cmeans \u201csampled from,\u201d and N(\u00b5,\u03c32)denotes the Gaussian distribution\nwith mean \u00b5and standard deviation \u03c3whose pdf is given by:\nf\u03b8(z) =1\n\u03c3\u221a\n2\u03c0exp(\n\u2212(z\u2212\u00b5)2\n2\u03c32)\n.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 18", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 743, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8722d910-24e8-454b-b1f5-29edf3e09175": {"__data__": {"id_": "8722d910-24e8-454b-b1f5-29edf3e09175", "embedding": null, "metadata": {"page_label": "93", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dc51e147-8f17-430e-add5-69dcb9ecc197", "node_type": "4", "metadata": {"page_label": "93", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "0904a81c388236c50ab29eca2f768ed9d44a6e28d9752a6c9fce20cc422421ce", "class_name": "RelatedNodeInfo"}}, "text": "In the above equation, \u03c0is the constant 3.14159...and\u03b8def=[\u00b5,\u03c3]is a hyperparameter. The\nnew, corrupted value of the feature x(j)is given by x(j)+n(j).\nAladder network is a denoising autoencoder with an upgrade. The encoder and the decoder\nhave the same number of layers. The bottleneck layer is used directly to predict the label\n(using the softmax activation function). The network has several cost functions. For each\nlayerlof the encoder and the corresponding layer lof the decoder, one cost Cl\ndpenalizes\nthe di\ufb00erence between the outputs of the two layers (using the squared Euclidean distance).\nWhen a labeled example is used during training, another cost function, Cc, penalizes the error\nin prediction of the label (the negative log-likelihood cost function is used). The combined\ncost function, Cc+\u2211L\nl=1\u03bblCl\nd(averaged over all examples in the batch), is optimized by the\nminibatch stochastic gradient descent with backpropagation. The hyperparameters \u03bblfor\neach layerldetermine the tradeo\ufb00 between the classi\ufb01cation and encoding-decoding cost.\nIn the ladder network, not just the input is corrupted with the noise, but also the output of\neach encoder layer (during training). When we apply the trained model to the new input x\nto predict its label, we do not corrupt the input.\nOther semi-supervised learning techniques, not related to training\nneural networks, exist. One of them implies building the model\nusing the labeled data and then cluster the unlabeled and labeled\nexamples together using any clustering technique (we consider\nsome of them in Chapter 9). For each new example, we then\noutput as a prediction the majority label in the cluster it belongs\nto.\nAnother technique, called S3VM, is based on using SVM. We build\none SVM model for each possible labeling of unlabeled examples\nand then we pick the model with the largest margin. The paper on S3VM describes an\napproach that allows solving this problem without actually enumerating all possible labelings.\n7.10 One-Shot Learning\nThischapterwouldbeincompletewithout mentioning twoother importantsupervised learning\nparadigms. One of them is one-shot learning . In one-shot learning, typically applied in\nface recognition, we want to build a model that can recognize that two photos of the same\nperson represent that same person. If we present to the model two photos of two di\ufb00erent\npeople, we expect the model to recognize that the two people are di\ufb00erent.\nTo solve such a problem, we could go a traditional way and build a binary classi\ufb01er that\ntakes two images as input and predict either true (when the two pictures represent the same\nperson) or false (when the two pictures belong to di\ufb00erent people). However, in practice,\nthis would result in a neural network twice as big as a typical neural network, because each\nof the two pictures needs its own embedding subnetwork. Training such a network would\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2946, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c2f8ef3a-4caa-4109-9229-a1274e0e8f6a": {"__data__": {"id_": "c2f8ef3a-4caa-4109-9229-a1274e0e8f6a", "embedding": null, "metadata": {"page_label": "94", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bffdf3e8-ab45-427b-8751-0001c80ad499", "node_type": "4", "metadata": {"page_label": "94", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "86959f83ea85639966c000d9355c48fbcf1b6c3b251ca5fe774fffd173925272", "class_name": "RelatedNodeInfo"}}, "text": "be challenging not only because of its size but also because the positive examples would be\nmuch harder to obtain than negative ones. So the problem is highly imbalanced.\nOne way to e\ufb00ectively solve the problem is to train a siamese neural network (SNN). An\nSNN can be implemented as any kind of neural network, a CNN, an RNN, or an MLP. The\nnetwork only takes one image as input at a time; so the size of the network is not doubled.\nTo obtain a binary classi\ufb01er \u201csame_person\u201d/\u201cnot_same\u201d out of a network that only takes\none picture as input, we train the networks in a special way.\nTo train an SNN, we use the triplet loss function. For example, let us have three images of\na face: image A(for anchor), image P(for positive) and image N(for negative). AandP\nare two di\ufb00erent pictures of the same person; Nis a picture of another person. Each training\nexampleiis now a triplet (Ai,Pi,Ni).\nLet\u2019s say we have a neural network model fthat can take a picture of a face as input and\noutput an embedding of this picture. The triplet loss for example iis de\ufb01ned as,\nmax(\u2225f(Ai)\u2212f(Pi)\u22252\u2212\u2225f(Ai)\u2212f(Ni)\u22252+\u03b1,0). (3)\nThe cost function is de\ufb01ned as the average triplet loss:\n1\nNN\u2211\ni=1max(\u2225f(Ai)\u2212f(Pi)\u22252\u2212\u2225f(Ai)\u2212f(Ni)\u22252+\u03b1,0),\nwhere\u03b1is a positive hyperparameter. Intuitively, \u2225f(A)\u2212f(P)\u22252is low when our neural\nnetwork outputs similar embedding vectors for AandP;\u2225f(Ai)\u2212f(Ni)\u22252is high when the\nembedding for pictures of two di\ufb00erent people are di\ufb00erent. If our model works the way\nwe want, then the term m=\u2225f(Ai)\u2212f(Pi)\u22252\u2212\u2225f(Ai)\u2212f(Ni)\u22252will always be negative,\nbecause we subtract a high value from a small value. By setting \u03b1higher, we force the term\nmto be even smaller, to make sure that the model learned to recognize the two same faces\nand two di\ufb00erent faces with a high margin. If mis not small enough, then because of \u03b1the\ncost will be positive, and the model parameters will be adjusted in backpropagation.\nRather than randomly choosing an image for N, a better way to create triplets for training is\nto use the current model after several epochs of learning and \ufb01nd candidates for Nthat are\nsimilar toAandPaccording to that model. Using random examples as Nwould signi\ufb01cantly\nslow down the training because the neural network will easily see the di\ufb00erence between\npictures of two random people, so the average triplet loss will be low most of the time and\nthe parameters will not be updated fast enough.\nTo build an SNN, we \ufb01rst decide on the architecture of our neural network. For example,\nCNN is a typical choice if our inputs are images. Given an example, to calculate the average\ntriplet loss, we apply, consecutively, the model to A, then toP, then toN, and then we\ncompute the loss for that example using eq. 3. We repeat that for all triplets in the batch and\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 20", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2821, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "920d3e7f-6893-4d67-b475-292e6cd2aa89": {"__data__": {"id_": "920d3e7f-6893-4d67-b475-292e6cd2aa89", "embedding": null, "metadata": {"page_label": "95", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "836466aa-7804-4b6c-8c8f-9ace9f971975", "node_type": "4", "metadata": {"page_label": "95", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "7620a8f10c163250834ff0efa66702eb45020bd3f6fe5e3fa9e7348eec20759e", "class_name": "RelatedNodeInfo"}}, "text": "then compute the cost; gradient descent with backpropagation propagates the cost through\nthe network to update its parameters.\nIt\u2019s a common misconception that for one-shot learning we need only one example of each\nentity for training. In practice, we need more than one example of each person for the\nperson identi\ufb01cation model to be accurate. It\u2019s called one-shot because of the most frequent\napplication of such a model: face-based authentication. For example, such a model could be\nused to unlock your phone. If your model is good, then you only need to have one picture of\nyou on your phone and it will recognize you, and also it will recognize that someone else is\nnot you. When we have the model, to decide whether two pictures Aand \u02c6Abelong to the\nsame person, we check if \u2225f(A)\u2212f(\u02c6A)\u22252is less than \u03c4, a hyperparameter.\n7.11 Zero-Shot Learning\nI \ufb01nish this chapter with zero-shot learning . It is a relatively new research area, so there\nare no algorithms that proved to have a signi\ufb01cant practical utility yet. Therefore, I only\noutline here the basic idea and leave the details of various algorithms for further reading.\nIn zero-shot learning (ZSL) we want to train a model to assign labels to objects. The most\nfrequent application is to learn to assign labels to images.\nHowever, contrary to standard classi\ufb01cation, we want the model to be able to predict labels\nthat we didn\u2019t have in the training data. How is that possible?\nThe trick is to use embeddings not just to represent the input xbut also to represent the\noutputy. Imagine that we have a model that for any word in English can generate an\nembedding vector with the following property: if a word yihas a similar meaning to the\nwordyk, then the embedding vectors for these two words will be similar. For example, if yiis\nParisandykisRome, then they will have embeddings that are similar; on the other hand, if\nykispotato, then the embeddings of yiandykwill be dissimilar. Such embedding vectors are\ncalledword embeddings , and they are usually compared using cosine similarity metrics1.\nWord embeddings have such a property that each dimension of the embedding represents a\nspeci\ufb01c feature of the meaning of the word. For example, if our word embedding has four\ndimensions (usually they are much wider, between 50 and 300 dimensions), then these four\ndimensions could represent such features of the meaning as animalness ,abstractness ,sourness,\nandyellowness (yes, sounds funny, but it\u2019s just an example). So the word beewould have an\nembedding like this [1,0,0,1], the word yellowlike this [0,1,0,1], the word unicornlike this\n[1,1,0,0]. The values for each embedding are obtained using a speci\ufb01c training procedure\napplied to a vast text corpus.\nNow, in our classi\ufb01cation problem, we can replace the label yifor each example iin our\ntraining set with its word embedding and train a multi-label model that predicts word\nembeddings. To get the label for a new example x, we apply our model ftox, get the\n1I will show in Chapter 10 how to learn words embeddings from data.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 21", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3107, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "882b111b-2fb7-4160-9ace-1add16437a1d": {"__data__": {"id_": "882b111b-2fb7-4160-9ace-1add16437a1d", "embedding": null, "metadata": {"page_label": "96", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "704c6663-eadf-44ce-a249-f8a9748bf8c7", "node_type": "4", "metadata": {"page_label": "96", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "b4b192ddae6d04c75027281c56c92afac53f07dddc8ba3f07af600b0711e2e56", "class_name": "RelatedNodeInfo"}}, "text": "embedding \u02c6yand then search among all English words those whose embeddings are the most\nsimilar to \u02c6yusing cosine similarity.\nWhy does that work? Take a zebra for example. It is white, it\nis a mammal, and it has stripes. Take a clown\ufb01sh: it is orange,\nnot a mammal, and has stripes. Now take a tiger: it is orange,\nit has stripes, and it is a mammal. If these three features are\npresent in word embeddings, the CNN would learn to detect these\nsame features in pictures. Even if the label tigerwas absent in the\ntrainingdata, butotherobjectsincludingzebrasandclown\ufb01shwere,\nthen the CNN will most likely learn the notion of mammalness ,\norangeness , and stripeness to predict labels of those objects. Once\nwe present the picture of a tiger to the model, those features will be correctly identi\ufb01ed from\nthe image and most likely the closest word embedding from our English dictionary to the\npredicted embedding will be that of tiger.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 22", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 994, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3708e3bb-83f0-486c-be7f-ede014ac9f87": {"__data__": {"id_": "3708e3bb-83f0-486c-be7f-ede014ac9f87", "embedding": null, "metadata": {"page_label": "97", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "66e6121f-6154-4cab-ad5f-179959b99859", "node_type": "4", "metadata": {"page_label": "97", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "edcf253bcbc93df57e8add2e73081473bd460139bed565c613bd31180f9400e6", "class_name": "RelatedNodeInfo"}}, "text": "8 Advanced Practice\nThis chapter contains the description of techniques that you could \ufb01nd useful in your practice\nin some contexts. It\u2019s called \u201cAdvanced Practice\u201d not because the presented techniques are\nmore complex, but rather because they are applied in some very speci\ufb01c contexts. In many\npractical situations, you will most likely not need to resort to using these techniques, but\nsometimes they are very helpful.\n8.1 Handling Imbalanced Datasets\nOften in practice, examples of some class will be underrepresented in your training data.\nThis is the case, for example, when your classi\ufb01er has to distinguish between genuine and\nfraudulent e-commerce transactions: the examples of genuine transactions are much more\nfrequent. If you use SVM with soft margin, you can de\ufb01ne a cost for misclassi\ufb01ed examples.\nBecause noise is always present in the training data, there are high chances that many\nexamples of genuine transactions would end up on the wrong side of the decision boundary\nby contributing to the cost.\nx (2)\nx (1)\n(a)\nx (2)\nx (1) (b)\nFigure 1: An illustration of an imbalanced problem. (a) Both classes have the same weight;\n(b) examples of the minority class have a higher weight.\nThe SVM algorithm will try to move the hyperplane to avoid as much as possible misclassi\ufb01ed\nexamples. The \u201cfraudulent\u201d examples, which are in the minority, risk being misclassi\ufb01ed in\norder to classify more numerous examples of the majority class correctly. This situation is\nillustrated in Figure 1a. This problem is observed for most learning algorithms applied to\nimbalanced datasets.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 3", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1646, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ce9d2fef-bddc-456d-9877-79429b6e21c8": {"__data__": {"id_": "ce9d2fef-bddc-456d-9877-79429b6e21c8", "embedding": null, "metadata": {"page_label": "98", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "158b4fd4-84fa-46cb-ba88-cc6ce9af1377", "node_type": "4", "metadata": {"page_label": "98", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "cb2af18599ebcdf8217ac2769bfffa6573f674cbea24972bfb904371bd3bb2fe", "class_name": "RelatedNodeInfo"}}, "text": "If you set the cost of misclassi\ufb01cation of examples of the minority class higher, then the\nmodel will try harder to avoid misclassifying those examples, obviously for the cost of\nmisclassi\ufb01cation of some examples of the majority class, as illustrated in Figure 1b.\nSome SVM implementations allow you to provide weights for every class. The learning\nalgorithm takes this information into account when looking for the best hyperplane.\nIf a learning algorithm doesn\u2019t allow weighting classes, you can try the technique of over-\nsampling . It consists of increasing the importance of examples of some class by making\nmultiple copies of the examples of that class.\nAn opposite approach, undersampling , is to randomly remove from the training set some\nexamples of the majority class.\nYou might also try to create synthetic examples by randomly sampling feature values of\nseveral examples of the minority class and combining them to obtain a new example of that\nclass. There are two popular algorithms that oversample the minority class by creating\nsynthetic examples: the synthetic minority oversampling technique (SMOTE) and the\nadaptive synthetic sampling method (ADASYN).\nSMOTE and ADASYN work similarly in many ways. For a given example xiof the minority\nclass, they pick knearest neighbors of this example (let\u2019s denote this set of kexamplesSk)\nand then create a synthetic example xnewasxi+\u03bb(xzi\u2212xi), where xziis an example of the\nminority class chosen randomly from Sk. The interpolation hyperparameter \u03bbis a random\nnumber in the range [0,1].\nBoth SMOTE and ADASYN randomly pick all possible xiin the dataset. In ADASYN,\nthe number of synthetic examples generated for each xiis proportional to the number of\nexamples inSkwhich are not from the minority class. Therefore, more synthetic examples\nare generated in the area where the examples of the minority class are rare.\nSome algorithms are less sensitive to the problem of an imbalanced dataset. Decision trees,\nas well as random forest and gradient boosting, often perform well on imbalanced datasets.\n8.2 Combining Models\nEnsemble algorithms, like Random Forest, typically combine models of the same nature. They\nboost performance by combining hundreds of weak models. In practice, we can sometimes\nget an additional performance gain by combining strong models made with di\ufb00erent learning\nalgorithms. In this case, we usually use only two or three models.\nThree typical ways to combine models are 1) averaging, 2) majority vote and 3) stacking.\nAveraging works for regression as well as those classi\ufb01cation models that return classi\ufb01cation\nscores. You simply apply all your models, let\u2019s call them base models , to the input xand\nthen average the predictions. To see if the averaged model works better than each individual\nalgorithm, you test it on the validation set using a metric of your choice.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 4", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2916, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f8a06d2d-0eaa-4bd5-a0e6-9bdcfd3adc4d": {"__data__": {"id_": "f8a06d2d-0eaa-4bd5-a0e6-9bdcfd3adc4d", "embedding": null, "metadata": {"page_label": "99", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1529b363-1ad3-4341-b83e-e21ba1ab453b", "node_type": "4", "metadata": {"page_label": "99", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "53d13099f32cb8e2379345148cc8b8805f7a4d02c5710452841cfa68dd5e01bb", "class_name": "RelatedNodeInfo"}}, "text": "Majority vote works for classi\ufb01cation models. You apply all your base models to the input\nxand then return the majority class among all predictions. In the case of a tie, you either\nrandomly pick one of the classes, or, you return an error message (if the fact of misclassifying\nwould incur a signi\ufb01cant cost).\nStacking consists of building a meta-model that takes the output of base models as input.\nLet\u2019s say you want to combine classi\ufb01ers f1andf2, both predicting the same set of classes. To\ncreate a training example (\u02c6xi,\u02c6yi)for the stacked model, set \u02c6xi= [f1(x),f2(x)]and\u02c6yi=yi.\nIf some of your base models return not just a class, but also a score for each class, you can\nuse these values as features too.\nTo train the stacked model, it is recommended to use examples from the training set and\ntune the hyperparameters of the stacked model using cross-validation.\nObviously, you have to make sure that your stacked model performs better on the validation\nset than each of the base models you stacked.\nThe reason that combining multiple models can bring better performance is the observation\nthat when several uncorrelated strong models agree they are more likely to agree on the\ncorrect outcome. The keyword here is \u201cuncorrelated.\u201d Ideally, base models should be obtained\nusing di\ufb00erent features or using algorithms of a di\ufb00erent nature \u2014 for example, SVMs and\nRandom Forest. Combining di\ufb00erent versions of decision tree learning algorithm, or several\nSVMs with di\ufb00erent hyperparameters may not result in a signi\ufb01cant performance boost.\n8.3 Training Neural Networks\nIn neural network training, one challenging aspect is to convert your data into the input the\nnetwork can work with. If your input is images, \ufb01rst of all, you have to resize all images so\nthat they have the same dimensions. After that, pixels are usually \ufb01rst standardized and\nthen normalized to the range [0,1].\nTexts have to be tokenized (that is split into pieces, such as words, punctuation marks, and\nother symbols). For CNN and RNN, each token is converted into a vector using the one-hot\nencoding, so the text becomes a list of one-hot vectors. Another, often a better way to\nrepresent tokens is by using word embeddings . For multilayer perceptron, to convert texts\nto vectors the bag of words approach may work well, especially for larger texts (larger than\nSMS messages and tweets).\nThe choice of speci\ufb01c neural network architecture is a di\ufb03cult one. For the same problem,\nlike seq2seq learning, there is a variety of architectures, and new ones are proposed almost\nevery year. I recommend researching state of the art solutions for your problem using Google\nScholar or Microsoft Academic search engines that allow searching for scienti\ufb01c publications\nusing keywords and time range. If you don\u2019t mind working with less modern architecture, I\nrecommend looking for implemented architectures on GitHub and \ufb01nding one that could be\napplied to your data with minor modi\ufb01cations.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 5", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3021, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c95d4283-c882-4d33-9969-0d1954aa35fe": {"__data__": {"id_": "c95d4283-c882-4d33-9969-0d1954aa35fe", "embedding": null, "metadata": {"page_label": "100", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b3340588-3f20-4dcb-80de-7ef76144940d", "node_type": "4", "metadata": {"page_label": "100", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "c63bc6312b87209af0df7d26e42306f3d6bb71f9cec34ab971483bc99e25610b", "class_name": "RelatedNodeInfo"}}, "text": "In practice, the advantage of modern architecture over an older one becomes less signi\ufb01cant\nas you preprocess, clean and normalize your data, and create a larger training set. Modern\nneural network architectures are a result of the collaboration of scientists from several labs\nand companies; such models could be very complex to implement on your own and usually\nrequire much computational power to train. Time spent trying to replicate results from a\nrecent scienti\ufb01c paper may not be worth it. This time could better be spent on building the\nsolution around a less modern but stable model and getting more training data.\nOnce you decided on the architecture of your network, you have to decide on the number of\nlayers, their type, and size. It is recommended to start with one or two layers, train a model\nand see if it \ufb01ts the training data well (has a low bias). If not, gradually increase the size of\neach layer and the number of layers until the model perfectly \ufb01ts the training data. Once this\nis the case, if the model doesn\u2019t perform well on the validation data (has a high variance), you\nshould add regularization to your model. If, after adding regularization, the model doesn\u2019t\n\ufb01t the training data anymore, slightly increase the size of the network. Continue iteratively\nuntil the model \ufb01ts both training and validation data well enough according to your metric.\n8.4 Advanced Regularization\nIn neural networks, besides L1 and L2 regularization, you can use neural network speci\ufb01c\nregularizers: dropout ,early stopping , andbatch normalization . Thelatteristechnically\nnot a regularization technique, but it often has a regularization e\ufb00ect on the model.\nThe concept of dropout is very simple. Each time you run a training example through the\nnetwork, you temporarily exclude at random some units from the computation. The higher\nthe percentage of units excluded the higher the regularization e\ufb00ect. Neural network libraries\nallow you to add a dropout layer between two successive layers, or you can specify the dropout\nparameter for the layer. The dropout parameter is in the range [0,1]and it has to be found\nexperimentally by tuning it on the validation data.\nEarly stopping is the way to train a neural network by saving the preliminary model after\nevery epoch and assessing the performance of the preliminary model on the validation set. As\nyou remember from the section about gradient descent in Chapter 4, as the number of epochs\nincreases, the cost decreases. The decreased cost means that the model \ufb01ts the training data\nwell. However, at some point, after some epoch e, the model can start over\ufb01tting: the cost\nkeeps decreasing, but the performance of the model on the validation data deteriorates. If\nyou keep, in a \ufb01le, the version of the model after each epoch, you can stop the training once\nyou start observing a decreased performance on the validation set. Alternatively, you can\nkeep running the training process for a \ufb01xed number of epochs and then, in the end, you\npick the best model. Models saved after each epoch are called checkpoints . Some machine\nlearning practitioners rely on this technique very often; others try to properly regularize the\nmodel to avoid such an undesirable behavior.\nBatch normalization (which rather has to be called batch standardization) is a technique\nthat consists of standardizing the outputs of each layer before the units of the subsequent\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 6", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3470, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cdba7ae4-5656-42f8-b574-c0645f15f604": {"__data__": {"id_": "cdba7ae4-5656-42f8-b574-c0645f15f604", "embedding": null, "metadata": {"page_label": "101", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8ee95c9a-9392-4039-b857-011841488536", "node_type": "4", "metadata": {"page_label": "101", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "ff16160cbb7ce1832b28a34ebfca112bf77288f0585c5a6779d4bc4daa3996f9", "class_name": "RelatedNodeInfo"}}, "text": "layer receive them as input. In practice, batch normalization results in faster and more stable\ntraining, as well as some regularization e\ufb00ect. So it\u2019s always a good idea to try to use batch\nnormalization. In neural network libraries, you can often insert a batch normalization layer\nbetween two layers.\nAnother regularization technique that can be applied not just to neural networks, but to\nvirtually any learning algorithm, is called data augmentation . This technique is often\nused to regularize models that work with images. Once you have your original labeled\ntraining set, you can create a synthetic example from an original example by applying various\ntransformations to the original image: zooming it slightly, rotating, \ufb02ipping, darkening, and\nso on. You keep the original label in these synthetic examples. In practice, this often results\nin increased performance of the model.\n8.5 Handling Multiple Inputs\nOften in practice, you will work with multimodal data. For example, your input could be an\nimage and text and the binary output could indicate whether the text describes this image.\nIt\u2019s hard to adapt shallow learning algorithms to work with multimodal data. However, it\u2019s\nnot impossible. You could train one shallow model on the image and another one on the text.\nThen you can use a model combination technique we discussed above.\nIf you cannot divide your problem into two independent subproblems, you can try to vectorize\neach input (by applying the corresponding feature engineering method) and then simply\nconcatenate two feature vectors together to form one wider feature vector. For example,\nif your image has features [i(1),i(2),i(3)]and your text has features [t(1),t(2),t(3),t(4)]your\nconcatenated feature vector will be [i(1),i(2),i(3),t(1),t(2),t(3),t(4)].\nWith neural networks, you have more \ufb02exibility. You can build two subnetworks, one for\neach type of input. For example, a CNN subnetwork would read the image while an RNN\nsubnetwork would read the text. Both subnetworks have as their last layer an embedding:\nCNN has an embedding of the image, while RNN has an embedding of the text. You can now\nconcatenate two embeddings and then add a classi\ufb01cation layer, such as softmax or sigmoid,\non top of the concatenated embeddings. Neural network libraries provide simple to use tools\nthat allow concatenating or averaging layers from several subnetworks.\n8.6 Handling Multiple Outputs\nIn some problems, you would like to predict multiple outputs for one input. We considered\nmulti-label classi\ufb01cation in the previous chapter. Some problems with multiple outputs can\nbe e\ufb00ectively converted into a multi-label classi\ufb01cation problem. Especially those that have\nlabels of the same nature (like tags) or fake labels can be created as a full enumeration of\ncombinations of original labels.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2879, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5638b917-863e-4c6f-83e6-c7a962868021": {"__data__": {"id_": "5638b917-863e-4c6f-83e6-c7a962868021", "embedding": null, "metadata": {"page_label": "102", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f4ee1c8a-3e53-44a5-af15-43c0e0c155df", "node_type": "4", "metadata": {"page_label": "102", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "acbe0348226df096e006ce7450f1ea3338bb34aa2ad7e16e86d0c554dff582ca", "class_name": "RelatedNodeInfo"}}, "text": "However, in some cases the outputs are multimodal, and their combinations cannot be\ne\ufb00ectively enumerated. Consider the following example: you want to build a model that\ndetects an object on an image and returns its coordinates. In addition, the model has to\nreturn a tag describing the object, such as \u201cperson,\u201d \u201ccat,\u201d or \u201chamster.\u201d Your training\nexample will be a feature vector that represents an image. The label will be represented as a\nvector of coordinates of the object and another vector with a one-hot encoded tag.\nTo handle a situation like that, you can create one subnetwork that would work as an\nencoder. It will read the input image using, for example, one or several convolution layers.\nThe encoder\u2019s last layer would be the embedding of the image. Then you add two other\nsubnetworks on top of the embedding layer: one that takes the embedding vector as input\nand predicts the coordinates of an object. This \ufb01rst subnetwork can have a ReLU as the\nlast layer, which is a good choice for predicting positive real numbers, such as coordinates;\nthis subnetwork could use the mean squared error cost C1. The second subnetwork will take\nthe same embedding vector as input and predict the probabilities for each tag. This second\nsubnetwork can have a softmax as the last layer, which is appropriate for the probabilistic\noutput, and use the averaged negative log-likelihood cost C2(also called cross-entropy cost).\nObviously, you are interested in both accurately predicted coordinates and the tags. However,\nit is impossible to optimize the two cost functions at the same time. By trying to optimize one,\nyou risk hurting the second one and the other way around. What you can do is add another\nhyperparameter \u03b3in the range (0,1)and de\ufb01ne the combined cost function as \u03b3C1+(1\u2212\u03b3)C2.\nThen you tune the value for \u03b3on the validation data just like any other hyperparameter.\n8.7 Transfer Learning\nTransfer learning is probably where neural networks have a unique advantage over the\nshallow models. In transfer learning, you pick an existing model trained on some dataset,\nand you adapt this model to predict examples from another dataset, di\ufb00erent from the one\nthe model was built on. This second dataset is not like holdout sets you use for validation\nand test. It may represent some other phenomenon, or, as machine learning scientists say, it\nmay come from another statistical distribution.\nFor example, imagine you have trained your model to recognize (and label) wild animals on a\nbig labeled dataset. After some time, you have another problem to solve: you need to build a\nmodel that would recognize domestic animals. With shallow learning algorithms, you do not\nhave many options: you have to build another big labeled dataset, now for domestic animals.\nWith neural networks, the situation is much more favorable. Transfer learning in neural\nnetworks works like this.\n1. You build a deep model on the original big dataset (wild animals).\n2.You compile a much smaller labeled dataset for your second model (domestic animals).\n3.You remove the last one or several layers from the \ufb01rst model. Usually, these are layers\nresponsible for the classi\ufb01cation or regression; they usually follow the embedding layer.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 8", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3279, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b52fe620-5fcb-417a-8c21-22d6042a0e65": {"__data__": {"id_": "b52fe620-5fcb-417a-8c21-22d6042a0e65", "embedding": null, "metadata": {"page_label": "103", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d3e9c11-2d62-4873-be37-dd8c745e2e4e", "node_type": "4", "metadata": {"page_label": "103", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "eb4d73d4bcfb863b3a0ce0f801e6702fb276fe295a3645c4d909019631aded16", "class_name": "RelatedNodeInfo"}}, "text": "4. You replace the removed layers with new layers adapted for your new problem.\n5. You \u201cfreeze\u201d the parameters of the layers remaining from the \ufb01rst model.\n6.You use your smaller labeled dataset and gradient descent to train the parameters of\nonly the new layers.\nUsually, there is an abundance of deep models for visual problems available online. You can\n\ufb01nd one that has high chances to be of use for your problem, download that model, remove\nseveral last layers (the quantity of layers to remove is a hyperparameter), put your own\nprediction layers and train your model.\nEven if you don\u2019t have an existing model, transfer learning can still help you in situations when\nyour problem requires a labeled dataset very costly to obtain, but you can get another dataset\nfor which labels are more readily available. Let\u2019s say you build a document classi\ufb01cation\nmodel. You got the taxonomy of labels from your employer, and it contains a thousand\ncategories. In this case, you would need to pay someone to a) read, understand and memorize\nthe di\ufb00erences between categories and b) read up to a million documents and annotate them.\nTo save on labeling so many examples, you could consider using Wikipedia pages as the dataset\nto build your \ufb01rst model. The labels for a Wikipedia page can be obtained automatically by\ntaking the category the Wikipedia page belongs to. Once your \ufb01rst model has learned to\npredict Wikipedia categories, you can \u201c\ufb01ne tune\u201d this model to predict the categories of your\nemployer\u2019s taxonomy. You will need much fewer annotated examples for your employer\u2019s\nproblem than you would need if you started solving your original problem from scratch.\n8.8 Algorithmic E\ufb03ciency\nNot all algorithms capable of solving a problem are practical. Some can be too slow. Some\nproblems can be solved by a fast algorithm, for others, no fast algorithms can exist.\nThe sub\ufb01eld of computer science called analysis of algorithms is concerned with determining\nand comparing the complexity of algorithms. The big O notation is used to classify\nalgorithms according to how their running time or space requirements grow as the input size\ngrows.\nFor example, let\u2019s say we have the problem of \ufb01nding the two most distant one-dimensional\nexamples in the set of examples Sof sizeN. One algorithm we could craft to solve this\nproblem would look like this (here and below, in Python):\n1def find_max_distance(S):\n2 result = None\n3 max_distance = 0\n4 for x1inS:\n5 for x2inS:\n6 ifabs(x1 - x2) >= max_distance:\n7 max_distance = abs(x1 - x2)\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 9", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2586, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cde656c5-9dba-41e3-a2d6-66db017044ad": {"__data__": {"id_": "cde656c5-9dba-41e3-a2d6-66db017044ad", "embedding": null, "metadata": {"page_label": "104", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1b14d0e2-118b-4c3c-86dc-d2bc8b126a26", "node_type": "4", "metadata": {"page_label": "104", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "fac5b23775dcc8d5ef33a10fe540803774563a1338600592b85d58d9c1c417e8", "class_name": "RelatedNodeInfo"}}, "text": "8 result = (x1, x2)\n9 return result\nIn the above algorithm, we loop over all values in S, and at every iteration of the \ufb01rst loop, we\nloop over all values in Sonce again. Therefore, the above algorithm makes N2comparisons of\nnumbers. If we take as a unit time the time the comparison ,absandassignment operations\ntake, then the time complexity (or, simply, complexity) of this algorithm is at most 5N2. (At\neach iteration, we have one comparison , two absand two assignment operations.) When the\ncomplexity of an algorithm is measured in the worst case, the big O notation is used. For\nthe above algorithm, using the big O notation, we write that the algorithm\u2019s complexity is\nO(N2); the constants, like 5, are ignored.\nFor the same problem, we can craft another algorithm like this:\n1def find_max_distance(S):\n2 result = None\n3 min_x = float(\"inf\")\n4 max_x = float(\"-inf\")\n5 for xinS:\n6 ifx < min_x:\n7 min_x = x\n8 elif x > max_x:\n9 max_x = x\n10 result = (max_x, min_x)\n11 return result\nIn the above algorithm, we loop over all values in Sonly once, so the algorithm\u2019s complexity\nisO(N). In this case, we say that the latter algorithm is more e\ufb03cient than the former.\nAn algorithm is called e\ufb03cient when its complexity is polynomial in the size of the input.\nTherefore both O(N)andO(N2)are e\ufb03cient because Nis a polynomial of degree 1andN2\nis a polynomial of degree 2. However, for very large inputs, an O(N2)algorithm can be slow.\nIn the big data era, scientists often look for O(logN)algorithms.\nFrom a practical standpoint, when you implement your algorithm, you should avoid using\nloops whenever possible . For example, you should use operations on matrices and vectors,\ninstead of loops. In Python, to compute wx, you should write,\n1import numpy\n2wx = numpy.dot(w,x)\nand not,\n1wx = 0\n2for iinrange(N):\n3 wx += w[i]*x[i]\nUse appropriate data structures. If the order of elements in a collection doesn\u2019t matter, use\nsetinstead of list. In Python, the operation of verifying whether a speci\ufb01c example xbelongs\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 10", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2075, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8796bac7-75aa-408b-9b9a-40e055321fec": {"__data__": {"id_": "8796bac7-75aa-408b-9b9a-40e055321fec", "embedding": null, "metadata": {"page_label": "105", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "88cb42ce-2a10-45a2-a6db-d00847e1251c", "node_type": "4", "metadata": {"page_label": "105", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "0a660b8ffaf98e51315c39fd528791a0d457c5235746c7ed75960a0c13da3cc5", "class_name": "RelatedNodeInfo"}}, "text": "toSis e\ufb03cient whenSis declared as a setand is ine\ufb03cient when Sis declared as a list.\nAnother important data structure, which you can use to make your Python code more e\ufb03cient\nisdict. It is called a dictionary or a hashmap in other languages. It allows you to de\ufb01ne a\ncollection of key-value pairs with very fast lookups for keys.\nUnless you know exactly what you do, always prefer using popular libraries to writing your\nown scienti\ufb01c code. Scienti\ufb01c Python packages like numpy, scipy, and scikit-learn were built\nby experienced scientists and engineers with e\ufb03ciency in mind. They have many methods\nimplemented in the C programming language for maximum e\ufb03ciency.\nIf you need to iterate over a vast collection of elements, use generators that create a function\nthat returns one element at a time rather than all the elements at once.\nUsecPro\ufb01lepackage in Python to \ufb01nd ine\ufb03ciencies in your code.\nFinally, when nothing can be improved in your code from the algorithmic perspective, you\ncan further boost the speed of your code by using:\n\u2022multiprocessing package to run computations in parallel, and\n\u2022PyPy,Numbaor similar tools to compile your Python code into fast, optimized machine\ncode.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 11", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1252, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "76fe4eb1-3815-42bc-be6c-3cc1198294ad": {"__data__": {"id_": "76fe4eb1-3815-42bc-be6c-3cc1198294ad", "embedding": null, "metadata": {"page_label": "106", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1163e26b-b26e-417b-a405-bffa47ae1dfa", "node_type": "4", "metadata": {"page_label": "106", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "afbe23810d8f03e48eaaba0be680ca0a1354313d64d12846d247ff625b2c6d4b", "class_name": "RelatedNodeInfo"}}, "text": "9 Unsupervised Learning\nUnsupervised learning deals with problems in which data doesn\u2019t have labels. That property\nmakes it very problematic for many applications. The absence of labels representing the\ndesired behavior for your model means the absence of a solid reference point to judge the\nquality of your model. In this book, I only present unsupervised learning methods that allow\nbuilding models that can be evaluated based on data as opposed to human judgment.\n9.1 Density Estimation\nDensity estimation is a problem of modeling the probability density function (pdf) of the\nunknown probability distribution from which the dataset has been drawn. It can be useful for\nmany applications, in particular for novelty or intrusion detection. In Chapter 7, we already\nestimated the pdf to solve the one-class classi\ufb01cation problem. To do that, we decided that\nour model would be parametric , more precisely a multivariate normal distribution (MVN).\nThis decision was somewhat arbitrary because if the real distribution from which our dataset\nwas drawn is di\ufb00erent from the MVN, our model will be very likely far from perfect. We\nalso know that models can be nonparametric. We used a nonparametric model in kernel\nregression. It turns out that the same approach can work for density estimation.\nLet{xi}N\ni=1be a one-dimensional dataset (a multi-dimensional case is similar) whose examples\nwere drawn from a distribution with an unknown pdf fwithxi\u2208Rfor alli= 1,...,N. We\nare interested in modeling the shape of f. Our kernel model of f, denoted as \u02c6fb, is given by,\n\u02c6fb(x) =1\nNbN\u2211\ni=1k(x\u2212xi\nb)\n, (1)\nwherebis a hyperparameter that controls the tradeo\ufb00 between bias and variance of our\nmodel and kis a kernel. Again, like in Chapter 7, we use a Gaussian kernel:\nk(z) =1\u221a\n2\u03c0exp(\u2212z2\n2)\n.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 3", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1846, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "75e19981-a089-4f42-ad13-5f3aa9e8635e": {"__data__": {"id_": "75e19981-a089-4f42-ad13-5f3aa9e8635e", "embedding": null, "metadata": {"page_label": "107", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "be0ae290-c25e-4b00-a405-6401e9e485e9", "node_type": "4", "metadata": {"page_label": "107", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "8147d55bae8d2f9eec1847481b5ba96494ba7d33ad589de6e568b7122bb77181", "class_name": "RelatedNodeInfo"}}, "text": "0 2 4 6 8 10 12\nx0.000.050.100.150.200.250.30pdffb, b=0.67\ntrue pdf\ntraining examples(a)\n0 2 4 6 8 10 12\nx0.000.050.100.150.200.250.30pdffb, b=0.2\ntrue pdf\ntraining examples (b)\n0 2 4 6 8 10 12\nx0.000.050.100.150.200.250.30pdffb, b=2.0\ntrue pdf\ntraining examples\n(c)\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\nb0.10\n0.05\n0.000.050.100.15l (d)\nFigure 1: Kernel density estimation: (a) good \ufb01t; (b) over\ufb01tting; (c) under\ufb01tting; (d) the\ncurve of grid search for the best value for b.\nWe look for such a value of bthat minimizes the di\ufb00erence between the real shape of fand\nthe shape of our model \u02c6fb. A reasonable choice of measure of this di\ufb00erence is called the\nmean integrated squared error (MISE):\nMISE(b) =E[\u222b\nR(\u02c6fb(x)\u2212f(x))2dx]\n. (2)\nIntuitively, you see in eq. 2 that we square the di\ufb00erence between the real pdf fand our\nmodel of it \u02c6fb. The integral\u222b\nRreplaces the summation\u2211N\ni=1we employed in the mean\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 4", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 957, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d52acce8-77d5-454d-ad59-17ba89909c4f": {"__data__": {"id_": "d52acce8-77d5-454d-ad59-17ba89909c4f", "embedding": null, "metadata": {"page_label": "108", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fb00f818-6704-4b93-8229-5a69a872387f", "node_type": "4", "metadata": {"page_label": "108", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "3ca6753be2de72cc12cef105acedfb4f0c6fb3567d03e36d4b1525cf6ad174bd", "class_name": "RelatedNodeInfo"}}, "text": "squared error, while the expectation operator Ereplaces the average1\nN.\nIndeed, when our loss is a function with a continuous domain, such as (\u02c6fb(x)\u2212f(x))2, we\nhave to replace the summation with the integral. The expectation operation Emeans that\nwe wantbto be optimal for all possible realizations of our training set {xi}N\ni=1. That is\nimportant because \u02c6fbis de\ufb01ned on a \ufb01nitesample of some probability distribution, while the\nreal pdffis de\ufb01ned on an in\ufb01nite domain (the set R).\nNow, we can rewrite the right-hand side term in eq. 2 like this:\nE[\u222b\nR\u02c6f2\nb(x)dx]\n\u22122E[\u222b\nR\u02c6fb(x)f(x)dx]\n+E[\u222b\nRf(x)2dx]\n.\nThe third term in the above summation is independent of band thus can be ignored. An\nunbiased estimator of the \ufb01rst term is given by\u222b\nR\u02c6f2\nb(x)dxwhile the unbiased estimator of\nthe second term can be approximated by cross-validation \u22122\nN\u2211N\ni=1\u02c6f(i)\nb(xi), where \u02c6f(i)\nbis\na kernel model of fcomputed on our training set with the example xiexcluded.\nThe term\u2211N\ni=1\u02c6f(i)\nb(xi)is known in statistics as the leave one out estimate , a form of cross-\nvalidation in which each fold consists of one example. You could have noticed that the term\u222b\nR\u02c6fb(x)f(x)dx(let\u2019s call it a) is the expected value of the function \u02c6fb, becausefis a pdf. It\ncan be demonstrated that the leave one out estimate is an unbiased estimator of E[a].\nNow, to \ufb01nd the optimal value b\u2217forb, we minimize the cost de\ufb01ned as,\n\u222b\nR\u02c6f2\nb(x)dx\u22122\nNN\u2211\ni=1\u02c6f(i)\nb(xi).\nWe can \ufb01nd b\u2217using grid search. For D-dimensional feature vectors x, the error term x\u2212xi\nin eq. 1 can be replaced by the Euclidean distance \u2225x\u2212xi\u2225. In Figure 1 you can see the\nestimates for the same pdf obtained with three di\ufb00erent values of bfrom a 100-example\ndataset, as well as the grid search curve. We pick b\u2217at the minimum of the grid search curve.\n9.2 Clustering\nClustering is a problem of learning to assign a label to examples by leveraging an unlabeled\ndataset. Because the dataset is completely unlabeled, deciding on whether the learned model\nis optimal is much more complicated than in supervised learning.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 5", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2113, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ed2815a6-0544-4727-ba7e-2e4b44d83b0d": {"__data__": {"id_": "ed2815a6-0544-4727-ba7e-2e4b44d83b0d", "embedding": null, "metadata": {"page_label": "109", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "acb9bc96-3194-42ec-94d7-3216d5d4c7bd", "node_type": "4", "metadata": {"page_label": "109", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "93423b2729a04cad7057503acdef08866ef826a992d42b1b5dccfae8d4b956a6", "class_name": "RelatedNodeInfo"}}, "text": "(a) original data\n (b) iteration 1\n(c) iteration 3\n (d) iteration 5\nFigure 2: The progress of the k-means algorithm for k= 3.\nThere is a variety of clustering algorithms, and, unfortunately, it\u2019s hard to tell which one is\nbetter in quality for your dataset. Usually, the performance of each algorithm depends on\nthe unknown properties of the probability distribution the dataset was drawn from. In this\nChapter, I outline the most useful and widely used clustering algorithms.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 6", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 539, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3297d0df-bdac-4205-8d02-636e82965685": {"__data__": {"id_": "3297d0df-bdac-4205-8d02-636e82965685", "embedding": null, "metadata": {"page_label": "110", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1207bb23-3cc4-4ccf-8acd-e7fbf0dbde9c", "node_type": "4", "metadata": {"page_label": "110", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "79544cd3a1c0ad64d3429b5ecf96a91c5ba05226d5fe2b115b48d89652ea020a", "class_name": "RelatedNodeInfo"}}, "text": "9.2.1 K-Means\nThek-means clustering algorithm works as follows. First, you choose k\u2014 the number of\nclusters. Then you randomly put kfeature vectors, called centroids , to the feature space.\nWe then compute the distance from each example xto each centroid cusing some metric,\nlike the Euclidean distance. Then we assign the closest centroid to each example (like if we\nlabeled each example with a centroid id as the label). For each centroid, we calculate the\naverage feature vector of the examples labeled with it. These average feature vectors become\nthe new locations of the centroids.\nWe recompute the distance from each example to each centroid, modify the assignment and\nrepeat the procedure until the assignments don\u2019t change after the centroid locations were\nrecomputed. The model is the list of assignments of centroids IDs to the examples.\nThe initial position of centroids in\ufb02uence the \ufb01nal positions, so two runs of k-means can\nresult in two di\ufb00erent models. Some variants of k-means compute the initial positions of\ncentroids based on some properties of the dataset.\nOne run of the k-means algorithm is illustrated in Figure 2. The circles in Figure 2 are\ntwo-dimensional feature vectors; the squares are moving centroids. Di\ufb00erent background\ncolors represent regions in which all points belong to the same cluster.\nThe value of k, the number of clusters, is a hyperparameter that has to be tuned by the\ndata analyst. There are some techniques for selecting k. None of them is proven optimal.\nMost of those techniques require the analyst to make an \u201ceducated guess\u201d by looking at some\nmetrics or by examining cluster assignments visually. In this chapter, I present one approach\nto choose a reasonably good value for kwithout looking at the data and making guesses.\n9.2.2 DBSCAN and HDBSCAN\nWhile k-means and similar algorithms are centroid-based, DBSCAN is a density-based\nclustering algorithm. Instead of guessing how many clusters you need, by using DBSCAN,\nyou de\ufb01ne two hyperparameters: \u03f5andn. You start by picking an example xfrom your\ndataset at random and assign it to cluster 1. Then you count how many examples have\nthe distance from xless than or equal to \u03f5. If this quantity is greater than or equal to n,\nthen you put all these \u03f5-neighbors to the same cluster 1. You then examine each member of\ncluster 1and \ufb01nd their respective \u03f5-neighbors. If some member of cluster 1hasnor more\n\u03f5-neighbors, you expand cluster 1by putting those \u03f5-neighbors to the cluster. You continue\nexpanding cluster 1until there are no more examples to put in it. In the latter case, you pick\nfrom the dataset another example not belonging to any cluster and put it to cluster 2. You\ncontinue like this until all examples either belong to some cluster or are marked as outliers.\nAn outlier is an example whose \u03f5-neighborhood contains less than nexamples.\nThe advantage of DBSCAN is that it can build clusters that have an arbitrary shape, while k-\nmeans and other centroid-based algorithms create clusters that have a shape of a hypersphere.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3103, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0cf6e7f5-5659-4230-9c66-0714b781e6e2": {"__data__": {"id_": "0cf6e7f5-5659-4230-9c66-0714b781e6e2", "embedding": null, "metadata": {"page_label": "111", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5c26d7fb-a644-4547-bd3a-86589b716f75", "node_type": "4", "metadata": {"page_label": "111", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "9bdd5f6ee694bd51c49be62df83ceecfcb9da62960a23bbe4a9354368e38023f", "class_name": "RelatedNodeInfo"}}, "text": "An obvious drawback of DBSCAN is that it has two hyperparameters and choosing good\nvalues for them (especially \u03f5) could be challenging. Furthermore, having \u03f5\ufb01xed, the clustering\nalgorithm cannot e\ufb00ectively deal with clusters of varying density.\nHDBSCAN is the clustering algorithm that keeps the advantages of DBSCAN, by removing\nthe need to decide on the value of \u03f5. The algorithm is capable of building clusters of\nvarying density. HDBSCAN is an ingenious combination of multiple ideas and describing the\nalgorithm in full is beyond the scope of this book.\nHDBSCAN only has one important hyperparameter: n, the minimum number of examples to\nput in a cluster. This hyperparameter is relatively simple to choose by intuition. HDBSCAN\nhas very fast implementations: it can deal with millions of examples e\ufb00ectively. Modern\nimplementations of k-means are much faster than HDBSCAN though, but the qualities of\nthe latter may outweigh its drawbacks for many practical tasks. I recommend to always try\nHDBSCAN on your data \ufb01rst.\n9.2.3 Determining the Number of Clusters\nThe most important question is how many clusters does your dataset have? When the feature\nvectors are one-, two- or three-dimensional, you can look at the data and see \u201cclouds\u201d of\npoints in the feature space. Each cloud is a potential cluster. However, for D-dimensional\ndata, withD> 3, looking at the data is problematic1.\nOne way of determining the reasonable number of clusters is based on the concept of\nprediction strength . The idea is to split the data into training and test set, similarly to\nhow we do in supervised learning. Once you have the training and test sets, Strof size\nNtrandSteof sizeNterespectively, you \ufb01x k, the number of clusters, and run a clustering\nalgorithmCon setsStrandSteand obtain the clustering results C(Str,k)andC(Ste,k).\nLetAbe the clustering C(Str,k)built using the training set. The clusters in Acan be seen\nas regions. If an example falls within one of those regions, then that example belongs to\nsome speci\ufb01c cluster. For example, if we apply the k-means algorithm to some dataset, it\nresults in a partition of the feature space into kpolygonal regions, as we saw in Figure 2.\nDe\ufb01ne the Nte\u00d7Nteco-membership matrix D[A,Ste]as follows: D[A,Ste](i,i\u2032)= 1if and\nonly if examples xiandxi\u2032from the test set belong to the same cluster according to the\nclusteringA. Otherwise D[A,Ste](i,i\u2032)= 0.\nLet\u2019s take a break and see what we have here. We have built, using the training set of\nexamples, a clustering Athat haskclusters. Then we have built the co-membership matrix\nthat indicates whether two examples from the test set belong to the same cluster in A.\n1Some analysts look at multiple two-dimensional plots, in which only a pair of features are present at the\nsame time. It might give an intuition about the number of clusters. However, such an approach su\ufb00ers from\nsubjectivity, is prone to error and counts as an educated guess rather than a scienti\ufb01c method.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 8", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3025, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc840564-39ae-4273-b4b2-3cc78a887177": {"__data__": {"id_": "cc840564-39ae-4273-b4b2-3cc78a887177", "embedding": null, "metadata": {"page_label": "112", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "35e98686-cc2b-4fc4-9aa9-5e0d2ce033e3", "node_type": "4", "metadata": {"page_label": "112", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "aa15c482268f4ad3a84322effab379056cf7cb3cfc0bec128ce2657e4b292cdb", "class_name": "RelatedNodeInfo"}}, "text": "Intuitively, if the quantity kis the reasonable number of clusters, then two examples that\nbelong to the same cluster in clustering C(Ste,k)will most likely belong to the same cluster\nin clustering C(Str,k). On the other hand, if kis not reasonable (too high or too low), then\ntraining data-based and test data-based clusterings will likely be less consistent.\n2\n 0 2 40246\nFull dataset\n2\n 0 2 40246 Training set\n2\n 0 2 40246 Test set\nFigure 3: Data used for clustering illustrated in Figure 4\n2\n 0 2 40246\n(a)\n2\n 0 2 40246\n (b)\n2\n 0 2 40246\n (c)\nFigure 4: The clustering for k= 4: (a) training data clustering; (b) test data clustering; (c)\ntest data plotted over the training clustering.\nUsing the data shown in Figure 3, the idea is illustrated in Figure 4. The plots in Figure 4a\nand 4b show respectively C(Str,4)andC(Ste,4)with their respective cluster regions. Figure\n4c shows the test examples plotted over the training data cluster regions. You can see in\n4c that orange test examples don\u2019t belong anymore to the same cluster according to the\nclustering regions obtained from the training data. This will result in many zeroes in the\nmatrix D[A,Ste]which, in turn, is an indicator that k= 4is likely not the best number of\nclusters.\nMore formally, the prediction strength for the number of clusters kis given by,\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 9", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1383, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9949ef84-c95b-4021-a186-09be6e6ffd3c": {"__data__": {"id_": "9949ef84-c95b-4021-a186-09be6e6ffd3c", "embedding": null, "metadata": {"page_label": "113", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "66ec9c54-e70f-4f8f-9774-e291067e35cc", "node_type": "4", "metadata": {"page_label": "113", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "7a984990f8498f83b18aa2799a9e5fc5750b58e2bf320066694dc0756f78ead6", "class_name": "RelatedNodeInfo"}}, "text": "ps(k)def= min\nj=1,...,k1\n|Aj|(|Aj|\u22121)\u2211\ni,i\u2032\u2208AjD[A,Ste](i,i\u2032),\nwhereAdef=C(Str,k),Ajisjthcluster from the clustering C(Ste,k)and|Aj|is the number\nof examples in cluster Aj.\nGiven a clustering C(Str,k), for each test cluster, we compute the proportion of observation\npairs in that cluster that are also assigned to the same cluster by the training set centroids.\nThe prediction strength is the minimum of this quantity over the ktest clusters.\nExperiments suggest that a reasonable number of clusters is the\nlargestksuch that ps(k)is above 0.8. You can see in Figure 5\nexamples of predictive strength for di\ufb00erent values of kfor two-,\nthree- and four-cluster data.\nFor non-deterministic clustering algorithms, such as k-means,\nwhich can generate di\ufb00erent clusterings depending on the ini-\ntial positions of centroids, it is recommended to do multiple runs of\nthe clustering algorithm for the same kand compute the average\nprediction strength \u00af ps(k)over multiple runs.\nAnother e\ufb00ective method to estimate the number of clusters is the gap statistic method.\nOther, less automatic methods, which some analysts still use, include the elbow method\nand the average silhouette method .\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 10", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1241, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7983bfc7-6ae6-4fbd-8762-d1b320e07bd6": {"__data__": {"id_": "7983bfc7-6ae6-4fbd-8762-d1b320e07bd6", "embedding": null, "metadata": {"page_label": "114", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1f3abe10-a5a1-459e-915f-ccf799871a58", "node_type": "4", "metadata": {"page_label": "114", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "7673708a4c1e996bb4372a3a5aa695f86b4524550b57ae49cbea0851040521e7", "class_name": "RelatedNodeInfo"}}, "text": "0 1 2 3 4\nx10 2 4 6x2\n2\n 1\n 0 1 2 3 4\nx10 2 4 6x2\n3\n 2\n 1\n 0 1 2 3 4\nx10 2 4 6 810x2\n1 2 3 4 5 6 7 8\nk0.0 0.2 0.4 0.6 0.8 1.0ps(k)\n1 2 3 4 5 6 7 8\nk0.0 0.2 0.4 0.6 0.8 1.0ps(k)\n1 2 3 4 5 6 7 8\nk0.0 0.2 0.4 0.6 0.8 1.0ps(k)Figure 5: Predictive strength for di\ufb00erent values of kfor two-, three- and four-cluster data.\n9.2.4 Other Clustering Algorithms\nDBSCAN and k-means compute so-called hard clustering , in which each example can\nbelong to only one cluster. Gaussian mixture model (GMM) allow each example to be\na member of several clusters with di\ufb00erent membership score (HDBSCAN allows that\ntoo). Computing a GMM is very similar to doing model-based density estimation. In GMM,\ninstead of having just one multivariate normal distribution (MND), we have a weighted sum\nof several MNDs:\nfX=k\u2211\nj=1\u03c6jf\u00b5j,\u03a3j,\nwheref\u00b5j,\u03a3jis a MNDj, and\u03c6jis its weight in the sum. The values of parameters \u00b5j,\u03a3j,\nand\u03c6j, for allj= 1,...,kare obtained using the expectation maximization algorithm\n(EM) to optimize the maximum likelihood criterion.\nAgain, for simplicity, let us look at the one-dimensional data. Also assume that there are two\nclusters:k= 2. In this case, we have two Gaussian distributions,\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 11", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1248, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d529e27b-abd6-4808-82da-ec8daeebd7e5": {"__data__": {"id_": "d529e27b-abd6-4808-82da-ec8daeebd7e5", "embedding": null, "metadata": {"page_label": "115", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e6d3dcf4-4ba5-4eef-8cf1-7dee1d6b622a", "node_type": "4", "metadata": {"page_label": "115", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "c14d1835f0f8a53efc72da28a8549747fb5aa9c1672ed14ee1b47ac22e1ac756", "class_name": "RelatedNodeInfo"}}, "text": "f(x|\u00b51,\u03c32\n1) =1\u221a\n2\u03c0\u03c32\n1exp\u2212(x\u2212\u00b51)2\n2\u03c32\n1andf(x|\u00b52,\u03c32\n2) =1\u221a\n2\u03c0\u03c32\n2exp\u2212(x\u2212\u00b52)2\n2\u03c32\n2,(3)\nwheref(x|\u00b51,\u03c32\n1)andf(x|\u00b52,\u03c32\n2)are two pdf de\ufb01ning the likelihood of X=x.\nWe use the EM algorithm to estimate \u00b51,\u03c32\n1,\u00b52,\u03c32\n2,\u03c61, and\u03c62. The parameters \u03c61and\u03c62\nare useful for the density estimation and less useful for clustering, as we will see below.\nEM works like follows. In the beginning, we guess the initial values for \u00b51,\u03c32\n1,\u00b52, and\u03c32\n2,\nand set\u03c61=\u03c62=1\n2(in general, it\u2019s1\nkfor each\u03c6j,j\u22081,...,k).\nAt each iteration of EM, the following four steps are executed:\n1. For alli= 1,...,N, calculate the likelihood of each xiusing eq. 3:\nf(xi|\u00b51,\u03c32\n1)\u21901\u221a\n2\u03c0\u03c32\n1exp\u2212(xi\u2212\u00b51)2\n2\u03c32\n1andf(xi|\u00b52,\u03c32\n2)\u21901\u221a\n2\u03c0\u03c32\n2exp\u2212(xi\u2212\u00b52)2\n2\u03c32\n2.\n2.Using Bayes\u2019 Rule , for each example xi, calculate the likelihood b(j)\nithat the example\nbelongs to cluster j\u2208{1,2}(in other words, the likelihood that the example was drawn\nfrom the Gaussian j):\nb(j)\ni\u2190f(xi|\u00b5j,\u03c32\nj)\u03c6j\nf(xi|\u00b51,\u03c32\n1)\u03c61+f(xi|\u00b52,\u03c32\n2)\u03c62.\nThe parameter \u03c6jre\ufb02ects how likely is that our Gaussian distribution jwith parameters \u00b5j\nand\u03c32\njmay have produced our dataset. That is why in the beginning we set \u03c61=\u03c62=1\n2:\nwe don\u2019t know how each of the two Gaussians is likely, and we re\ufb02ect our ignorance by setting\nthe likelihood of both to one half.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 12", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1336, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9036412e-c813-4d74-ad6f-d408795e9e22": {"__data__": {"id_": "9036412e-c813-4d74-ad6f-d408795e9e22", "embedding": null, "metadata": {"page_label": "116", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b939cf73-df5f-4b35-86a6-8bee5adb074b", "node_type": "4", "metadata": {"page_label": "116", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "f3c0cfcec24a458414fdaea6eb81d5d17900e56289afc9013ab74321e4e96c6a", "class_name": "RelatedNodeInfo"}}, "text": "2\n 0 2 4 6 8 10 12\nx0.00.10.20.30.40.50.60.70.8pdff(xi 1,2\n1)\nf(xi 2,2\n2)\ntrue pdf\ntraining examplesIteration 1\n2\n 0 2 4 6 8 10 12\nx0.00.10.20.30.40.50.60.70.8pdff(xi 1,2\n1)\nf(xi 2,2\n2)\ntrue pdf\ntraining examples Iteration 2\n2\n 0 2 4 6 8 10 12\nx0.00.10.20.30.40.50.60.70.8pdff(xi 1,2\n1)\nf(xi 2,2\n2)\ntrue pdf\ntraining examples\nIteration 10\n2\n 0 2 4 6 8 10 12\nx0.00.10.20.30.40.50.60.70.8pdff(xi 1,2\n1)\nf(xi 2,2\n2)\ntrue pdf\ntraining examples Iteration 40\nFigure 6: The progress of the Gaussian mixture model estimation using the EM algorithm\nfor two clusters ( k= 2).\n3. Compute the new values of \u00b5jand\u03c32\nj,j\u2208{1,2}as,\n\u00b5j\u2190\u2211N\ni=1b(j)\nixi\u2211N\ni=1b(j)\niand\u03c32\nj\u2190\u2211N\ni=1b(j)\ni(xi\u2212\u00b5j)2\n\u2211N\ni=1b(j)\ni. (4)\n4. Update\u03c6j,j\u2208{1,2}as,\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 13", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 778, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a4335725-15bb-47b3-8afc-4a2d3b31c7bd": {"__data__": {"id_": "a4335725-15bb-47b3-8afc-4a2d3b31c7bd", "embedding": null, "metadata": {"page_label": "117", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6df73be9-77f3-4296-b9df-b20a1d1e3b6e", "node_type": "4", "metadata": {"page_label": "117", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "02d4e2613c8f76d69a2db6cdc89b129b9c934d518db4e1bd62122e7daaaa2b2b", "class_name": "RelatedNodeInfo"}}, "text": "\u03c6j\u21901\nNN\u2211\ni=1b(j)\ni.\nThe steps 1\u22124are executed iteratively until the values \u00b5jand\u03c32\njdon\u2019t change much: for\nexample, the change is below some threshold \u03f5. Figure 6 illustrates this process.\nYou may have noticed that the EM algorithm is very similar to the k-means algorithm: start\nwith random clusters, then iteratively update each cluster\u2019s parameters by averaging the\ndata that is assigned to that cluster. The only di\ufb00erence in the case of the GMM is that the\nassignment of an example xito the cluster jissoft:xibelongs to cluster jwith probability\nb(j)\ni. This is why we calculate the new values for \u00b5jand\u03c32\njin eq. 4 not as an average (used\nin k-means) but as a weighted average with weights b(j)\ni.\nOnce we have learned the parameters \u00b5jand\u03c32\njfor each cluster j, the membership score of\nexamplexin clusterjis given by f(x|\u00b5j,\u03c32\nj).\nThe extension to D-dimensional data ( D > 1) is straightforward. The only di\ufb00erence is\nthat instead of the variance \u03c32, we now have the covariance matrix \u03a3that parametrizes the\nmultinomial normal distribution (MND).\nContrary to k-means where clusters can only be circular, the\nclusters in GMM have a form of an ellipse that can have an\narbitrary elongation and rotation. The values in the covariance\nmatrix control these properties.\nThere\u2019s no universally recognized method to choose the right k\nin GMM. I recommend to \ufb01rst split the dataset into training and\ntest set. Then you try di\ufb00erent kand build a di\ufb00erent model\nfk\ntrfor eachkon the training data. You pick the value of kthat\nmaximizes the likelihood of examples in the test set:\narg max\nk|Nte|\u220f\ni=1fk\ntr(xi),\nwhere|Nte|is the size of the test set.\nThere is a variety of clustering algorithms described in the literature. Worth mentioning are\nspectral clustering andhierarchical clustering . For some datasets, you may \ufb01nd those\nmore appropriate. However, in most practical cases, kmeans, HDBSCAN and the Gaussian\nmixture model would satisfy your needs.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 14", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2013, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e5552675-92cd-408f-bf76-7b5d271b2374": {"__data__": {"id_": "e5552675-92cd-408f-bf76-7b5d271b2374", "embedding": null, "metadata": {"page_label": "118", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36acd708-e0f9-49e6-a6fb-74218b35fa92", "node_type": "4", "metadata": {"page_label": "118", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "b263c189fb9804199b68cb0313dca6930ab383f0cb878a28d97f23703c1180c5", "class_name": "RelatedNodeInfo"}}, "text": "9.3 Dimensionality Reduction\nModern machine learning algorithms, such as ensemble algorithms and neural networks handle\nwell very high-dimensional examples, up to millions of features. With modern computers\nand graphical processing units (GPUs), dimensionality reduction techniques are used less in\npractice than in the past. The most frequent use case for dimensionality reduction is data\nvisualization: humans can only interpret on a plot the maximum of three dimensions.\nAnother situation in which you could bene\ufb01t from dimensionality reduction is when you\nhave to build an interpretable model and to do so you are limited in your choice of learning\nalgorithms. For example, you can only use decision tree learning or linear regression. By\nreducing your data to lower dimensionality and by \ufb01guring out which quality of the original\nexample each new feature in the reduced feature space re\ufb02ects, one can use simpler algorithms.\nDimensionality reduction removes redundant or highly correlated features; it also reduces the\nnoise in the data \u2014 all that contributes to the interpretability of the model.\nThree widely used techniques of dimensionality reduction are principal component anal-\nysis(PCA), uniform manifold approximation and projection (UMAP), and autoen-\ncoders.\nI already explained autoencoders in Chapter 7. You can use the low-dimensional output of the\nbottleneck layer of the autoencoder as the vector of reduced dimensionality that represents\nthe high-dimensional input feature vector. You know that this low-dimensional vector\nrepresents the essential information contained in the input vector because the autoencoder is\ncapable of reconstructing the input feature vector based on the bottleneck layer output alone.\n2\n 0 2\nx12\n1\n012x2\n(a)\n2\n 0 2\nx12\n1\n012x2 (b)\n2\n 0 2\nx12\n1\n012x2 (c)\nFigure 7: PCA: (a) the original data; (b) two principal components displayed as vectors; (c)\nthe data projected on the \ufb01rst principal component.\n9.3.1 Principal Component Analysis\nPrincipal component analysis or PCA is one of the oldest dimensionality reduction methods.\nThe math behind it involves operation on matrices that I didn\u2019t explain in Chapter 2, so I\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2228, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d20653fb-2fed-43da-9e36-8b6b6ad2a265": {"__data__": {"id_": "d20653fb-2fed-43da-9e36-8b6b6ad2a265", "embedding": null, "metadata": {"page_label": "119", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e71a8006-d0b0-4e85-b8b4-a1b1209a86f1", "node_type": "4", "metadata": {"page_label": "119", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "37a150baa618cbf55f869a801f8d65c2aebb7663734aef3d63c970f9f89eff2e", "class_name": "RelatedNodeInfo"}}, "text": "leave the math of PCA for your further reading. Here, I only provide intuition and illustrate\nthe method on an example.\nConsider a two-dimensional data as shown in Figure 7a. Principal components are vectors\nthat de\ufb01ne a new coordinate system in which the \ufb01rst axis goes in the direction of the highest\nvariance in the data. The second axis is orthogonal to the \ufb01rst one and goes in the direction\nof the second highest variance in the data. If our data was three-dimensional, the third axis\nwould be orthogonal to both the \ufb01rst and the second axes and go in the direction of the third\nhighest variance, and so on. In Figure 7b, the two principal components are shown as arrows.\nThe length of the arrow re\ufb02ects the variance in this direction.\nNow, if we want to reduce the dimensionality of our data to\nDnew<D, we pickDnewlargest principal components and project\nour data points on them. For our two-dimensional illustration, we\ncan setDnew= 1and project our examples to the \ufb01rst principal\ncomponent to obtain the orange points in Figure 7c.\nTo describe each orange point, we need only one coordinate instead\nof two: the coordinate with respect to the \ufb01rst principal compo-\nnent. When our data is very high-dimensional, it often happens in\npractice that the \ufb01rst two or three principal components account\nfor most of the variation in the data, so by displaying the data on a 2D or 3D plot we can\nindeed see a very high-dimensional data and its properties.\n9.3.2 UMAP\nThe idea behind many of the modern dimensionality reduction algorithms, especially those\ndesigned speci\ufb01cally for visualization purposes, such as t-SNEandUMAP, is basically\nthe same. We \ufb01rst design a similarity metric for two examples. For visualization purposes,\nbesides the Euclidean distance between the two examples, this similarity metric often re\ufb02ects\nsome local properties of the two examples, such as the density of other examples around\nthem.\nIn UMAP, this similarity metric wis de\ufb01ned as follows,\nw(xi,xj)def=wi(xi,xj) +wj(xj,xi)\u2212wi(xi,xj)wj(xj,xi). (5)\nThe function wi(xi,xj)is de\ufb01ned as,\nwi(xi,xj)def= exp(\n\u2212d(xi,xj)\u2212\u03c1i\n\u03c3i)\n,\nwhered(xi,xj)is the Euclidean distance between two examples, \u03c1iis the distance from xi\nto its closest neighbor, and \u03c3iis the distance from xito itskthclosest neighbor ( kis a\nhyperparameter of the algorithm).\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 16", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2376, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c88faa36-2a24-4b24-8a0c-3ddc63d49193": {"__data__": {"id_": "c88faa36-2a24-4b24-8a0c-3ddc63d49193", "embedding": null, "metadata": {"page_label": "120", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3a140472-3f96-4831-a7ee-0a8d9ea53dd6", "node_type": "4", "metadata": {"page_label": "120", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "9dbbfa4bf5ef1c9bc822812ae704c6c8a380c027ec7a485841e7b17579c6b2cb", "class_name": "RelatedNodeInfo"}}, "text": "It can be shown that the metric in eq. 5 varies in the range from 0to1and is symmetric,\nwhich means that w(xi,xj) =w(xj,xi).\nLetwdenote the similarity of two examples in the original high-dimensional space and let w\u2032\nbe the similarity given by the same eq. 5 in the new low-dimensional space.\nTo continue, I need to quickly introduce the notion of a fuzzy set . A fuzzy set is a\ngeneralization of a set. For each element xin a fuzzy setS, there\u2019s a membership function\n\u00b5S(x)\u2208[0,1]that de\ufb01nes the membership strength ofxto the setS. We say that xweakly\nbelongs to a fuzzy set Sif\u00b5S(x)is close to zero. On the other hand, if \u00b5S(x)is close to 1,\nthenxhas a strong membership in S. If\u00b5(x) = 1for allx\u2208S, then a fuzzy set Sbecomes\nequivalent to a normal, nonfuzzy set.\nLet\u2019s now see why we need this notion of a fuzzy set here.\nBecause the values of wandw\u2032lie in the range between 0and1, we can see w(xi,xj)as\nmembership of the pair of examples (xi,xj)in a certain fuzzy set. The same can be said\naboutw\u2032. The notion of similarity of two fuzzy sets is called fuzzy set cross-entropy and\nis de\ufb01ned as,\nCw,w\u2032=N\u2211\ni=1N\u2211\nj=1[\nw(xi,xj) ln(\nw(xi,xj)\nw\u2032(x\u2032\ni,x\u2032\nj))\n+ (1\u2212w(xi,xj)) ln(\n1\u2212w(xi,xj)\n1\u2212w\u2032(x\u2032\ni,x\u2032\nj))]\n,(6)\nwhere x\u2032is the low-dimensional \u201cversion\u201d of the original high-dimensional example x.\nIn eq. 6 the unknown parameters are x\u2032\ni(for alli= 1,...,N), the low-dimensional examples\nwe look for. We can compute them by gradient descent by minimizing Cw,w\u2032.\nPCA\n UMAP\n Autoencoder\nFigure 8: Dimensionality reduction of the MNIST dataset using three di\ufb00erent techniques.\nIn Figure 8, you can see the result of dimensionality reduction applied to the MNIST dataset\nof handwritten digits. MNIST is commonly used for benchmarking various image processing\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 17", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1811, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c6e54bac-f0bb-47f2-b803-0c23b71cf13e": {"__data__": {"id_": "c6e54bac-f0bb-47f2-b803-0c23b71cf13e", "embedding": null, "metadata": {"page_label": "121", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e1d3c0c2-8fa4-435a-ab65-3e9cbbef7c2c", "node_type": "4", "metadata": {"page_label": "121", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "7383ef572cf836d6a16d9285ac9f88068a32ae9585e7a194aa3cbc43d70e9f83", "class_name": "RelatedNodeInfo"}}, "text": "systems; it contains 70,000 labeled examples. Ten di\ufb00erent colors on the plot correspond to\nten classes. Each point on the plot corresponds a speci\ufb01c example in the dataset. As you can\nsee, UMAP separates examples visually better (remember, it doesn\u2019t have access to labels).\nIn practice, UMAP is slightly slower than PCA but faster than autoencoder.\n9.4 Outlier Detection\nOutlier detection is the problem of detecting in the dataset the examples that are very\ndi\ufb00erent from what a typical example in the dataset looks like. We have already seen several\ntechniques that could help to solve this problem: autoencoder and one-class classi\ufb01er learning.\nIf we use an autoencoder, we train it on our dataset. Then, if we want to predict whether an\nexample is an outlier, we can use the autoencoder model to reconstruct the example from\nthe bottleneck layer. The model will unlikely be capable of reconstructing an outlier.\nIn one-class classi\ufb01cation, the model either predicts that the input example belongs to the\nclass, or it\u2019s an outlier.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 18", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1100, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "634f5e64-ea86-4a55-a803-ed6c47e7f121": {"__data__": {"id_": "634f5e64-ea86-4a55-a803-ed6c47e7f121", "embedding": null, "metadata": {"page_label": "122", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1aacf9a1-a264-4d8c-ad62-4ed02ae7df37", "node_type": "4", "metadata": {"page_label": "122", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "6443a2b7dc3068b9ead38d7ce9b65175a9633922ef8af0346718d7bed9ad455d", "class_name": "RelatedNodeInfo"}}, "text": "10 Other Forms of Learning\n10.1 Metric Learning\nI mentioned that the most frequently used metrics of similarity (or dissimilarity) between\ntwo feature vectors are Euclidean distance andcosine similarity . Such choices of metric\nseem logical but arbitrary, just like the choice of the squared error in linear regression (or\nthe form of linear regression itself). The fact that one metric can work better than another\ndepending on the dataset is an indicator that none of them are perfect.\nYou can createa metric that would work better for your dataset. It\u2019s then possible to\nintegrate your metric into any learning algorithm that needs a metric, like k-means or kNN.\nHow can you know, without trying all possibilities, which equation would be a good metric?\nAs you could already guess, a metric can be learned from data.\nRemember the Euclidean distance between two feature vectors xandx\u2032:\nd(x,x\u2032) =\u2225x\u2212x\u2032\u2225def=\u221a\n(x\u2212x\u2032)2=\u221a\n(x\u2212x\u2032)(x\u2212x\u2032).\nWe can slightly modify this metric to make it parametrizable and then learn these parameters\nfrom data. Consider the following modi\ufb01cation:\ndA(x,x\u2032) =\u2225x\u2212x\u2032\u2225Adef=\u2211\n(x\u2212x\u2032)\u22a4A(x\u2212x\u2032),\nwhere Ais aD\u00d7Dmatrix. Let\u2019s say D= 3. If we let Abe the identity matrix,\nAdef=\uf8ee\n\uf8f01 0 0\n0 1 0\n0 0 1\uf8f9\n\uf8fb,\nthendAbecomes the Euclidean distance. If we have a general diagonal matrix, like this:\nAdef=\uf8ee\n\uf8f02 0 0\n0 8 0\n0 0 1\uf8f9\n\uf8fb,\nthen di\ufb00erent dimensions have di\ufb00erent importance in the metric. (In the above example,\nthe second dimension is the most important in the metric calculation.) More generally, to be\ncalled a metric a function of two variables has to satisfy three conditions:\n1. d(x,x\u2032)\u22650 nonnegativity,\n2. d(x,x\u2032)\u2264d(x,z) +d(z,x\u2032)triangle inequality,\n3. d(x,x\u2032) =d(x\u2032,x) symmetry.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 3", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1757, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "93d52f67-c9a2-400f-b662-9e73f7ebb849": {"__data__": {"id_": "93d52f67-c9a2-400f-b662-9e73f7ebb849", "embedding": null, "metadata": {"page_label": "123", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cbead1e3-8b11-46c1-8d1c-49a9f8a3349c", "node_type": "4", "metadata": {"page_label": "123", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "3b8f533656a0fc28c72df9415c48957d0902551835dc495d47abb6252ad3cdde", "class_name": "RelatedNodeInfo"}}, "text": "To satisfy the \ufb01rst two conditions, the matrix Ahas to be positive semide\ufb01nite . You can\nsee a positive semide\ufb01nite matrix as the generalization of the notion of a nonnegative real\nnumber to matrices. Any positive semide\ufb01nite matrix Msatis\ufb01es:\nz\u22a4Mz\u22650,\nfor any vector zhaving the same dimensionality as the number of rows and columns in M.\nThe above property follows from the de\ufb01nition of a positive semide\ufb01nite matrix. The proof\nthat the second condition is satis\ufb01ed when the matrix Ais positive semide\ufb01nite can be found\non the book\u2019s companion website.\nTo satisfy the third condition, we can simply take (d(x,x\u2032) +d(x\u2032,x))/2.\nLet\u2019s say we have an unannotated set X={xi}N\ni=1. To build the training data for our\nmetric learning problem, we manually create two sets. The \ufb01rst set Sis such that a pair of\nexamples (xi,xk)belongs to setSifxiandxkare similar (from our subjective perspective).\nThe second setDis such that a pair of examples (xi,xk)belongs to setDifxiandxkare\ndissimilar.\nTo train the matrix of parameters Afrom the data, we want to \ufb01nd a positive semide\ufb01nite\nmatrix Athat solves the following optimization problem:\nmin\nA\u2211\n(xi,xk)\u2208S\u2225x\u2212x\u2032\u22252\nAsuch that\u2211\n(xi,xk)\u2208D\u2225x\u2212x\u2032\u2225A\u2265c,\nwherecis a positive constant (can be any number).\nThe solution to this optimization problem is found by gradient\ndescent with a modi\ufb01cation that ensures that the found matrix A\nis positive semide\ufb01nite. We leave the description of the algorithm\nout of the scope of this book for further reading.\nI should point out that one-shot learning withsiamese net-\nworksandtriplet loss can be seen as metric learning problem:\nthe pairs of pictures of the same person belong to the set S, while\npairs of random pictures belong to D.\nThere are many other ways to learn a metric, including non-linear\nand kernel-based. However, the one presented in this book, as well as the adaptation of\none-shot learning should su\ufb03ce for most practical applications.\n10.2 Learning to Rank\nLearning to rank is a supervised learning problem. Among others, one frequent problem\nsolved using learning to rank is the optimization of search results returned by a search engine\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 4", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2189, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a37ddfb-990f-4fac-8584-a910dd65b34f": {"__data__": {"id_": "3a37ddfb-990f-4fac-8584-a910dd65b34f", "embedding": null, "metadata": {"page_label": "124", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "700d1c7e-836f-4f29-bf39-1e714843a563", "node_type": "4", "metadata": {"page_label": "124", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "0b0871080c73d42fa20fdb5eed0d299addb97ed49b0fb9361f8555eb7ce10190", "class_name": "RelatedNodeInfo"}}, "text": "for a query. In search result ranking optimization, a labeled example Xiin the training set\nof sizeNis a ranked collection of documents of size ri(labels are ranks of documents). A\nfeature vector represents each document in the collection. The goal of the learning is to \ufb01nd\na ranking function fwhich outputs values that can be used to rank documents. For each\ntraining example, an ideal function fwould output values that induce the same ranking of\ndocuments as given by the labels.\nEach exampleXi,i= 1,...,N, is a collection of feature vectors with labels: Xi=\n{(xi,j,yi,j)}ri\nj=1. Features in a feature vector xi,jrepresent the document j= 1,...,r i.\nFor example, x(1)\ni,jcould represent how recent is the document, x(2)\ni,jwould re\ufb02ect whether the\nwords of the query can be found in the document title, x(3)\ni,jcould represent the size of the\ndocument, and so on. The label yi,jcould be the rank ( 1,2,...,r i) or a score. For example,\nthe lower the score, the higher the document should be ranked.\nThere are three approaches to solve that problem: pointwise ,pairwise , andlistwise.\nThepointwiseapproachtransformseachtrainingexampleintomultipleexamples: oneexample\nper document. The learning problem becomes a standard supervised learning problem, either\nregression or logistic regression. In each example (x,y)of the pointwise learning problem,\nxis the feature vector of some document, and yis the original score (if yi,jis a score) or a\nsynthetic score obtained from the ranking (the higher the rank, the lower is the synthetic\nscore). Any supervised learning algorithm can be used in this case. The solution is usually\nfar from perfect. Principally, this is because each document is considered in isolation, while\nthe original ranking (given by the labels yi,jof the original training set) could optimize the\npositions of the whole set of documents. For example, if we have already given a high rank\nto a Wikipedia page in some collection of documents, we would prefer not giving a high rank\nto another Wikipedia page for the same query.\nIn the pairwise approach, we also consider documents in isolation, however, in this case, a\npair of documents is considered at once. Given a pair of documents (xi,xk)we build a model\nf, which, given (xi,xk)as input, outputs a value close to 1, ifxishould be higher than xk\nin the ranking; otherwise, foutputs a value close to 0. At the test time, the \ufb01nal ranking for\nan unlabeled example Xis obtained by aggregating the predictions for all pairs of documents\ninX. The pairwise approach works better than pointwise, but still far from perfect.\nThe state of the art rank learning algorithms, such as LambdaMART , implement the\nlistwise approach. In the listwise approach, we try to optimize the model directly on some\nmetric that re\ufb02ects the quality of ranking. There are various metrics for assessing search\nengine result ranking, including precision and recall. One popular metric that combines both\nprecision and recall is called mean average precision (MAP).\nTo de\ufb01ne MAP, let us ask judges (Google call those people rankers) to examine a collection\nof search results for a query and assign relevancy labels to each search result. Labels could\nbe binary ( 1for \u201crelevant\u201d and 0for \u201cirrelevant\u201d) or on some scale, say from 1to5: the\nhigher the value, the more relevant the document is to the search query. Let our judges build\nsuch relevancy labeling for a collection of 100queries. Now, let us test our ranking model on\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 5", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3531, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "40dd251d-e054-4b84-9c97-4d371ee085ec": {"__data__": {"id_": "40dd251d-e054-4b84-9c97-4d371ee085ec", "embedding": null, "metadata": {"page_label": "125", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d7786585-6867-429c-9ddc-2acd93d53254", "node_type": "4", "metadata": {"page_label": "125", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "54915eba257f7c45c45184902d95d8ccbfbb7125da4b3e49d75177f29769509f", "class_name": "RelatedNodeInfo"}}, "text": "this collection. The precision of our model for some query is given by:\nprecision =|{relevant documents }\u2229{retrieved documents }|\n|{retrieved documents }|,\nwhere the notation |\u00b7|means \u201cthe number of.\u201d The average precision metric, AveP, is\nde\ufb01ned for a ranked collection of documents returned by a search engine for a query qas,\nAveP(q) =\u2211n\nk=1(P(k)\u00b7rel(k))\n|{relevant documents }|,\nwherenis the number of retrieved documents, P(k)denotes the precision computed for\nthe topksearch results returned by our ranking model for the query, rel(k)is an indicator\nfunction equaling 1if the item at rank kis a relevant document (according to judges) and\nzero otherwise. Finally, the MAP for a collection of search queries of size Qis given by,\nMAP =\u2211Q\nq=1AveP(q)\nQ.\nNow we get back to LambdaMART. This algorithm implements a pairwise approach, and it\nuses gradient boosting to train the ranking function h(x). Then the binary model f(xi,xk)\nthat predicts whether the document xishould have a higher rank than the document xk(for\nthe same search query) is given by a sigmoid with a hyperparameter \u03b1,\nf(xi,xk)def=1\n1 + exp((h(xi)\u2212h(xk))\u03b1.\nAgain, as with many models that predict probability, the cost function is cross-entropy\ncomputed using the model f. In our gradient boosting, we combine multiple regression trees\nto build the function hby trying to minimize the cost. Remember that in gradient boosting\nwe add a tree to the model to reduce the error that the current model makes on the training\ndata. For the classi\ufb01cation problem, we computed the derivative of the cost function to\nreplace real labels of training examples with these derivatives. LambdaMART works similarly,\nwith one exception. It replaces the real gradient with a combination of the gradient and\nanother factor that depends on the metric, such as MAP. This factor modi\ufb01es the original\ngradient by increasing or decreasing it so that the metric value is improved.\nThat is a very bright idea and not many supervised learning algorithms can boast that they\noptimize a metric directly. Optimizing a metric is what we really want, but what we do in a\ntypical supervised learning algorithm is we optimize the cost instead of the metric (we do\nthat because metrics are usually not di\ufb00erentiable). Usually, in supervised learning, as soon\nas we have found a model that optimizes the cost function, we try to tweak hyperparameters\nto improve the value of the metric. LambdaMART optimizes the metric directly.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 6", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2525, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "31ff7da7-0539-48d4-9fd7-501cd97e96bb": {"__data__": {"id_": "31ff7da7-0539-48d4-9fd7-501cd97e96bb", "embedding": null, "metadata": {"page_label": "126", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ced98744-aa5b-45bc-8fba-8adaf28df761", "node_type": "4", "metadata": {"page_label": "126", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "1a066dc1e1821d7bd10837014cb93ffa147731a7a071fa360be0ef71d9eadb95", "class_name": "RelatedNodeInfo"}}, "text": "The remaining question is how do we build the ranked list of results based on the predictions\nof the model fwhich predicts whether its \ufb01rst input has to be ranked higher than the second\ninput. It\u2019s generally a computationally hard problem, and there are multiple implementations\nof rankers capable of transforming pairwise comparisons into a ranking list.\nThe most straightforward approach is to use an existing sorting\nalgorithm. Sorting algorithms sort a collection of numbers in\nincreasing or decreasing order. (The simplest sorting algorithm\nis called bubble sort . It\u2019s usually taught in engineering schools.)\nTypically, sorting algorithms iteratively compare a pair of numbers\nin the collection and change their positions in the list based on the\nresult of that comparison. If we plug our function finto a sorting\nalgorithm to execute this comparison, the sorting algorithm will\nsort documents and not numbers.\n10.3 Learning to Recommend\nLearning to recommend is an approach to build recommender systems. Usually, we have a\nuser who consumes content. We have the history of consumption and want to suggest this\nuser new content that they would like. It could be a movie on Net\ufb02ix or a book on Amazon.\nTraditionally, two approaches were used to give recommendations: content-based \ufb01ltering\nandcollaborative \ufb01ltering .\nContent-based \ufb01ltering consists of learning what users like based on the description of the\ncontent they consume. For example, if the user of a news site often reads news articles on\nscience and technology, then we would suggest to this user more documents on science and\ntechnology. More generally, we could create one training set per user and add news articles\nto this dataset as a feature vector xand whether the user recently read this news article as a\nlabely. Then we build the model of each user and can regularly examine each new piece of\ncontent to determine whether a speci\ufb01c user would read it or not.\nThe content-based approach has many limitations. For example, the user can be trapped in\nthe so-called \ufb01lter bubble: the system will always suggest to that user the information that\nlooks very similar to what user already consumed. That could result in complete isolation of\nthe user from information that disagrees with their viewpoints or expands them. On a more\npractical side, the users might just stop following recommendations, which is undesirable.\nCollaborative \ufb01ltering has a signi\ufb01cant advantage over content-based \ufb01ltering: the recommen-\ndations to one user are computed based on what other users consume or rate. For instance,\nif two users gave high ratings to the same ten movies, then it\u2019s more likely that user 1 will\nappreciate new movies recommended based on the tastes of the user 2 and vice versa. The\ndrawback of this approach is that the content of the recommended items is ignored.\nIn collaborative \ufb01ltering, the information on user preferences is organized in a matrix. Each\nrow corresponds to a user, and each column corresponds to a piece of content that user rated\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3090, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6146f99-597a-49ab-9af9-89f7622b9f7a": {"__data__": {"id_": "e6146f99-597a-49ab-9af9-89f7622b9f7a", "embedding": null, "metadata": {"page_label": "127", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6d528b76-e117-40de-ac36-9e604926f4c5", "node_type": "4", "metadata": {"page_label": "127", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "7e407013d97cdcc61af4a7ae37d8e03d9b1984fd84eac4aa2eef603d28105338", "class_name": "RelatedNodeInfo"}}, "text": "or consumed. Usually, this matrix is huge and extremely sparse, which means that most of\nits cells aren\u2019t \ufb01lled (or \ufb01lled with a zero). The reason for such a sparsity is that most users\nconsume or rate just a tiny fraction of available content items. It\u2019s is very hard to make\nmeaningful recommendations based on such sparse data.\nMost real-world recommender systems use a hybrid approach: they combine recommendations\nobtained by the content-based and collaborative \ufb01ltering models.\nI already mentioned that content-based recommender model could be built using a classi\ufb01-\ncation or regression model that predicts whether a user will like the content based on the\ncontent\u2019s features. Examples of features could include the words in books or news articles the\nuser liked, the price, the recency of the content, the identity of the content author and so on.\nTwo e\ufb00ective recommender system learning algorithms are factorization machines (FM)\nanddenoising autoencoders (DAE).\n10.3.1 Factorization Machines\nFactorization machines is a relatively new kind of algorithm. It was explicitly designed for\nsparse datasets. Let\u2019s illustrate the problem.\nx(1)\nx(2)\nx(3)\nx(4)\nx(5)\nx(6)\n...\nx(D)1\n1\n1\n1\n1\n1\n0 0 0... ... ... ...0 0\n0 0\n0 0\n0 0\n0 0\n0 0...\n..................user\nEd Al Zak...\n1\n0\n0\n0\n0\n0\n1 0 0... ... ... ...0 0\n0 1\n0 1\n0 1\n0 0\n1 0...\n..................\n0\n0...0\n0\n0\n0\n1It Up Jaws Hermovie\n0.2\n0.2\n0.2\n0\n0\n0\n1 0 0... ... ... ...0.8 0.4\n0.4 0.8\n0.8 \u00a0 0.4\n0 0.7\n0 0.7\n0.8 0...\n..................\n0.6\n0...0\n0\n0.7\n0.1\n0.1It Up Jaws Herrated\u00a0movies\nx 99 x 100\n0.3\n0.3\n0.3\n0.35\n0.35\n0.5\n0.95...0.8\n0.8\n0.8 \u00a0\n0.78\n0.78\n0.77\n...\n0.851\n3\n2\n3\n1\n4\n5...y \ny(1)\ny(2)\ny(3)\ny(4)\ny(5)\ny(6)\ny(D)...x 1 x 2 x 3 x 21 x 22 x 23 x 24... ... x 40 x 41 x 42 x 43...\nFigure 1: Example for sparse feature vectors xand their respective labels y.\nIn Figure 1 you see an example of sparse feature vectors with labels. Each feature vector\nrepresents information about one speci\ufb01c user and one speci\ufb01c movie. Features in the blue\nsection represent a user. Users are encoded as one-hot vectors. Features in the green section\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 8", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2155, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "44248835-4d68-4f3e-b70e-9a87b856747c": {"__data__": {"id_": "44248835-4d68-4f3e-b70e-9a87b856747c", "embedding": null, "metadata": {"page_label": "128", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b185674b-5baf-49fb-88d6-4411302ae73d", "node_type": "4", "metadata": {"page_label": "128", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "9151c565d1321750b80aee4158f3ed09ecd685f6d5c9be0731b033f1c3df5d44", "class_name": "RelatedNodeInfo"}}, "text": "represent a movie. Movies are also encoded as one-hot vectors. Features in the yellow section\nrepresent scores the user in blue gave to each movie they rated. Feature x99represents the\nratio of movies with an Oscar among those the user has watched. Feature x100represents the\npercentage of the movie watched by the user in blue before they scored the movie in green.\nThe target yrepresents the score given by the user in blue to the movie in green.\nIn real recommender systems, the number of users can count in millions, so the matrix in\nFigure 1 would count hundreds of millions of rows. The number of features could be hundreds\nof thousands, depending on how rich is the choice of content and how creative you, as a\ndata analyst, are in feature engineering. Features x99andx100were handcrafted during the\nfeature engineering process, and I only show two features for the purposes of illustration.\nTrying to \ufb01t a regression or classi\ufb01cation model to such an extremely sparse dataset would\nresult in poor generalization. Factorization machines approach this problem di\ufb00erently.\nThe factorization machine model is de\ufb01ned as follows:\nf(x)def=b+D\u2211\ni=1wixi+D\u2211\ni=1D\u2211\nj=i+1(vivj)xixj,\nwherebandwi,i= 1,...,D, are scalar parameters similar to those used in linear regression.\nVectors viarek-dimensional vectors of factors.kis a hyperparameter and is usually much\nsmaller than D. The expression vivjis a dot-product of the ithandjthvectors of factors.\nAs you can see, instead looking for one wide vector of parameters, which can re\ufb02ect poorly\ninteractions between features because of sparsity, we complete it by additional parameters that\napply to pairwise interactions xixjbetween features. However, instead of having a parameter\nwi,jfor each interaction, which would add an enormous1quantity of new parameters to the\nmodel, we factorize wi,jintovivjby adding only Dk\u226aD(D\u22121)parameters to the model2.\nDepending on the problem, the loss function could be squared error loss (for regression) or\nhinge loss. For classi\ufb01cation with y\u2208{\u2212 1,+1}, with hinge loss or logistic loss the prediction\nis made as y= sign(f(x)). The logistic loss is de\ufb01ned as,\nloss(f(x),y) =1\nln 2ln(1 +e\u2212yf(x)).\nGradient descent can be used to optimize the average loss. In the example in Figure 1, the\nlabels are in{1,2,3,4,5}, so it\u2019s a multiclass problem. We can use one versus rest strategy\nto convert this multiclass problem into \ufb01ve binary classi\ufb01cation problems.\n1To be more precise we would add D(D\u22121)parameters wi,j.\n2The notation \u226ameans \u201cmuch less than.\u201d\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 9", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2589, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2d304acc-89e9-4414-a422-1409fec55507": {"__data__": {"id_": "2d304acc-89e9-4414-a422-1409fec55507", "embedding": null, "metadata": {"page_label": "129", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d73481fc-c3c2-4e7e-b776-32a0958e0574", "node_type": "4", "metadata": {"page_label": "129", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "a559018c1423d9d711e91726df54eaed275732e9ab529dcea7d6b65b6582d034", "class_name": "RelatedNodeInfo"}}, "text": "10.3.2 Denoising Autoencoders\nFrom Chapter 7, you know what a denoising autoencoder is: it\u2019s a neural network that\nreconstructs its input from the bottleneck layer. The fact that the input is corrupted by\nnoise while the output shouldn\u2019t be, makes denoising autoencoders an ideal tool to build a\nrecommender model.\nThe idea is very straightforward: new movies a user could like are seen as if they were\nremoved from the complete set of preferred movies by some corruption process. The goal of\nthe denoising autoencoder is to reconstruct those removed items.\nTo prepare the training set for our denoising autoencoder, remove the blue and green features\nfrom the training set in Figure 1. Because now some examples become duplicates, keep only\nthe unique ones.\nAt the training time, randomly replace some of the non-zero\nyellow features in the input feature vectors with zeros. Train the\nautoencoder to reconstruct the uncorrupted input.\nAt the prediction time, build a feature vector for the user. The\nfeature vector will include uncorrupted yellow features as well as\nthe handcrafted features like x99andx100. Use the trained DAE\nmodel to reconstruct the uncorrupted input. Recommend to the\nuser movies that have the highest scores at the model\u2019s output.\nAnother e\ufb00ective collaborative-\ufb01ltering model is an FFNN with\ntwo inputs and one output. Remember from Chapter 8 that neural networks are good at\nhandling multiple simultaneous inputs. A training example here is a triplet (u,m,r). The\ninput vector uis aone-hot encoding of a user. The second input vector mis a one-hot\nencoding of a movie. The output layer could be either a sigmoid (in which case the label ris\nin[0,1]) or ReLU, in which case rcan be in some typical range, [1,5]for example.\n10.4 Self-Supervised Learning: Word Embeddings\nWe have already discussed word embeddings in Chapter 7. Recall that word embeddings\nare feature vectors that represent words. They have the property that similar words have\nsimilar feature vectors. The question that you probably wanted to ask is where these word\nembeddings come from. The answer is (again): they are learned from data.\nThere are many algorithms to learn word embeddings. Here, we consider only one of them:\nword2vec , and only one version of word2vec called skip-gram , which works well in practice.\nPretrained word2vec embeddings for many languages are available to download online.\nIn word embedding learning, our goal is to build a model which we can use to convert a\none-hot encoding of a word into a word embedding. Let our dictionary contain 10,000 words.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 10", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2637, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "533ea260-62dc-4fcd-a029-63d208dea861": {"__data__": {"id_": "533ea260-62dc-4fcd-a029-63d208dea861", "embedding": null, "metadata": {"page_label": "130", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "87873179-575f-41fd-9513-6bfdd647d1fb", "node_type": "4", "metadata": {"page_label": "130", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "d689aeacc6414a86f056878467de96bc35ca6f3e4ecfc0fc6ed72b078dd84c48", "class_name": "RelatedNodeInfo"}}, "text": "The one-hot vector for each word is a 10,000-dimensional vector of all zeroes except for one\ndimension that contains a 1. Di\ufb00erent words have a 1in di\ufb00erent dimensions.\nConsider a sentence: \u201cI almost \ufb01nished reading the book on machine learning.\u201d Now, consider\nthe same sentence from which we have removed one word, say \u201cbook.\u201d Our sentence becomes:\n\u201cI almost \ufb01nished reading the \u00b7on machine learning.\u201d Now let\u2019s only keep the three words\nbefore the\u00b7and three words after: \u201c\ufb01nished reading the \u00b7on machine learning.\u201d Looking\nat this seven-word window around the \u00b7, if I ask you to guess what \u00b7stands for, you would\nprobably say: \u201cbook,\u201d \u201carticle,\u201d or \u201cpaper.\u201d That\u2019s how the context words let you predict\nthe word they surround. It\u2019s also how the machine can learn that words \u201cbook,\u201d \u201cpaper,\u201d\nand \u201carticle\u201d have a similar meaning: because they share similar contexts in multiple texts.\nIt turns out that it works the other way around too: a word can predict the context that\nsurrounds it. The piece \u201c\ufb01nished reading the \u00b7on machine learning\u201d is called a skip-gram\nwith window size 7 (3 + 1 + 3). By using the documents available on the Web, we can easily\ncreate hundreds of millions of skip-grams.\nLet\u2019s denote a skip-gram like this: [x\u22123,x\u22122,x\u22121,x,x+1,x+2,x+3]. In our sentence, x\u22123is\nthe one-hot vector for \u201c\ufb01nished,\u201d x\u22122corresponds to \u201creading,\u201d xis the skipped word ( \u00b7),x+1\nis \u201con\u201d and so on. A skip-gram with window size 5 will look like this: [x\u22122,x\u22121,x,x+1,x+2].\nThe skip-gram model with window size 5is schematically depicted in Figure 2. It is a\nfully-connected network, like the multilayer perceptron. The input word is the one denoted\nas\u00b7in the skip-gram. The neural network has to learn to predict the context words of the\nskip-gram given the central word.\nYou can see now why the learning of this kind is called self-supervised : the labeled examples\nget extracted from the unlabeled data such as text.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 11", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1980, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea51a011-aa38-47f7-a153-44ae35d82a79": {"__data__": {"id_": "ea51a011-aa38-47f7-a153-44ae35d82a79", "embedding": null, "metadata": {"page_label": "131", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e23c8d66-ff75-4c09-aae8-c634e024b8f7", "node_type": "4", "metadata": {"page_label": "131", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "7b16291df60cd9684173df2fda545673c20165db5ebfa1e9912463542fb6ffdb", "class_name": "RelatedNodeInfo"}}, "text": "Figure 2: The skip-gram model with window size 5and the embedding layer of 300units.\nThe activation function used in the output layer is softmax. The\ncost function is the negative log-likelihood. The embedding for a\nword is obtained as the output of the embedding layer when the\none-hot encoding of this word is given as the input to the model.\nBecause of the large number of parameters in the word2vec models,\ntwo techniques are used to make the computation more e\ufb03cient:\nhierarchical softmax (an e\ufb03cient way of computing softmax that\nconsists in representing the outputs of softmax as leaves of a binary\ntree) and negative sampling (the idea is only to update a random\nsample of all outputs per iteration of gradient descent). I leave these for further reading.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 12", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 827, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0a111d9-262c-4d36-83e7-2e92be7a017e": {"__data__": {"id_": "c0a111d9-262c-4d36-83e7-2e92be7a017e", "embedding": null, "metadata": {"page_label": "132", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d35b2684-ad48-4be0-943c-09979736fad0", "node_type": "4", "metadata": {"page_label": "132", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "2f462e978f1ed8ab1d8732cafb9d903eaec79279754ad43fae24ad7b764320a8", "class_name": "RelatedNodeInfo"}}, "text": "11 Conclusion\nWow, that was fast! You are really good if you got here and managed to understand most of\nthe book\u2019s material.\nIf you look at the number at the bottom of this page, you see that I have overspent paper,\nwhich means that the title of the book was slightly misleading. I hope that you forgive me\nfor this little marketing trick. After all, if I wanted to make this book exactly a hundred\npages, I could reduce font size, white margins, and line spacing, or remove the section on\nUMAP and leave you on your own with the original paper. Believe me: you would not want\nto be left on your own with the original paper on UMAP! (Just kidding.)\nHowever, by stopping now, I feel con\ufb01dent that you have got everything you need to become\na great modern data analyst or machine learning engineer. That doesn\u2019t mean that I covered\neverything, but what I covered in a hundred+ pages you would \ufb01nd in a bunch of books,\neach a thousand pages thick. Much of what I covered is not in the books at all: typical\nmachine learning books are conservative and academic, while I emphasized those algorithms\nand methods that you will \ufb01nd useful in your day to day work.\nWhat exactly would I have covered if it was a thousand-page machine learning book?\n11.1 What Wasn\u2019t Covered\n11.1.1 Topic Modeling\nIn text analysis, topic modeling is a prevalent unsupervised learning problem. You have a\ncollection of text documents, and you would like to discover topics present in each document.\nLatent Dirichlet Allocation (LDA) is a very e\ufb00ective algorithm of topic discovery. You\ndecide how many topics are present in your collection of documents and the algorithm assigns\na topic to each word in this collection. Then, to extract the topics from a document, you\nsimply count how many words of each topic are present in that document.\n11.1.2 Gaussian Processes\nGaussian processes (GP) is a supervised learning method that competes with kernel\nregression. It has some advantages over the latter. For example, it provides con\ufb01dence\nintervals for the regression line in each point. I decided not to explain GP because I could\nnot \ufb01gure out a simple way to explain them, but you de\ufb01nitely could spend some time to\nlearn about GP. It will be time well spent.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 3", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2293, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "efa2f2e1-3999-4cab-96e6-1e9d478f11db": {"__data__": {"id_": "efa2f2e1-3999-4cab-96e6-1e9d478f11db", "embedding": null, "metadata": {"page_label": "133", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "62755857-a7ba-4aab-bfa3-b8336c238a7e", "node_type": "4", "metadata": {"page_label": "133", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "c75e9336de404856ad8b0b70f40088e414604eb4be38276c3a4778ee66256a91", "class_name": "RelatedNodeInfo"}}, "text": "11.1.3 Generalized Linear Models\nGeneralized Linear Model (GLM) is a generalization of the linear regression to modeling\nvarious forms of dependency between the input feature vector and the target. Logistic\nregression, for instance, is one form of GLMs. If you are interested in regression and you\nlook for simple and explainable models, you should de\ufb01nitely read more on GLM.\n11.1.4 Probabilistic Graphical Models\nI have mentioned one example of probabilistic graphical models (PGMs) in Chapter 7:\nconditional random \ufb01elds (CRF). With CRF you can model the input sequence of words\nand relationships between the features and labels in this sequence as a sequential dependency\ngraph. More generally, a PGM can be any graph. A graphis a structure consisting of a\ncollection of nodes and edges that join a pair of nodes. Each node in PGM represents some\nrandom variable (values of which can be observed or unobserved), and edges represent the\nconditional dependence of one random variable on another random variable. For example,\nthe random variable \u201csidewalk wetness\u201d depends on the random variable \u201cweather condition.\u201d\nBy observing values of some random variables, an optimization algorithm can learn from\ndata the dependency between observed and unobserved variables.\nIf you decide to learn more about PGMs, they are also known under names of Bayesian\nnetworks, belief networks, and probabilistic independence networks.\n11.1.5 Markov Chain Monte Carlo\nIf you work with graphical models and want to sample examples from a very complex\ndistribution de\ufb01ned by the dependency graph, you could use Markov Chain Monte Carlo\n(MCMC) algorithms. MCMC is a class of algorithms for sampling from any probability\ndistribution de\ufb01ned mathematically. Remember that when we talked about denoising\nautoencoders , we sampled noise from the normal distribution. Sampling from standard\ndistributions, such as normal or uniform, is relatively easy because their properties are well\nknown. However, the task of sampling becomes signi\ufb01cantly more complicated when the\nprobability distribution can have an arbitrary form de\ufb01ned by a complex formula.\n11.1.6 Generative Adversarial Networks\nGenerative adversarial networks , or GANs, are a class of neural networks used in\nunsupervised learning. They are implemented as a system of two neural networks contesting\nwith each other in a zero-sum game setting. The most popular application of GANs is to\nlearn to generate photographs that look authentic to human observers. The \ufb01rst of the two\nnetworks takes a random input (typically Gaussian noise) and learns to generate an image as\na matrix of pixels. The second network takes as input two images: one \u201creal\u201d image from\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 4", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2757, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "636a79f7-22eb-488c-ba38-9aa470f16466": {"__data__": {"id_": "636a79f7-22eb-488c-ba38-9aa470f16466", "embedding": null, "metadata": {"page_label": "134", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "db91506a-d180-47dc-9c99-1e35423a8894", "node_type": "4", "metadata": {"page_label": "134", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "4f2dffcc24a957cc70ad7125cf163b2aafdea38d883def8ee6dd4d8a7efb660e", "class_name": "RelatedNodeInfo"}}, "text": "some collection of images as well as the image generated by the \ufb01rst network. The second\nnetwork has to learn to recognize which one of the two images was generated by the \ufb01rst\nnetwork. The \ufb01rst network gets a negative loss if the second network recognizes the \u201cfake\u201d\nimage. The second network, on the other hand, gets penalized if it fails to recognize which\none of the two images is fake.\n11.1.7 Genetic Algorithms\nGenetic algorithms (GA) are a numerical optimization technique used to optimize undif-\nferentiable optimization objective functions. They use concepts from evolutionary biology\nto search for a global optimum (minimum or maximum) of an optimization problem, by\nmimicking evolutionary biological processes.\nGA work by starting with an initial generation of candidate solutions. If we look for optimal\nvalues of the parameters of our model, we \ufb01rst randomly generate multiple combinations of\nparameter values. We then test each combination of parameter values against the objective\nfunction. Imagine each combination of parameter values as a point in a multi-dimensional\nspace. We then generate a subsequent generation of points from the previous generation by\napplying such concepts as \u201cselection,\u201d \u201ccrossover,\u201d and \u201cmutation.\u201d\nIn a nutshell, that results in each new generation keeping more points similar to those points\nfrom the previous generation that performed the best against the objective. In the new\ngeneration, the points that performed the worst in the previous generation are replaced by\n\u201cmutations\u201d and \u201ccrossovers\u201d of the points that performed the best. A mutation of a point is\nobtained by a random distortion of some attributes of the original point. A crossover is a\ncertain combination of several points (for example, an average).\nGenetic algorithms allow \ufb01nding solutions to any measurable optimization criteria. For\nexample, GA can be used to optimize the hyperparameters of a learning algorithm. They are\ntypically much slower than gradient-based optimization techniques.\n11.1.8 Reinforcement Learning\nAs we already discussed, reinforcement learning (RL) solves a very speci\ufb01c kind of problem\nwhere the decision making is sequential. Usually, there\u2019s an agent acting in an unknown\nenvironment. Each action brings a reward and moves the agent to another state of the\nenvironment (usually, as a result of some random process with unknown properties). The\ngoal of the agent is to optimize its long-term reward.\nReinforcement learning algorithms, such as Q-learning, as well as its neural network based\ncounterparts, are used in learning to play video games, robotic navigation and coordination,\ninventory and supply chain management, optimization of complex electric power systems\n(power grids), and learning \ufb01nancial trading strategies.\n\u2217 \u2217 \u2217\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 5", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2840, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a37df407-add8-460e-8ca1-9c4a620269bb": {"__data__": {"id_": "a37df407-add8-460e-8ca1-9c4a620269bb", "embedding": null, "metadata": {"page_label": "135", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e10369e5-eccd-4cca-a8e4-1e4e829f6e10", "node_type": "4", "metadata": {"page_label": "135", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}, "hash": "ba1cd82a42519b2eb943796327e000ee3b53d99e182a7b7307e812b6b9483008", "class_name": "RelatedNodeInfo"}}, "text": "The book stops here. Don\u2019t forget to occasionally visit the book\u2019s companion wiki to stay\nupdated on new developments in each machine learning area considered in the book. As I\nsaid in the Preface, this book, thanks to the constantly updated wiki, like a good wine keeps\ngetting better after you buy it.\nOh, and don\u2019t forget that the book is distributed on the read \ufb01rst, buy later principle. That\nmeans that if while reading these words you look at a PDF \ufb01le and cannot remember having\npaid to get it, you are probably the right person for buying the book.\n11.2 Acknowledgements\nThe high quality of this book would be impossible without volunteering editors. I especially\nthank the following readers for their systematic contributions: Martijn van Attekum, Daniel\nMaraini, Ali Aziz, Rachel Mak, Kelvin Sundli, and John Robinson.\nOther wonderful people to whom I am grateful for their help are Michael Anuzis, Knut\nSverdrup, Freddy Drennan, Carl W. Handlin, Abhijit Kumar, Lazze Veddb\u00e4rd, Ricardo\nReis, Daniel Gross, Johann Faouzi, Akash Agrawal, Nathanael Weill, Filip Jekic, Abhishek\nBabuji, Luan Vieira, Sayak Paul, Vaheid Wallets, Lorenzo Bu\ufb00oni, Eli Friedman, \u0141ukasz\nM\u0105dry, Haolan Qin, Bibek Behera, Jennifer Cooper, Nishant Tyagi, Denis Akhiyarov, Aron\nJanarv, Alexander Ovcharenko, Ricardo Rios, Michael Mullen, Matthew Edwards, David\nEtlin, Manoj Balaji J, David Roy, Luan Vieira, Luiz Felix, Anand Mohan, Hadi Sotudeh,\nCharlie Newey, Zamir Akimbekov, Jesus Renero, Karan Gadiya, Mustafa An\u0131l Derbent, JQ\nVeenstra, Zsolt Kreisz, Ian Kelly, Lukasz Zawada, Robert Wareham, Thomas Bosman, Lv\nSteven, Ariel Rossanigo, Michael Lumpkins, and Luciano Segura.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 6", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1722, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"42dfc06b-bed4-4399-a4d5-585e7c3d8a9d": {"node_ids": ["13c717ad-ba15-463a-9bf6-25f490b025a3"], "metadata": {"page_label": "1", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "2c13a6b1-322e-413f-886d-15fb0dad4c30": {"node_ids": ["09257282-d717-4e7f-b8ba-6054f3eb6e6f"], "metadata": {"page_label": "2", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "c2c679f6-aa44-4219-b7b2-02b80385c7d4": {"node_ids": ["339c6e91-87aa-4a42-a4e4-14104d548941"], "metadata": {"page_label": "3", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "ff5a837d-9dac-4240-a556-379ad7c0fb63": {"node_ids": ["61e7e04e-0b15-40fe-9d07-806ed0a30f74"], "metadata": {"page_label": "4", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "bcbe17cb-d5a2-44cd-b718-cd59130b531e": {"node_ids": ["0e5eacef-d23e-4e25-ac93-322e17e40379"], "metadata": {"page_label": "5", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "a3356b4b-8302-4edb-8271-ab839ddf2b5a": {"node_ids": ["78fc2af1-a8b9-47ad-a011-f0d3b12ffb12"], "metadata": {"page_label": "6", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "b5f15307-d81e-4764-874a-4061b39198f6": {"node_ids": ["8f63876c-4705-40c2-a6ec-20e85593ac7a"], "metadata": {"page_label": "7", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "9a727675-6b4c-4065-bf0a-2c31152f7cb2": {"node_ids": ["c3ef254a-3da0-4f1c-9f18-5bdf1ecf2ca6"], "metadata": {"page_label": "8", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "ec1486e7-a8af-46f9-b662-61e579b2c00b": {"node_ids": ["87ed0bd6-70ac-470f-bfb5-f0b1620ee280"], "metadata": {"page_label": "9", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "236cfce7-63f0-422a-9231-4f6cd1e49bfa": {"node_ids": ["6f34b1dd-be63-4574-8d45-27e98d5ec2e7"], "metadata": {"page_label": "10", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "40155453-fe17-4363-a9e6-cf3e7b94541b": {"node_ids": ["f080200b-667b-474f-bb2d-4b322d5838d7"], "metadata": {"page_label": "11", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "463eb972-64fb-47d6-a299-e117425adc0d": {"node_ids": ["e080ee58-03f1-44bf-9487-7f672ae6880a"], "metadata": {"page_label": "12", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "b14cf05b-cb8f-4706-98c7-5aa7838e6b00": {"node_ids": ["cf6fda56-25ab-4f3c-9a72-05c6d440bf16"], "metadata": {"page_label": "13", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "f043dc5b-32a0-44b0-8003-76ef4c10d44e": {"node_ids": ["eb88094e-6ff4-4470-8002-7a1adce41a06"], "metadata": {"page_label": "14", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "3b495b58-2261-4c79-9ef7-56ef823b29ae": {"node_ids": ["61a87f38-60a2-4e69-b999-e73b2e335de2"], "metadata": {"page_label": "15", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "300fa914-1a49-4e28-be76-ffa821a9c22c": {"node_ids": ["c85028cd-14f7-4ea8-8e7a-a12da317ac27"], "metadata": {"page_label": "16", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "9e9ee98d-f957-414c-a7be-b407a3f0eff8": {"node_ids": ["e7ca454d-295b-46a2-9fbb-90681869df2a"], "metadata": {"page_label": "17", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "145d45b6-8460-4056-911a-382d6e4d322b": {"node_ids": ["8a57dad2-394d-4dad-ba35-de02065cc213"], "metadata": {"page_label": "18", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "9b562cbf-8e90-4b91-96ba-3094fe3fa559": {"node_ids": ["e6ebfc45-f578-460e-8d9e-21c4042ce86d"], "metadata": {"page_label": "19", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "dc5316fe-3305-4445-9d15-30dd1f46117d": {"node_ids": ["957fa270-d43a-482e-87de-8492a4674010"], "metadata": {"page_label": "20", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "e8d1afae-2b38-41a4-a784-141fb81c6fd4": {"node_ids": ["afac65e7-07d0-4fb7-9722-af0d81399971"], "metadata": {"page_label": "21", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "91528d1a-8dbc-41b0-922f-dfe21a9cca72": {"node_ids": ["09c3568d-e935-4c2e-baea-4a463ec3c6e9"], "metadata": {"page_label": "22", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "6391795a-8c64-4f4c-a62e-922cfd00a928": {"node_ids": ["8390815e-ba1b-49cf-a931-f9d8039fd2f2"], "metadata": {"page_label": "23", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "92edad98-3281-46f2-968f-5a6a78cde3e2": {"node_ids": ["378763e0-901e-4403-b815-db458afaf31e"], "metadata": {"page_label": "24", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "529005d4-ff38-410a-8c57-399b16b82484": {"node_ids": ["171aaa29-6868-4a13-9a86-eecd0dac2811"], "metadata": {"page_label": "25", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "3ae2fc96-95f8-4af2-b2d5-1f343bbd6cd8": {"node_ids": ["b721aae6-72f1-4b58-bc87-b6109094c0e1"], "metadata": {"page_label": "26", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "05c91038-c4fd-45dc-bb30-fe11f6af60d8": {"node_ids": ["5043bf18-90e8-4d1c-bf6a-6915f07b3627"], "metadata": {"page_label": "27", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "f62527f3-faf3-4cb1-a502-bcb3672b72b2": {"node_ids": ["9f62ce4f-1e0f-4597-81df-aa14959f78f8"], "metadata": {"page_label": "28", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "896b708d-b98a-4393-bde9-b572afdd793f": {"node_ids": ["b96b7a7b-a003-4363-8382-49651fd7e713"], "metadata": {"page_label": "29", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "2d714854-2f05-4db6-a0ae-010d0c7207dd": {"node_ids": ["160e5079-55d8-4dd2-8bf5-c61f2331bcf3"], "metadata": {"page_label": "30", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "cac1dd84-9e09-4d57-8b7e-9f26057bd77e": {"node_ids": ["b5b4c5d0-d013-4f2c-971a-dbc62905ea04"], "metadata": {"page_label": "31", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "e255d32d-2304-43e8-a73a-84dc3a995e17": {"node_ids": ["5b7ea7af-c383-4743-815e-c56b6fab8543"], "metadata": {"page_label": "32", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "3a49a07b-e5f0-4d89-9143-35441564041b": {"node_ids": ["d28791c8-40ec-42d5-b583-c80855cf254e"], "metadata": {"page_label": "33", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "e6252d24-3ec6-433c-bf6c-71387eb1ef4c": {"node_ids": ["be60ad8c-2f1f-442d-9516-2264a1301be3"], "metadata": {"page_label": "34", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "231d88f9-4a99-41a2-9b70-2686e067d0d4": {"node_ids": ["b80e2dda-07a0-445c-a92c-c45d51495ad5"], "metadata": {"page_label": "35", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "77bca458-a819-4b0e-ac9b-740545893ab4": {"node_ids": ["c902e9a2-14e3-4f69-85c5-01da062b492d"], "metadata": {"page_label": "36", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "1aa2b63d-c630-48f0-9759-a1a20df01941": {"node_ids": ["4802546c-8ee0-4757-abe6-33c7b68425af"], "metadata": {"page_label": "37", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "57646547-3bed-458b-9a97-9d67154c7286": {"node_ids": ["c5f6b640-7eb9-407c-9380-262be5fcc394"], "metadata": {"page_label": "38", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "22ee1e88-5647-465a-b62c-61af6cdad8b4": {"node_ids": ["552b268a-2e01-49f8-90c3-ce7149701707"], "metadata": {"page_label": "39", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "8b6851e6-3986-426b-a835-95a001537067": {"node_ids": ["fb43f2b9-c458-4c8a-9d71-e10e350b13c0"], "metadata": {"page_label": "40", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "eca461b2-31dd-49f4-8e89-684463bb7d30": {"node_ids": ["bd3adb36-d586-4f85-a2a0-6863cf86b86f"], "metadata": {"page_label": "41", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "ff04144a-ff21-400c-a57a-dc4a8e83c8dc": {"node_ids": ["5c2684d3-8221-4508-8569-270867c838be"], "metadata": {"page_label": "42", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "bee74d58-9e81-4711-ade1-92cc2922b28d": {"node_ids": ["70bde962-238f-4dc4-8114-9cc0201d4853"], "metadata": {"page_label": "43", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "166efef4-6439-467e-bbbf-bc4c9959597d": {"node_ids": ["ec72f00c-b906-488c-ab23-0f243b45ea9d"], "metadata": {"page_label": "44", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "16f5ca3f-72d2-42e7-a58b-5efa196c2798": {"node_ids": ["ff02cec2-25b4-4f21-b701-dae6fb6d791c"], "metadata": {"page_label": "45", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "952adb7b-a96b-47ae-838a-13381806395a": {"node_ids": ["84547772-9245-4382-8c65-836bd343c163"], "metadata": {"page_label": "46", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "72f70266-cc7c-4783-bd8f-54e3da726b64": {"node_ids": ["12ba2ef1-b1c8-4169-ac3d-0f26ea3b916b"], "metadata": {"page_label": "47", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "dad7c135-9a03-4c63-aaa8-e6c056775732": {"node_ids": ["daea85b6-9b80-452a-b059-9882c8c05669"], "metadata": {"page_label": "48", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "a05dd0f5-f3e3-4614-b721-c22c184e4171": {"node_ids": ["77ebf0ba-23af-4770-a416-870596cbcb94"], "metadata": {"page_label": "49", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "3b66cd3f-b0a9-46b2-9169-efd9f5ab88b3": {"node_ids": ["dc09360e-c541-48ca-b29d-e975d0220eed"], "metadata": {"page_label": "50", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "8337b4ab-37fd-4c5c-94dd-e73ae77bf82b": {"node_ids": ["6230913a-b347-4e8a-99d3-14b30e9476f0"], "metadata": {"page_label": "51", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "5b7cb74b-91ca-4cba-9b0a-4ea45ec07018": {"node_ids": ["d92335ae-48ba-4e2e-999e-f956a6c15a95"], "metadata": {"page_label": "52", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "17700155-9b95-4044-8bdd-56f6a25b41c5": {"node_ids": ["8819ea56-9a5e-4ae5-a4dd-d5a772861dd4"], "metadata": {"page_label": "53", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "04f346ca-c21f-4cbc-b462-70b60b75c872": {"node_ids": ["3a3d8a86-f212-4b9b-afa1-4eae5e2f8459"], "metadata": {"page_label": "54", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "f8564556-755a-4f62-a64a-b923a7b64262": {"node_ids": ["c100e789-01e9-4636-8efd-ad7064a5b64c"], "metadata": {"page_label": "55", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "62936da7-cbd5-44bd-8952-1899d9bd1736": {"node_ids": ["6504eceb-ebe5-4edd-98f4-5cd88b2e0691"], "metadata": {"page_label": "56", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "eac05390-70d0-49cc-be8e-3c0c617a38ca": {"node_ids": ["0d3061b7-a8a4-4983-a95a-13ed81e98d18"], "metadata": {"page_label": "57", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "11c5b1e0-d02a-48eb-ab94-04a17e350133": {"node_ids": ["54371945-0542-4347-9775-9123bf3fa463"], "metadata": {"page_label": "58", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "82b97858-dab1-4527-bc5f-56061962431e": {"node_ids": ["b9dd4991-8ee5-4f26-b5c0-6585063741fb"], "metadata": {"page_label": "59", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "beebe7be-037d-4c1f-abfa-6a5b6f3cf974": {"node_ids": ["94dc798e-abf9-4c56-ac23-42d5448aa4e2"], "metadata": {"page_label": "60", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "f685bc46-fec8-417a-b9f1-5363da5466f9": {"node_ids": ["51dd81db-2ce1-4c68-93fc-df03f52c2d15"], "metadata": {"page_label": "61", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "ab2e30c6-ceb5-48e0-bb32-0bb147deec11": {"node_ids": ["f8e27359-9c32-4dea-88fc-ba574ed60b37"], "metadata": {"page_label": "62", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "93c3cf00-5dab-4807-b566-2173f8e32b19": {"node_ids": ["5a7023ac-ef10-4246-9abf-ae044c30701c"], "metadata": {"page_label": "63", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "c5948655-dac7-4e52-91a7-9184c2c27339": {"node_ids": ["4344d8c1-7033-4e68-bfc9-11ebc7912263"], "metadata": {"page_label": "64", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "dbf10e25-8ce0-4cb4-bdd8-bea644d79cf6": {"node_ids": ["8788a6ea-b5c8-4b29-8336-415514881aff"], "metadata": {"page_label": "65", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "1a618400-0ee9-45f9-a8ae-5d06524711ac": {"node_ids": ["0e466327-b3fb-4d2f-b7ef-2caea8fb064a"], "metadata": {"page_label": "66", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "4aba9541-cccd-49e5-b2ac-e250522d9618": {"node_ids": ["c2bd1df4-508e-4184-857b-2fbdaa7f8812"], "metadata": {"page_label": "67", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "fa1c767d-4e6b-4a38-b356-672d353b0e17": {"node_ids": ["ff0a2bfd-77d4-4ed0-88e6-ec1699e4375e"], "metadata": {"page_label": "68", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "4ef05452-1812-44b9-90ae-7fea2cdd255a": {"node_ids": ["0af700ff-94de-4a09-acce-ff9436f1e0f1"], "metadata": {"page_label": "69", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "3e69a925-8276-4828-8be8-47e81923fca0": {"node_ids": ["84f69743-60b0-4e4e-8812-87c86274e7e5"], "metadata": {"page_label": "70", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "73a89634-2cb5-4c16-a73b-89ad22fcb4ed": {"node_ids": ["7c5d768e-c814-4553-9e41-702fc4addd71"], "metadata": {"page_label": "71", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "4b16ebc7-bc2e-47e4-b6cb-043478a5b937": {"node_ids": ["d446ef3c-9ec8-4f4d-979e-6d99bac96e05"], "metadata": {"page_label": "72", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "f83e2fd0-70ac-4676-8137-970b4fa47ea0": {"node_ids": ["b6519a99-6eb7-439f-8d2b-f0ae36f643f1"], "metadata": {"page_label": "73", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "8b62eb6c-9ca7-4889-b8e5-96ece790887d": {"node_ids": ["158146fd-2925-4430-9c4d-a7d15b06900a"], "metadata": {"page_label": "74", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "f2de2223-c69c-450a-a273-63b345aa8043": {"node_ids": ["b08db4ef-854f-412b-9eea-6de6b74341f2"], "metadata": {"page_label": "75", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "02c012ff-ec10-4a3d-8f41-f60e812ac611": {"node_ids": ["7d8f1905-1e73-402c-a430-0a773fe11a12"], "metadata": {"page_label": "76", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "671a4e26-0453-4a71-9a17-08cd9faca12c": {"node_ids": ["4e5b1d44-088b-400e-b864-b86d851a2e0e"], "metadata": {"page_label": "77", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "84dc5868-0544-492d-9926-8ebcce33a25d": {"node_ids": ["82d812b8-e543-4d5d-94c5-6891b9c3794c"], "metadata": {"page_label": "78", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "a2b046e6-16b2-4143-ac1a-e10772a9b55a": {"node_ids": ["4a8af527-96b6-4b97-a8c9-00dcaee75ec0"], "metadata": {"page_label": "79", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "cb66a87e-c56c-40a2-b8a1-4a75ea693a26": {"node_ids": ["280f327d-d51f-443c-abd9-7c6dd0a88929"], "metadata": {"page_label": "80", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "83f9605f-2565-4e30-8d4e-14d2d3b22650": {"node_ids": ["29586fdf-d098-40a1-9119-5ba5880435d1"], "metadata": {"page_label": "81", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "500061dc-92f7-4365-8406-99234abea413": {"node_ids": ["8b6d9912-c11b-4511-a5b2-69e2d609a437"], "metadata": {"page_label": "82", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "63e3323b-29c5-437a-a896-71540232be79": {"node_ids": ["664ad79e-23ce-4054-9fb4-3a96e2927e86"], "metadata": {"page_label": "83", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "921bca7a-668b-409a-ad25-e3f04b3cc4ec": {"node_ids": ["db0c9cee-c83e-4a0e-9e1d-cce392154337"], "metadata": {"page_label": "84", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "013a546c-781c-4734-b316-8f1b3271edb8": {"node_ids": ["17ab93d6-753c-401e-82fe-1e291fc890f9"], "metadata": {"page_label": "85", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "57b8399e-2bed-45ed-bc3e-57bb94ea2fc8": {"node_ids": ["4c8c2f8a-d6a9-46f1-8755-67c16356aa44"], "metadata": {"page_label": "86", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "97d240b7-4ed6-4279-98e0-2cc3afa86524": {"node_ids": ["6e67c100-6106-4f4b-ae96-7c43c6a418f3"], "metadata": {"page_label": "87", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "8d812b50-3a71-491e-b361-a30b1036a4a9": {"node_ids": ["614cff0d-f071-40c4-9704-2cfbbbce903f"], "metadata": {"page_label": "88", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "e2a67bef-a949-46fc-ac65-7bdcc16566d3": {"node_ids": ["3cab4973-9e01-40af-ba9e-882bdd6ed504"], "metadata": {"page_label": "89", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "02083bcc-7bb1-43f8-bfb1-f61efafa959a": {"node_ids": ["903d10a6-d5dd-408e-9833-8f04cb28c663"], "metadata": {"page_label": "90", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "ccea3dca-d0a4-4c92-bef0-34cd17edca09": {"node_ids": ["f1e70401-da95-483f-98d7-c56e4cd89f67"], "metadata": {"page_label": "91", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "43ba82ed-d9c5-48e5-a435-11251cd31cf0": {"node_ids": ["98749495-f5fc-465f-a4e8-b2d791d58b3a"], "metadata": {"page_label": "92", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "dc51e147-8f17-430e-add5-69dcb9ecc197": {"node_ids": ["8722d910-24e8-454b-b1f5-29edf3e09175"], "metadata": {"page_label": "93", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "bffdf3e8-ab45-427b-8751-0001c80ad499": {"node_ids": ["c2f8ef3a-4caa-4109-9229-a1274e0e8f6a"], "metadata": {"page_label": "94", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "836466aa-7804-4b6c-8c8f-9ace9f971975": {"node_ids": ["920d3e7f-6893-4d67-b475-292e6cd2aa89"], "metadata": {"page_label": "95", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "704c6663-eadf-44ce-a249-f8a9748bf8c7": {"node_ids": ["882b111b-2fb7-4160-9ace-1add16437a1d"], "metadata": {"page_label": "96", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "66e6121f-6154-4cab-ad5f-179959b99859": {"node_ids": ["3708e3bb-83f0-486c-be7f-ede014ac9f87"], "metadata": {"page_label": "97", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "158b4fd4-84fa-46cb-ba88-cc6ce9af1377": {"node_ids": ["ce9d2fef-bddc-456d-9877-79429b6e21c8"], "metadata": {"page_label": "98", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "1529b363-1ad3-4341-b83e-e21ba1ab453b": {"node_ids": ["f8a06d2d-0eaa-4bd5-a0e6-9bdcfd3adc4d"], "metadata": {"page_label": "99", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "b3340588-3f20-4dcb-80de-7ef76144940d": {"node_ids": ["c95d4283-c882-4d33-9969-0d1954aa35fe"], "metadata": {"page_label": "100", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "8ee95c9a-9392-4039-b857-011841488536": {"node_ids": ["cdba7ae4-5656-42f8-b574-c0645f15f604"], "metadata": {"page_label": "101", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "f4ee1c8a-3e53-44a5-af15-43c0e0c155df": {"node_ids": ["5638b917-863e-4c6f-83e6-c7a962868021"], "metadata": {"page_label": "102", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "5d3e9c11-2d62-4873-be37-dd8c745e2e4e": {"node_ids": ["b52fe620-5fcb-417a-8c21-22d6042a0e65"], "metadata": {"page_label": "103", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "1b14d0e2-118b-4c3c-86dc-d2bc8b126a26": {"node_ids": ["cde656c5-9dba-41e3-a2d6-66db017044ad"], "metadata": {"page_label": "104", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "88cb42ce-2a10-45a2-a6db-d00847e1251c": {"node_ids": ["8796bac7-75aa-408b-9b9a-40e055321fec"], "metadata": {"page_label": "105", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "1163e26b-b26e-417b-a405-bffa47ae1dfa": {"node_ids": ["76fe4eb1-3815-42bc-be6c-3cc1198294ad"], "metadata": {"page_label": "106", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "be0ae290-c25e-4b00-a405-6401e9e485e9": {"node_ids": ["75e19981-a089-4f42-ad13-5f3aa9e8635e"], "metadata": {"page_label": "107", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "fb00f818-6704-4b93-8229-5a69a872387f": {"node_ids": ["d52acce8-77d5-454d-ad59-17ba89909c4f"], "metadata": {"page_label": "108", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "acb9bc96-3194-42ec-94d7-3216d5d4c7bd": {"node_ids": ["ed2815a6-0544-4727-ba7e-2e4b44d83b0d"], "metadata": {"page_label": "109", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "1207bb23-3cc4-4ccf-8acd-e7fbf0dbde9c": {"node_ids": ["3297d0df-bdac-4205-8d02-636e82965685"], "metadata": {"page_label": "110", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "5c26d7fb-a644-4547-bd3a-86589b716f75": {"node_ids": ["0cf6e7f5-5659-4230-9c66-0714b781e6e2"], "metadata": {"page_label": "111", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "35e98686-cc2b-4fc4-9aa9-5e0d2ce033e3": {"node_ids": ["cc840564-39ae-4273-b4b2-3cc78a887177"], "metadata": {"page_label": "112", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "66ec9c54-e70f-4f8f-9774-e291067e35cc": {"node_ids": ["9949ef84-c95b-4021-a186-09be6e6ffd3c"], "metadata": {"page_label": "113", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "1f3abe10-a5a1-459e-915f-ccf799871a58": {"node_ids": ["7983bfc7-6ae6-4fbd-8762-d1b320e07bd6"], "metadata": {"page_label": "114", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "e6d3dcf4-4ba5-4eef-8cf1-7dee1d6b622a": {"node_ids": ["d529e27b-abd6-4808-82da-ec8daeebd7e5"], "metadata": {"page_label": "115", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "b939cf73-df5f-4b35-86a6-8bee5adb074b": {"node_ids": ["9036412e-c813-4d74-ad6f-d408795e9e22"], "metadata": {"page_label": "116", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "6df73be9-77f3-4296-b9df-b20a1d1e3b6e": {"node_ids": ["a4335725-15bb-47b3-8afc-4a2d3b31c7bd"], "metadata": {"page_label": "117", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "36acd708-e0f9-49e6-a6fb-74218b35fa92": {"node_ids": ["e5552675-92cd-408f-bf76-7b5d271b2374"], "metadata": {"page_label": "118", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "e71a8006-d0b0-4e85-b8b4-a1b1209a86f1": {"node_ids": ["d20653fb-2fed-43da-9e36-8b6b6ad2a265"], "metadata": {"page_label": "119", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "3a140472-3f96-4831-a7ee-0a8d9ea53dd6": {"node_ids": ["c88faa36-2a24-4b24-8a0c-3ddc63d49193"], "metadata": {"page_label": "120", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "e1d3c0c2-8fa4-435a-ab65-3e9cbbef7c2c": {"node_ids": ["c6e54bac-f0bb-47f2-b803-0c23b71cf13e"], "metadata": {"page_label": "121", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "1aacf9a1-a264-4d8c-ad62-4ed02ae7df37": {"node_ids": ["634f5e64-ea86-4a55-a803-ed6c47e7f121"], "metadata": {"page_label": "122", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "cbead1e3-8b11-46c1-8d1c-49a9f8a3349c": {"node_ids": ["93d52f67-c9a2-400f-b662-9e73f7ebb849"], "metadata": {"page_label": "123", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "700d1c7e-836f-4f29-bf39-1e714843a563": {"node_ids": ["3a37ddfb-990f-4fac-8584-a910dd65b34f"], "metadata": {"page_label": "124", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "d7786585-6867-429c-9ddc-2acd93d53254": {"node_ids": ["40dd251d-e054-4b84-9c97-4d371ee085ec"], "metadata": {"page_label": "125", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "ced98744-aa5b-45bc-8fba-8adaf28df761": {"node_ids": ["31ff7da7-0539-48d4-9fd7-501cd97e96bb"], "metadata": {"page_label": "126", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "6d528b76-e117-40de-ac36-9e604926f4c5": {"node_ids": ["e6146f99-597a-49ab-9af9-89f7622b9f7a"], "metadata": {"page_label": "127", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "b185674b-5baf-49fb-88d6-4411302ae73d": {"node_ids": ["44248835-4d68-4f3e-b70e-9a87b856747c"], "metadata": {"page_label": "128", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "d73481fc-c3c2-4e7e-b776-32a0958e0574": {"node_ids": ["2d304acc-89e9-4414-a422-1409fec55507"], "metadata": {"page_label": "129", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "87873179-575f-41fd-9513-6bfdd647d1fb": {"node_ids": ["533ea260-62dc-4fcd-a029-63d208dea861"], "metadata": {"page_label": "130", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "e23c8d66-ff75-4c09-aae8-c634e024b8f7": {"node_ids": ["ea51a011-aa38-47f7-a153-44ae35d82a79"], "metadata": {"page_label": "131", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "d35b2684-ad48-4be0-943c-09979736fad0": {"node_ids": ["c0a111d9-262c-4d36-83e7-2e92be7a017e"], "metadata": {"page_label": "132", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "62755857-a7ba-4aab-bfa3-b8336c238a7e": {"node_ids": ["efa2f2e1-3999-4cab-96e6-1e9d478f11db"], "metadata": {"page_label": "133", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "db91506a-d180-47dc-9c99-1e35423a8894": {"node_ids": ["636a79f7-22eb-488c-ba38-9aa470f16466"], "metadata": {"page_label": "134", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}, "e10369e5-eccd-4cca-a8e4-1e4e829f6e10": {"node_ids": ["a37df407-add8-460e-8ca1-9c4a620269bb"], "metadata": {"page_label": "135", "file_name": "pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_path": "d:\\LLM_Project\\RAG_BASED_CHATBOT\\data\\pdfcoffee.com_the-hundred-page-machine-learning-book-2-pdf-free.pdf", "file_type": "application/pdf", "file_size": 6546199, "creation_date": "2024-08-11", "last_modified_date": "2024-08-11"}}}}